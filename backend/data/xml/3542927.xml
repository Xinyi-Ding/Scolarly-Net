<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_RppgyDY" coord="1,80.70,104.40,443.83,13.56;1,80.70,121.34,253.78,13.56">Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,80.70,146.42,44.52,11.73"><forename type="first">Sijie</forename><surname>Mai</surname></persName>
						</author>
						<author>
							<persName coords="1,80.35,188.27,57.54,11.73"><forename type="first">Ying</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<note type="raw_affiliation">School of Electronics and Information Technology, Sun Yat-sen University, China</note>
								<orgName type="department">School of Electronics and Information Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<note type="raw_affiliation">SONGLONG XING, School of Electronics and Information Technology, Sun Yat-sen University, China</note>
								<orgName type="department" key="dep1">SONGLONG XING</orgName>
								<orgName type="department" key="dep2">School of Electronics and Information Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<note type="raw_affiliation">JIAXUAN HE, School of Electronics and Information Technology, Sun Yat-sen University, China</note>
								<orgName type="department" key="dep1">JIAXUAN HE</orgName>
								<orgName type="department" key="dep2">School of Electronics and Information Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<note type="raw_affiliation">School of Electronics and Information Technology, Sun Yat-sen University, China</note>
								<orgName type="department">School of Electronics and Information Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<note type="raw_affiliation">HAIFENG HU, School of Electronics and Information Technology, Sun Yat-sen University, China</note>
								<orgName type="department">School of Electronics and Information Technology</orgName>
								<orgName type="institution" key="instit1">HAIFENG HU</orgName>
								<orgName type="institution" key="instit2">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<note type="raw_affiliation">School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China; Songlong Xing,</note>
								<orgName type="department" key="dep1">School of Electronics and Information Technology</orgName>
								<orgName type="department" key="dep2">Songlong Xing</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<note type="raw_affiliation">School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China;</note>
								<orgName type="department">School of Electronics and Information Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<note type="raw_affiliation">Jiaxuan He, School of Electronics and Information Technology, Sun Yat-sen University, Guangzhou, China; Ying Zeng, School of Electronics and Information Technology,</note>
								<orgName type="department" key="dep1">School of Electronics and Information Technology</orgName>
								<orgName type="department" key="dep2">School of Electronics and Information Technology</orgName>
								<orgName type="institution" key="instit1">Jiaxuan He</orgName>
								<orgName type="institution" key="instit2">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou, Ying Zeng</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<note type="raw_affiliation">Sun Yat-sen University, Guangzhou, China; Haifeng Hu, School of Electronics and Information Technology,</note>
								<orgName type="department">School of Electronics and Information Technology</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<addrLine>China; Haifeng Hu</addrLine>
									<settlement>Guangzhou</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<note type="raw_affiliation">Sun Yat-sen University, Guangzhou, China.</note>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_ze67xZT" coord="1,80.70,104.40,443.83,13.56;1,80.70,121.34,253.78,13.56">Multimodal Graph for Unaligned Multimodal Sequence Analysis via Graph Convolution and Graph Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">545464DC922EA5A4D8AE5CD66CA139B0</idno>
					<idno type="DOI">10.1145/3542927</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-13T15:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_6bwTfcr">CCS Concepts:</term>
					<term xml:id="_SDwKDNb">Computing methodologies → Natural language generation;</term>
					<term xml:id="_u7uJCDA">Theory of computation → Machine learning theory Graph pooling, Multimodal Graph, Multimodal sequence analysis, Sentiment analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_rnYPMWk"><p xml:id="_hmBbd7a"><s xml:id="_7U8CqhM" coords="1,80.70,222.72,368.23,8.72">Multimodal sequence analysis aims to draw inferences from visual, language and acoustic sequences.</s><s xml:id="_E3FSbGr" coords="1,451.17,222.72,80.13,8.72;1,80.37,233.68,450.94,8.72;1,80.70,244.64,35.73,8.72">A majority of existing works focus on the aligned fusion of three modalities to explore inter-modal interactions, which is impractical in real-world scenarios.</s><s xml:id="_JybSWjc" coords="1,118.67,244.64,412.63,8.72;1,80.70,255.60,96.23,8.72">To overcome this issue, we seek to focus on analyzing unaligned sequences which is still relatively underexplored and also more challenging.</s><s xml:id="_TnDDqKm" coords="1,179.13,255.60,352.18,8.72;1,80.70,266.56,135.62,8.72">We propose Multimodal Graph, whose novelty mainly lies in transforming the sequential learning problem into graph learning problem.</s><s xml:id="_ewcr2Ya" coords="1,218.57,266.56,312.73,8.72;1,80.70,277.52,411.55,8.72">The graph-based structure enables parallel computation in time dimension (as opposed to RNNs) and can efectively learn longer intra-and inter-modal temporal dependency in unaligned sequences.</s><s xml:id="_aMEdM2Q" coords="1,494.49,277.52,36.81,8.72;1,80.70,288.47,418.74,8.72">Firstly we propose multiple ways to construct the adjacency matrix for sequence to perform sequence to graph transformation.</s><s xml:id="_DFnuzJ4" coords="1,501.67,288.47,29.64,8.72;1,80.70,299.43,439.34,8.72">To learn intra-modal dynamics, a graph convolution network is employed for each modality based on the deined adjacency matrix.</s><s xml:id="_dUNKcyE" coords="1,522.19,299.43,9.11,8.72;1,80.70,310.39,450.60,8.72;1,80.70,321.35,63.07,8.72">To learn inter-modal dynamics, given that the unimodal sequences are unaligned, the commonly-considered word-level fusion does not pertain.</s><s xml:id="_3Ea9MBt" coords="1,146.19,321.35,385.12,8.72;1,80.70,332.31,405.47,8.72">To this end, we innovatively devise graph pooling algorithms to automatically explore the associations between various time slices from diferent modalities and learn high-level graph representation hierarchically.</s><s xml:id="_5JcP9rB" coords="1,488.43,332.31,42.88,8.72;1,80.70,343.27,359.39,8.72">Multimodal Graph outperforms state-of-the-art models on three datasets under the same experimental setting.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="612.0" lry="792.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_7PV4pRu">INTRODUCTION</head><p xml:id="_SaqNQJb"><s xml:id="_6QkyM4J" coords="1,80.22,424.34,451.34,9.69;1,80.70,436.30,450.60,9.69;1,80.70,448.25,155.41,9.69">With the development of the Internet and social platforms, there has been a large number of videos produced by users to express their views and posted online, which provides a source of multimodal data to analyze people's opinion in large quantities <ref type="bibr" coords="1,193.61,448.25,10.49,9.69" target="#b2">[3,</ref><ref type="bibr" coords="1,206.79,448.25,11.50,9.69" target="#b25">26,</ref><ref type="bibr" coords="1,220.99,448.25,11.34,9.69" target="#b27">28]</ref>.</s><s xml:id="_5HgCKBU" coords="1,238.80,448.25,292.50,9.69;1,80.70,460.21,452.14,9.69">As videos are a typical form of multimodal information, people do not rely only on the spoken language, but they also use facial expressions and acoustic tones to convey information.</s><s xml:id="_9kaJrqR" coords="1,80.70,472.16,451.70,9.69;1,80.70,484.12,268.32,9.69">In this paper, our downstream task is to use three modalities, i.e., language, visual and acoustic modalities, to draw inferences for the sentiment polarities of speakers <ref type="bibr" coords="1,330.28,484.12,14.99,9.69" target="#b33">[34]</ref>.</s><s xml:id="_KSjpS6a" coords="1,352.13,484.12,179.43,9.69;1,80.70,496.07,452.14,9.69">These three modalities are complementary and actively interact with one another, providing more comprehensive information than one single modality.</s></p><p xml:id="_rnqTgRn"><s xml:id="_rH8MdMx" coords="2,80.70,107.25,450.60,9.69;2,80.70,119.20,299.92,9.69">Hence, to maximally utilize the various information sources to capture the speaker's opinion with a multimodal architecture is a heated topic in multimodal sequence (language) analysis.</s></p><p xml:id="_bSmjYCT"><s xml:id="_Cy6XU9j" coords="2,90.66,131.16,440.64,9.69;2,80.70,143.11,452.14,9.69">In the task of multimodal sequence analysis, two fundamental challenges exist, i.e., to learn the intra-modal dynamics of each modality, and the inter-modal counterpart for capturing cross-modal interactions <ref type="bibr" coords="2,486.38,143.11,15.00,9.69" target="#b20">[21,</ref><ref type="bibr" coords="2,503.88,143.11,11.46,9.69" target="#b58">59,</ref><ref type="bibr" coords="2,517.84,143.11,11.25,9.69" target="#b63">64]</ref>.</s><s xml:id="_zrCn7XM" coords="2,80.40,155.07,450.90,9.69;2,80.70,167.02,235.18,9.69">The former relates to interactions that take place across time steps within one modality, while the latter is associated with interactions between diferent modalities.</s><s xml:id="_GsYwx7b" coords="2,318.35,167.02,212.96,9.69;2,80.70,178.98,450.60,9.69;2,80.70,190.93,256.84,9.69">Previous researches mostly employ recurrent neural network (RNN) and its variants <ref type="bibr" coords="2,208.31,178.98,10.25,9.69" target="#b6">[7,</ref><ref type="bibr" coords="2,221.05,178.98,11.41,9.69" target="#b13">14,</ref><ref type="bibr" coords="2,234.95,178.98,12.74,9.69" target="#b18">19]</ref> to learn these two aspects <ref type="bibr" coords="2,355.96,178.98,14.89,9.69" target="#b27">[28,</ref><ref type="bibr" coords="2,373.33,178.98,11.41,9.69" target="#b53">54,</ref><ref type="bibr" coords="2,387.23,178.98,11.17,9.69" target="#b64">65]</ref>, which, however, are slow in the inference process due to their recurrence in the time dimension.</s><s xml:id="_5zzkzPj" coords="2,340.02,190.93,191.28,9.69;2,80.45,202.89,450.86,9.69;2,80.70,214.84,232.72,9.69">They are also prone to the problems of gradient vanishing and exploding as well as limited capacity of learning long-term dependency <ref type="bibr" coords="2,440.81,202.89,10.58,9.69" target="#b3">[4]</ref>, which adds to the diiculty in learning of intra-and inter-modal dynamics.</s><s xml:id="_njW9rFp" coords="2,315.91,214.84,215.58,9.69;2,80.70,226.80,357.67,9.69">Particularly, it is of great signiicance to learn longer temporal dependency for unaligned sequences because they are often much longer <ref type="bibr" coords="2,419.81,226.80,14.84,9.69" target="#b48">[49]</ref>.</s></p><p xml:id="_fcSVNHA"><s xml:id="_tfFTRuS" coords="2,90.66,238.75,440.64,9.69;2,80.70,250.71,124.51,9.69">Exploring efective approaches to learn inter-modal dynamics has been one primary focus in the research of multimodal sequence analysis.</s><s xml:id="_zCPP5Rk" coords="2,207.70,250.71,323.60,9.69;2,80.70,262.67,95.19,9.69">To this end, a large portion of previous works fuse the three modalities at word level <ref type="bibr" coords="2,101.83,262.67,14.91,9.69" target="#b27">[28,</ref><ref type="bibr" coords="2,119.24,262.67,11.42,9.69" target="#b31">32,</ref><ref type="bibr" coords="2,133.15,262.67,11.42,9.69" target="#b53">54,</ref><ref type="bibr" coords="2,147.06,262.67,11.42,9.69" target="#b64">65,</ref><ref type="bibr" coords="2,160.98,262.67,11.18,9.69" target="#b65">66]</ref>.</s><s xml:id="_Zw6cqCX" coords="2,178.38,262.67,352.92,9.69;2,80.70,274.62,450.61,9.69;2,80.70,286.58,248.67,9.69">However, the interactions between various modalities are usually more complicated and last for longer than one word, i.e., cross-modal interactions may take place among words and the word-level fusion may break a complete interaction into multiple parts.</s><s xml:id="_WVm5uR7" coords="2,331.86,286.58,199.43,9.69;2,80.70,298.53,183.12,9.69">Additionally, word-level fusion requires that the sequences are strictly aligned at word level.</s><s xml:id="_E95ATcb" coords="2,266.76,298.53,264.54,9.69;2,80.70,310.49,217.02,9.69">However, in real-world scenarios, such word-level alignment is time-consuming and computationally expensive <ref type="bibr" coords="2,279.15,310.49,14.85,9.69" target="#b62">[63]</ref>.</s><s xml:id="_4jBUJKj" coords="2,300.20,310.49,231.11,9.69;2,80.70,322.44,450.60,9.69;2,80.70,334.40,98.60,9.69">Therefore, we claim that a fusion strategy should be able to dynamically and automatically associate various time steps from multiple modalities instead of considering fusion at each time step.</s></p><p xml:id="_GPyRT2c"><s xml:id="_jBvVWdu" coords="2,90.66,346.35,440.64,9.69;2,80.40,358.31,282.80,9.69">To address the irst issue, we propose to utilize the high expressiveness of graph convolution networks (GCNs) to model unimodal sequential signals as an alternative to RNN.</s><s xml:id="_ecAaj8b" coords="2,365.64,358.31,165.65,9.69;2,80.70,370.26,450.61,9.69;2,80.70,382.22,49.10,9.69">Recently, GCNs have attracted signiicant attention to model graph structured data, and yielded state-of-the-art performance on a broad variety of tasks <ref type="bibr" coords="2,80.70,382.22,15.12,9.69" target="#b36">[37,</ref><ref type="bibr" coords="2,99.50,382.22,11.50,9.69" target="#b45">46,</ref><ref type="bibr" coords="2,114.68,382.22,11.34,9.69" target="#b55">56]</ref>.</s><s xml:id="_gndyhq6" coords="2,133.48,382.22,397.82,9.69;2,80.70,394.17,451.70,9.69;2,80.33,406.13,275.17,9.69">GCN demonstrates high efectiveness in learning the relevance of nodes via the operation of convolution, and importantly, they dispense with the need for recurrence and can be computed in parallel, which greatly boosts eiciency in inferring time compared to RNN.</s><s xml:id="_zS57XtW" coords="2,358.00,406.13,173.31,9.69;2,80.70,418.08,156.92,9.69">Existing researches on GCN mainly utilize it for modeling graph-structured data.</s><s xml:id="_d3tVPQR" coords="2,240.11,418.08,291.19,9.69;2,80.70,430.04,450.61,9.69;2,80.70,441.99,245.05,9.69">In contrast, in this work, we extend GCN to model sequential data and comparative analysis is conducted to show that GCN exhibits greater efectiveness compared to RNN <ref type="bibr" coords="2,487.27,430.04,14.90,9.69" target="#b36">[37,</ref><ref type="bibr" coords="2,504.65,430.04,11.42,9.69" target="#b45">46,</ref><ref type="bibr" coords="2,518.56,430.04,12.75,9.69" target="#b55">56]</ref> and temporal convolutional network (TCN) variants <ref type="bibr" coords="2,301.89,441.99,10.49,9.69" target="#b0">[1,</ref><ref type="bibr" coords="2,315.26,441.99,6.99,9.69" target="#b1">2]</ref>.</s><s xml:id="_36UpbyU" coords="2,328.63,441.99,202.86,9.69;2,80.70,453.95,450.61,9.69;2,80.70,465.90,177.39,9.69">With the operation of graph convolution, longer temporal dependency can be learnt by viewing each time step as a node and associating the relevant nodes even though they are far apart in time dimension.</s><s xml:id="_GZ7hn4F" coords="2,260.58,465.90,270.73,9.69;2,80.70,477.86,450.60,9.69;2,80.70,489.81,31.57,9.69">Note that graph neural networks are also utilized in previous works to conduct fusion <ref type="bibr" coords="2,151.58,477.86,16.38,9.69" target="#b30">[31,</ref><ref type="bibr" coords="2,170.45,477.86,11.50,9.69" target="#b56">57,</ref><ref type="bibr" coords="2,184.44,477.86,11.33,9.69" target="#b65">66]</ref>, they have major diferences from our model (see Related Work section for more details).</s></p><p xml:id="_mDP64Nt"><s xml:id="_Gua96zU" coords="2,90.66,501.77,440.64,9.69;2,80.70,513.72,274.84,9.69">However, one major obstacle to applying graph convolution to sequential learning is that sequences have no adjacency matrices as graph-structured data conventionally do.</s><s xml:id="_J6VhHsM" coords="2,358.01,513.72,173.29,9.69;2,80.70,525.68,114.86,9.69">Therefore, the deinition and deduction of adjacency matrices is crucial.</s><s xml:id="_A5cmM4R" coords="2,197.54,525.68,333.76,9.69;2,80.70,537.63,128.14,9.69">In this paper, we design multiple ways to achieve this goal, including non-parametric methods and learnable methods.</s><s xml:id="_Nff9rfA" coords="2,210.99,537.63,320.32,9.69;2,80.70,549.59,422.85,9.69">In the non-parametric way, we mainly investigate the efectiveness of a proposed matrix, namely generalized diagonal matrix, which is extremely fast and almost free of computation.</s><s xml:id="_bsSrCQ6" coords="2,506.56,549.59,24.74,9.69;2,80.70,561.54,450.61,9.69;2,80.70,573.50,278.79,9.69">In the learnable way, we automatically learn the adjacency matrix via gradient descent (direct learning) and cross-node attention (indirect learning), which is more powerful and expressive.</s><s xml:id="_2e6rwdW" coords="2,361.98,573.50,169.32,9.69;2,80.70,585.45,169.74,9.69">We present the comparative results of the proposed adjacency matrices in Section 5.</s></p><p xml:id="_gx3Ja2g"><s xml:id="_pCcAaGM" coords="2,90.66,597.41,440.64,9.69;2,80.70,609.36,450.61,9.69;2,80.33,621.32,204.96,9.69">For addressing the second problem, i.e., inter-modal dynamics learning, we elaborately design a graph pooling fusion network (GPFN), which learns to aggregate various nodes from diferent modalities by graph pooling without the need of time alignment information.</s><s xml:id="_tMDKjne" coords="2,288.38,621.32,242.92,9.69;2,80.70,633.28,193.65,9.69">Firstly we analyze the rationality of mean and max graph pooling approaches via mathematical deduction.</s><s xml:id="_BPRvmUK" coords="2,276.84,633.28,254.46,9.69;2,80.70,645.23,327.46,9.69">However, mean and max graph pooling are still subject to some limitations in that they are not learnable and can only fuse neighboring nodes.</s><s xml:id="_3TY8JQf" coords="2,410.84,645.23,120.46,9.69;3,80.70,107.25,450.61,9.69;3,80.70,119.20,282.05,9.69">Hence, to fuse the nodes in a more expressive way, we further design a pooling strategy, termed link similarity pooling, which considers the association scores to the common neighbors of each two nodes.</s><s xml:id="_gZnP5wn" coords="3,365.59,119.20,165.71,9.69;3,80.70,131.16,450.61,9.69;3,80.70,143.11,414.50,9.69">This learnable approach is based on the following two assumptions, i.e., (i) two nodes are closely related if their neighbors signiicantly overlap and thus can be fused; and (ii) provided the two nodes are neighbors, they are integrable with a high possibility.</s><s xml:id="_VhTGvXD" coords="3,497.69,143.11,33.87,9.69;3,80.70,155.07,450.60,9.69;3,80.70,167.02,136.09,9.69">The link similarity pooling automatically updates the adjacency matrix and node embeddings for learning a high-level and reined graph representation.</s></p><p xml:id="_Jd4jzxs"><s xml:id="_Ydzxgdj" coords="3,90.66,178.98,440.64,9.69;3,80.70,190.93,72.19,9.69">In conclusion, we propose a brand-new architecture named Multimodal Graph to address the sequential learning problem.</s><s xml:id="_vC43BAT" coords="3,155.38,190.93,220.74,9.69">The contributions of this paper can be summarized as:</s></p><p xml:id="_tJyGn5f"><s xml:id="_kxgm24j" coords="3,96.17,206.99,436.82,9.69;3,105.11,218.95,273.03,9.69">• We propose a novel graph-based architecture to model multimodal sequences, which innovatively transforms the sequential learning problem into a graph learning problem.</s><s xml:id="_U8c8krE" coords="3,380.18,218.95,151.13,9.69;3,105.11,230.90,427.88,9.69;3,105.11,242.86,230.58,9.69">Speciically, we design three unimodal GCNs to explore intra-modal dynamics, and a graph pooling fusion network to explore cross-modal interactions and fuse various nodes from diferent modalities.</s><s xml:id="_rNrz5y5" coords="3,96.17,254.81,435.13,9.69;3,105.11,266.77,138.85,9.69">• We propose multiple approaches to deine adjacency matrices for sequential data, which can be extended to other sequential learning tasks.</s><s xml:id="_h5X2CHx" coords="3,246.44,266.77,284.86,9.69;3,105.11,278.72,426.20,9.69;3,105.11,290.68,74.70,9.69">We compare the performance of diferent kinds of adjacency matrices empirically, and the visualization of the adjacency matrices is also provided to give insight on multimodal sequence analysis.</s><s xml:id="_7HYbwSJ" coords="3,96.17,302.63,435.39,9.69;3,105.11,314.59,426.20,9.69;3,105.11,326.54,134.91,9.69">• In GPFN, we investigate mean/max pooling and link similarity pooling to cluster the nodes hierarchically and thus learn high-level and reined graph representation, which dispense with the need of time alignment information for modality fusion.</s><s xml:id="_FkGpGVt" coords="3,242.77,326.54,288.54,9.69;3,105.11,338.50,426.20,9.69;3,105.11,350.45,105.48,9.69">Specially, the proposed link similarity pooling considers the common neighbors of each two nodes (the topology information in the adjacency matrix) to better fuse the nodes from diferent modalities.</s><s xml:id="_5ZTwq56" coords="3,213.10,350.45,318.21,9.69;3,105.11,362.41,208.29,9.69">To the best of our knowledge, we are the irst to leverage the power of graph pooling to conduct fusion in a hierarchical manner.</s><s xml:id="_Ngns7hS" coords="3,96.17,374.36,401.04,9.69">• We show that the Multimodal Graph outperforms other methods on three widely-used datasets.</s><s xml:id="_ae9hfhP" coords="3,499.70,374.36,32.70,9.69;3,105.11,386.32,426.20,9.69;3,105.11,398.27,393.48,9.69">Besides, the contrastive experiments against RNN and TCN variants demonstrate the efectiveness of GCN on modeling sequence, which indicate a novel approach in the research of sequence modeling tasks.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_km88M7g">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1" xml:id="_nTDERdJ">Multimodal Sequence Analysis</head><p xml:id="_fw8mJ7n"><s xml:id="_NEYYb6F" coords="3,80.70,452.99,411.72,9.69">Multimodal sequence analysis has attracted signiicant research interest in recent years <ref type="bibr" coords="3,449.47,452.99,10.49,9.69" target="#b2">[3,</ref><ref type="bibr" coords="3,462.88,452.99,11.50,9.69" target="#b64">65,</ref><ref type="bibr" coords="3,477.30,452.99,11.34,9.69" target="#b69">70]</ref>.</s><s xml:id="_dscer6E" coords="3,495.35,452.99,35.95,9.69;3,80.33,464.95,337.16,9.69">Previous works focus on designing various fusion strategies to explore inter-modal dynamics.</s><s xml:id="_kbuhnv5" coords="3,419.97,464.95,111.33,9.69;3,80.70,476.90,450.79,9.69;3,80.70,488.86,112.61,9.69">One of the simplest ways to explore inter-modal dynamics is to concatenate features at input feature level, which shows improvement over single modality <ref type="bibr" coords="3,146.68,488.86,15.08,9.69" target="#b43">[44,</ref><ref type="bibr" coords="3,164.25,488.86,11.49,9.69" target="#b44">45,</ref><ref type="bibr" coords="3,178.23,488.86,11.31,9.69" target="#b54">55]</ref>.</s><s xml:id="_sNw5YU4" coords="3,195.80,488.86,335.50,9.69;3,80.70,500.81,399.24,9.69">In contrast, a large number of publications irstly infer decision according to each modality and combine the decisions from all modalities using some voting mechanisms <ref type="bibr" coords="3,433.66,500.81,14.93,9.69" target="#b22">[23,</ref><ref type="bibr" coords="3,451.09,500.81,11.43,9.69" target="#b38">39,</ref><ref type="bibr" coords="3,465.01,500.81,11.19,9.69" target="#b66">67]</ref>.</s><s xml:id="_dCXsKqA" coords="3,482.43,500.81,48.87,9.69;3,80.70,512.77,436.48,9.69">However, as elaborated by Zadeh et al. <ref type="bibr" coords="3,188.23,512.77,14.84,9.69" target="#b63">[64]</ref>, these two types of methods cannot efectively model inter-modal dynamics.</s><s xml:id="_pvj4mk7" coords="3,90.66,524.72,340.15,9.69">Consequently, more advanced fusion strategies are proposed in the past few years.</s><s xml:id="_uNnHEHP" coords="3,433.29,524.72,98.02,9.69;3,80.70,536.68,281.00,9.69">Speciically, performing tensor-based fusion has received much attention <ref type="bibr" coords="3,297.88,536.68,15.12,9.69" target="#b19">[20,</ref><ref type="bibr" coords="3,316.52,536.68,11.50,9.69" target="#b26">27,</ref><ref type="bibr" coords="3,331.55,536.68,11.50,9.69" target="#b28">29,</ref><ref type="bibr" coords="3,346.58,536.68,11.34,9.69" target="#b32">33]</ref>.</s><s xml:id="_z8ETWVR" coords="3,365.23,536.68,166.07,9.69;3,80.70,548.63,452.14,9.69">Tensor Fusion Network (TFN) <ref type="bibr" coords="3,495.98,536.68,16.50,9.69" target="#b63">[64]</ref> and Low-rank Modality Fusion (LMF) <ref type="bibr" coords="3,223.60,548.63,16.50,9.69" target="#b28">[29]</ref> adopt outer product to learn joint representation of three modalities.</s><s xml:id="_qrJhZU5" coords="3,80.70,560.59,450.61,9.69;3,80.70,572.54,31.08,9.69">More recently, Mai et al. <ref type="bibr" coords="3,185.16,560.59,16.50,9.69" target="#b29">[30]</ref> propose a 'Divide, Conquer and Combine' strategy to conduct local and global fusions.</s><s xml:id="_64sbXxS" coords="3,114.26,572.54,313.85,9.69">Interpretable multimodal fusion has also received high attention recently.</s><s xml:id="_wHseF62" coords="3,430.60,572.54,100.70,9.69;3,80.70,584.50,450.80,9.69;3,80.70,596.45,371.64,9.69">For example, Multimodal Routing <ref type="bibr" coords="3,115.83,584.50,16.42,9.69" target="#b49">[50]</ref> applies routing mechanism in capsule network to discover which interactions are importance for predicting each emotion, and Li et al. <ref type="bibr" coords="3,232.63,596.45,16.28,9.69" target="#b25">[26]</ref> address interpretable issue using quantum theory.</s><s xml:id="_sb5rCyg" coords="3,454.84,596.45,76.46,9.69;3,80.70,608.41,450.86,9.69;3,80.70,620.36,192.29,9.69">Furthermore, some translation methods such as Multimodal Transformer (MulT) <ref type="bibr" coords="3,344.70,608.41,16.50,9.69" target="#b48">[49]</ref> aim at learning a joint representation by translating source modality into target modality.</s><s xml:id="_8Exq8S6" coords="3,275.22,620.36,256.08,9.69;3,80.70,632.32,272.15,9.69">For graph-based methods, Graph-MFN <ref type="bibr" coords="3,442.00,620.36,16.22,9.69" target="#b65">[66]</ref> and Graph Fusion Network (GFN) <ref type="bibr" coords="3,147.60,632.32,16.50,9.69" target="#b30">[31]</ref> apply graph neural network to fuse features.</s><s xml:id="_b45e3NF" coords="3,355.59,632.32,175.71,9.69;3,80.70,644.27,273.40,9.69">Although graph neural network is used in these methods, the proposed model signiicantly difers from them.</s><s xml:id="_W7z6pCQ" coords="3,356.57,644.27,174.72,9.69;4,80.70,107.25,450.86,9.69;4,80.70,119.20,287.24,9.69">Firstly, they do not fuse features across the time dimension and merely regards each modality as one node, whereas we view each time step in each modality as one node and perform graph convolution over the node embeddings.</s><s xml:id="_MBpAabW" coords="4,370.44,119.20,160.86,9.69;4,80.70,131.16,450.60,9.69;4,80.70,143.11,158.79,9.69">Secondly, we employ graph convolution and graph pooling for fusion as well as propose multiple adjacency matrices for sequence learning, which are not involved in Graph-MFN and GFN.</s><s xml:id="_mVzK3G7" coords="4,242.34,143.11,288.96,9.69;4,80.70,155.07,450.60,9.69;4,80.70,167.02,339.52,9.69">More recently, Modal-Temporal Attention Graph (MTAG) <ref type="bibr" coords="4,479.17,143.11,20.14,9.69" target="#b56">[57]</ref> applies graph convolutional model to analyze multimodal sequential data, which uses dynamic pruning and read-out technique to explore cross-modal interactions and obtain multimodal embedding.</s><s xml:id="_gXDJQ5g" coords="4,423.02,167.02,108.29,9.69;4,80.70,178.98,450.60,9.69;4,80.70,190.93,33.99,9.69">However, MTAG does not comprehensively deine adjacency matrix for sequences, and simply uses average operation to perform graph readout.</s><s xml:id="_kqGJS5c" coords="4,117.56,190.93,413.74,9.69;4,80.70,202.89,450.80,9.69;4,80.70,214.84,43.36,9.69">In comparison, Multimodal Graph proposes novel graph pooling algorithm to fuse nodes and learn high-level graph representation hierarchically, and provides multiple ways to deine the adjacency matrix for sequences.</s></p><p xml:id="_MXs5QR2"><s xml:id="_RYAGX2J" coords="4,90.66,226.80,440.64,9.69;4,80.70,238.75,223.64,9.69">To avoid sarcasm and ambiguity, many methods learn cross-modal interactions at word level such that various modalities are aligned at time dimension <ref type="bibr" coords="4,248.56,238.75,10.37,9.69" target="#b5">[6,</ref><ref type="bibr" coords="4,261.42,238.75,11.46,9.69" target="#b27">28,</ref><ref type="bibr" coords="4,275.38,238.75,11.46,9.69" target="#b31">32,</ref><ref type="bibr" coords="4,289.33,238.75,11.25,9.69" target="#b33">34]</ref>.</s><s xml:id="_jZtfCvM" coords="4,306.83,238.75,224.48,9.69;4,80.70,250.71,450.61,9.69;4,80.70,262.67,236.09,9.69">For instance, Memory Fusion Network (MFN) <ref type="bibr" coords="4,494.93,238.75,16.36,9.69" target="#b64">[65]</ref> uses systems of LSTM to learn intra-modal dynamics, and it implements delta-memory attention and multi-view gated memory network to fuse memories of LSTMs across time.</s><s xml:id="_kCtEtp5" coords="4,319.27,262.67,212.62,9.69;4,80.70,274.62,450.61,9.69;4,80.70,286.58,107.53,9.69">In addition, Multi-Fusion Residual Memory (MFRM) <ref type="bibr" coords="4,80.70,274.62,16.25,9.69" target="#b31">[32]</ref> applies multi-stage fusion to fuse the three modalities, and it designs a residual memory network to capture the long-term dependency.</s><s xml:id="_vafDDuk" coords="4,190.72,286.58,342.12,9.69">However, in unaligned multimodal sequence, word-level fusion cannot be performed.</s></p><p xml:id="_XV2dZtw"><s xml:id="_BsXUMWx" coords="4,90.66,298.53,440.65,9.69;4,80.70,310.49,340.65,9.69">A clear distinction of the majority of these previous methods between our Multimodal Graph is that we do not apply any RNN or TCN variants to learn intra-modal and inter-modal dynamics.</s><s xml:id="_AtxrrWF" coords="4,423.84,310.49,107.47,9.69;4,80.70,322.44,355.85,9.69">Instead, we investigate the efectiveness of graph convolution and graph pooling on modeling multimodal sequence.</s><s xml:id="_qmd5EBq" coords="4,439.02,322.44,92.29,9.69;4,80.70,334.40,450.60,9.69;4,80.70,346.35,284.04,9.69">Our Multimodal Graph is very elegant and efective, which can efectively learn longer temporal dependency by directly associating the distant related nodes and allow parallel computing at time dimension.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" xml:id="_nVxjGK3">Graph Neural Networks (GNNs)</head><p xml:id="_auYJjy3"><s xml:id="_vvE9fFf" coords="4,80.35,383.72,450.95,10.55;4,80.70,395.81,87.83,9.69">A graph can be denoted as G = (N , E), where N = {n 1 , n 2 , ..., n T } is a set of nodes, and E ⊆ N × N refers to the set of observed edges.</s><s xml:id="_FpFkKcb" coords="4,171.03,394.43,210.33,11.08;4,385.09,395.81,146.48,9.69;4,80.70,407.77,229.59,9.69">The set of node embeddings is denoted as N ∈ R T ×d where d refers to the dimensionality of each node embedding and T is the number of nodes.</s><s xml:id="_74sagbf" coords="4,313.04,407.77,218.52,9.69;4,80.70,418.34,68.08,11.08;4,150.47,419.72,2.24,9.69">The edges can also be described using the adjacency matrix A ∈ R T ×T .</s><s xml:id="_wJGn3nj" coords="4,155.22,419.72,377.62,10.55">Each element of A is non-negative and A i, j = 0 means that nodes i and j are not connected.</s><s xml:id="_ZfyF2yx" coords="4,80.35,431.68,337.73,9.69">A wide variety of GNNs have been proposed in the last few years <ref type="bibr" coords="4,356.62,431.68,15.12,9.69" target="#b10">[11,</ref><ref type="bibr" coords="4,374.48,431.68,11.50,9.69" target="#b36">37,</ref><ref type="bibr" coords="4,388.72,431.68,11.50,9.69" target="#b45">46,</ref><ref type="bibr" coords="4,402.96,431.68,11.34,9.69" target="#b55">56]</ref>.</s><s xml:id="_AmRBQDP" coords="4,420.82,431.68,110.48,9.69;4,80.70,443.63,71.83,9.69">We mainly focus on GCNs <ref type="bibr" coords="4,80.70,443.63,16.37,9.69" target="#b24">[25]</ref> in this paper.</s><s xml:id="_yTGh4WN" coords="4,155.02,443.63,376.28,9.69;4,80.70,455.59,450.86,9.69;4,80.70,467.54,418.59,9.69">GCNs have become increasingly popular recently due to its applicability to graph structured data and yielded state-of-the-art performance on a variety of learning tasks, such as node classiication <ref type="bibr" coords="4,495.11,455.59,14.72,9.69" target="#b15">[16]</ref>, link prediction <ref type="bibr" coords="4,125.12,467.54,15.05,9.69" target="#b35">[36,</ref><ref type="bibr" coords="4,142.67,467.54,11.29,9.69" target="#b67">68]</ref>, image retagging <ref type="bibr" coords="4,229.10,467.54,14.90,9.69" target="#b47">[48]</ref>, group activity recognition <ref type="bibr" coords="4,359.98,467.54,14.90,9.69" target="#b46">[47]</ref>, and graph classiication <ref type="bibr" coords="4,480.66,467.54,14.90,9.69" target="#b68">[69]</ref>.</s><s xml:id="_D2DJFWd" coords="4,501.79,467.54,29.51,9.69;4,80.70,479.63,317.68,9.69">GCN is basically a convolutional operation on the nodes that are directly connected.</s><s xml:id="_rQ3WSHt" coords="4,400.99,479.63,23.12,9.69;4,424.78,478.25,7.37,6.82;4,435.70,479.63,95.60,9.69;4,80.70,491.59,103.28,9.69">The k t h iteration of the general GCN can be described as:</s></p><formula xml:id="formula_0" coords="4,221.54,502.73,309.76,12.43">(a i ) k -1 = AGGREGATE((N j ) k -1 ; j ∈ η(i))<label>(1)</label></formula><formula xml:id="formula_1" coords="4,231.44,520.11,299.86,12.43">(N i ) k = COMBINE((N i ) k -1 , (a i ) k -1 )<label>(2)</label></formula><p xml:id="_3eQrdE4"><s xml:id="_a57dCAr" coords="4,80.33,535.72,50.95,11.93;4,135.51,537.11,396.88,9.69;4,80.70,549.06,450.60,9.69;4,80.70,559.77,91.83,11.93;4,173.03,561.15,358.28,9.69;4,80.70,571.86,79.55,11.93;4,163.24,573.25,142.14,9.69">where (N i ) k is the embedding for node i at iteration k, η(i) represents the set of 1-hop neighbors of node i, the AGGREGATE function aggregates information from 1-hop neighbors of node i and output the aggregation representation (a i ) k -1 , and the COMBINE function combines the information of node i and its aggregation information (a i ) k -1 to update the embedding of node i.</s><s xml:id="_4aAhBVY" coords="4,90.66,585.20,394.18,9.69">Among all GCNs, those trying to learn or recover adjacency matrices are related to our method.</s><s xml:id="_Hrzpr3E" coords="4,487.32,585.20,43.98,9.69;4,80.70,597.16,450.61,9.69;4,80.70,609.11,171.45,9.69">Franceschi et al. <ref type="bibr" coords="4,103.54,597.16,16.50,9.69" target="#b9">[10]</ref> use bi-level program to irst sample adjacency matrix and then learn the parameters for the graph by minimizing inner and outer objectives.</s><s xml:id="_qghYgmz" coords="4,254.64,609.11,276.66,9.69;4,80.70,621.07,450.60,9.69;4,80.70,633.02,32.77,9.69">In contrast, in the direct learning way, we directly parameterize the adjacency matrix as a learnable matrix and jointly learn the adjacency matrix and graph parameters via gradient descent.</s><s xml:id="_Hxx8zZw" coords="4,115.96,633.02,255.66,9.69">Also, we provide multiple ways to deine the adjacency matrix.</s><s xml:id="_VCm5AAk" coords="4,374.11,633.02,157.19,9.69;4,80.70,644.98,452.14,9.69">As for graph pooling, the DifPool <ref type="bibr" coords="4,514.95,633.02,16.35,9.69" target="#b59">[60]</ref> learns a diferentiable soft cluster strategy for nodes using node embedding, mapping nodes to a set of clusters.</s><s xml:id="_5wGBKAp" coords="5,80.70,342.88,450.79,9.69;5,80.70,354.84,80.03,9.69">In contrast, we utilize the adjacency matrix to learn the cluster assignment matrix, which considers the neighbor similarity of nodes.</s><s xml:id="_QEpGGXs" coords="5,163.42,354.84,368.13,9.69;5,80.70,366.79,149.42,9.69">Compared to using node embedding to learn cluster assignment matrix, using adjacency matrix is more intuitive and simple.</s><s xml:id="_XJ5ySKx" coords="5,232.94,366.79,298.55,9.69;5,80.70,378.75,450.61,9.69;5,80.70,390.70,386.31,9.69">StructPool <ref type="bibr" coords="5,275.71,366.79,17.11,9.69" target="#b61">[62]</ref> uses conditional random ields to capture the high-order structural relationships among diferent nodes to learn a node cluster assignment matrix based on the node features, where the adjacency matrix is used to ind the topological information of the graph.</s><s xml:id="_pgHx7Hh" coords="5,469.57,390.70,61.73,9.69;5,80.70,402.66,446.23,9.69">In contrast, we directly utilize the link (neighbor) information in the adjacency matrix to learn the cluster assignment matrix.</s></p><p xml:id="_cp9ePXy"><s xml:id="_JX38TmY" coords="5,90.66,414.61,352.46,9.69">There also exist several works that try to apply GNNs to multimodal learning tasks.</s><s xml:id="_mTrngDP" coords="5,446.08,414.61,85.48,9.69;5,80.70,426.57,450.61,9.69;5,80.70,438.52,231.48,9.69">Gao et al. <ref type="bibr" coords="5,489.00,414.61,16.50,9.69" target="#b11">[12]</ref> apply fully-connected visual, semantic and numeric graphs to represent an image and conduct message passing between the graphs to utilize the contexts in various modalities.</s><s xml:id="_FcPRRnX" coords="5,315.17,438.52,216.13,9.69;5,80.70,450.48,106.54,9.69">Nevertheless, the graph here is fully-connected and only represents an image.</s><s xml:id="_zynHCFD" coords="5,189.80,450.48,341.50,9.69;5,80.50,462.43,241.12,9.69">Misras et al. <ref type="bibr" coords="5,242.46,450.48,16.50,9.69" target="#b37">[38]</ref> connect every image with its corresponding tags and the image's K-nearest neighbors to capture visual-semantic relationship.</s><s xml:id="_Uux3qwn" coords="5,324.00,462.43,207.30,9.69;5,80.70,474.39,152.73,9.69">Similarly, Wang et al. <ref type="bibr" coords="5,411.10,462.43,16.21,9.69" target="#b52">[53]</ref> learn a sparse multigraph construction from multimodal images.</s><s xml:id="_KsjKkph" coords="5,235.89,474.39,267.14,9.69">Nevertheless, these graphs are not targeted for sequential learning.</s><s xml:id="_NXfrE3z" coords="5,505.49,474.39,27.35,9.69;5,80.70,486.34,450.61,9.69;5,80.70,498.30,254.90,9.69">Ji et al. <ref type="bibr" coords="5,80.70,486.34,16.23,9.69" target="#b21">[22]</ref> propose a cross-modal hypergraph to capture the noisy correlation among heterogeneous modalities, where the relevance of the nodes is calculated by Euclidean distance.</s><s xml:id="_4hrMc7c" coords="5,338.08,498.30,193.22,9.69;5,80.70,510.25,450.60,9.69;5,80.70,522.21,91.91,9.69">In contrast, we propose multiple approaches to explore the relevance between nodes in the graph, and introduce graph pooling to learn reined representations for multimodal graph.</s><s xml:id="_Cd6cv5M" coords="5,175.43,522.21,355.88,9.69;5,80.70,534.16,450.60,9.69;5,80.70,546.12,450.61,9.69;5,80.70,558.07,450.61,9.69;5,80.70,570.03,450.61,9.69;5,80.70,581.98,229.41,9.69">Generally speaking, none of these approaches are similar to our work in terms of the main contributions: 1) we propose multiple ways to deine adjacency matrix for sequences and transform the sequential learning problem into a graph learning problem; 2) we propose new graph pooling algorithms to fuse the unimodal nodes and thus learn high-level and reined graph representations; 3) we conduct extensive experiments to verify that graph-based structure can outperform popular sequential models, which indicates a new methodology in the research of sequential learning.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_zty8Xwk">MODEL ARCHITECTURE</head><p xml:id="_89m76v7"><s xml:id="_uR7te5r" coords="5,80.70,620.50,450.61,9.69;5,80.70,632.45,450.61,9.69;5,80.70,644.41,142.93,9.69">In this section, we describe Multimodal Graph in detail, with its diagram illustrated in Fig. <ref type="figure" coords="5,442.74,620.50,4.54,9.69" target="#fig_0">1</ref> and the procedure is summarized in Algorithm 1. Multimodal Graph is hierarchically structured to cater to two stages, i.e., intra-modal and inter-modal dynamics learning.</s><s xml:id="_9RE84sj" coords="5,226.12,644.41,305.78,9.69">In the irst stage, we propose to employ a graph convolution network (GCN)</s></p><formula xml:id="formula_2" coords="6,80.37,109.99,317.51,23.65">Algorithm 1: Procedure for Multimodal Graph Input: Raw sequences of unimodality N a ∈ R T a ×d a , N v ∈ R T v ×d v , and N l ∈ R T l ×d l .</formula><p xml:id="_BhFdejx"><s xml:id="_AR7FkyX" coords="6,80.70,135.87,127.92,8.72">Output: The sentiment prediction.</s><s xml:id="_8ZUk7mv" coords="6,80.70,147.52,85.69,8.03">In Unimodal Graphs:</s></p><p xml:id="_2mruPu7"><s xml:id="_YjB2hB2" coords="6,94.15,157.79,79.41,8.72;6,105.36,168.75,137.37,8.72">For each modality do: Compute adjacency matrix by Eq. ( <ref type="formula" coords="6,234.11,168.75,2.87,8.72" target="#formula_5">3</ref>).</s><s xml:id="_E5bwaby" coords="6,105.36,179.71,154.45,8.72">Perform graph convolution by Eqs. ( <ref type="formula" coords="6,238.77,179.71,3.00,8.72" target="#formula_7">4</ref>)- <ref type="bibr" coords="6,247.79,179.71,9.01,8.72" target="#b6">(7)</ref>.</s><s xml:id="_b9dCUVQ" coords="6,94.15,190.67,14.39,8.72;6,80.70,202.32,36.94,8.03">End In GPFN:</s></p><p xml:id="_VQcF2Yh"><s xml:id="_VSnJQKN" coords="6,94.15,212.59,262.69,8.72">Perform node sorting to obtain multimodal sequence as in Section 3.3.1.</s><s xml:id="_qHewrpP" coords="6,94.15,223.55,220.95,8.72">Deine adjacency matrix for multimodal sequence by Eq. ( <ref type="formula" coords="6,306.48,223.55,2.87,8.72" target="#formula_5">3</ref>).</s><s xml:id="_QV7pSjR" coords="6,94.15,234.50,212.48,8.72">Perform graph convolution and graph pooling as in Fig. <ref type="figure" coords="6,300.48,234.50,3.07,8.72" target="#fig_1">2</ref>.</s></p><p xml:id="_EP3uv7f"><s xml:id="_DNfPfaF" coords="6,94.15,245.46,173.12,8.72">Obtain graph representation as in Section 3.3.4.</s><s xml:id="_Y3jRe5Q" coords="6,94.15,256.42,138.76,8.72">Perform prediction as in Section 3.3.4.</s></p><p xml:id="_KKGVuaB"><s xml:id="_HX9xY94" coords="6,80.70,293.61,181.86,9.69">for each modality, termed as Unimodal Graph.</s><s xml:id="_yUF3WVF" coords="6,264.61,293.61,266.88,9.69;6,80.70,305.56,142.06,9.69">In the second, a graph pooling fusion network (GPFN) is devised for capturing cross-modal interactions.</s><s xml:id="_n6N6ycu" coords="6,225.25,305.56,306.05,9.69;6,80.70,317.52,450.80,9.69;6,80.70,329.47,184.89,9.69">As an early attempt to employ GCN for sequential learning, the architecture of Multimodal Graph is free of RNNs which are commonly used for sequential learning but subject to a number of limitations such as slowing inferring speed.</s><s xml:id="_m6wUsTb" coords="6,268.10,329.47,263.21,9.69;6,80.70,341.43,372.16,9.69">Extensive discussion on applying this graph-oriented approach to sequential data is also provided in this section, including the deinition of adjacency matrices.</s><s xml:id="_NHFUG4m" coords="6,455.25,341.43,76.06,9.69;6,80.70,353.38,450.60,9.69;6,80.70,365.34,202.37,9.69">Moreover, with the proposed graph pooling algorithms in GPFN, our model can fuse the unaligned unimodal sequences and learn high-level and reined multimodal representation.</s><s xml:id="_uM9gsFq" coords="6,90.66,521.98,324.65,9.69">Our downstream task is multimodal sentiment analysis and emotion recognition.</s><s xml:id="_9KkJ2uE" coords="6,417.79,521.98,113.51,9.69;6,80.70,533.93,305.87,9.69">The input to the model is an utterance <ref type="bibr" coords="6,120.60,533.93,14.69,9.69" target="#b39">[40]</ref>, which is a segment of a video bounded by pauses and breaths.</s><s xml:id="_dZAnZCA" coords="6,388.83,533.93,143.57,9.69;6,80.70,545.89,184.99,9.69">Each utterance has three modalities, i.e., acoustic (a), visual (v), and language (l).</s><s xml:id="_Rh64rUa" coords="6,268.78,545.89,262.52,9.69;6,80.70,558.29,44.47,9.69">The sequences of acoustic, visual, and language modalities are denoted as</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_RFnnVvj">Notations and Task Definition</head><formula xml:id="formula_3" coords="6,127.80,556.91,116.26,11.08">N a ∈ R T a ×d a , N v ∈ R T v ×d v ,</formula><p xml:id="_jqeUtnG"><s xml:id="_VPd5ftD" coords="6,246.64,558.29,25.32,9.69;6,273.20,557.21,2.47,4.87;6,280.45,556.91,32.34,10.36;6,314.47,558.29,56.37,9.69">and N l ∈ R T l ×d l , respectively.</s><s xml:id="_aWk4tju" coords="6,373.43,558.29,157.88,9.69;6,80.70,570.25,246.26,9.69">We aim to predict the sentiment score or emotion of the utterance using the unimodal sequences.</s><s xml:id="_mMUaHhk" coords="6,329.83,570.25,201.48,9.69;6,80.70,582.20,191.13,9.69">We summarize the frequently-used symbols and abbreviations in Table <ref type="table" coords="6,173.02,582.20,4.63,9.69" target="#tab_0">1</ref> and 2 for convenience.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_KhYPeq2">Unimodal Graphs</head><p xml:id="_S7pf697"><s xml:id="_a2CEd6s" coords="6,80.40,620.57,450.91,9.69;6,80.70,632.52,135.72,9.69">To address the irst challenge, i.e., learn intra-modal interactions, we utilize three unimodal GCNs on unimodal sequences, each for one modality.</s><s xml:id="_R6ybQh9" coords="6,218.89,632.52,312.41,9.69;6,80.70,644.48,176.05,9.69">Intra-modal interactions are essential in multimodal language analysis which involves the modality-speciic information.</s><s xml:id="_rXffc6x" coords="6,259.24,644.48,272.07,9.69;7,80.70,107.25,407.43,9.69">The majority of prior works leverage RNNs to explore intra-modal dynamics <ref type="bibr" coords="7,122.65,107.25,15.10,9.69" target="#b27">[28,</ref><ref type="bibr" coords="7,140.24,107.25,11.49,9.69" target="#b31">32,</ref><ref type="bibr" coords="7,154.22,107.25,11.49,9.69" target="#b53">54,</ref><ref type="bibr" coords="7,168.21,107.25,11.32,9.69" target="#b64">65]</ref>, which are prone to some limitations as stated in the Introduction section.</s><s xml:id="_dWd8MEc" coords="7,490.61,107.25,41.78,9.69;7,80.40,119.20,451.16,9.69;7,80.70,131.16,74.96,9.69">Moreover, TCNs and RNNs use ixed patterns to model sequence, which are not lexible as they cannot automatically link related time steps.</s><s xml:id="_bA7chfM" coords="7,158.15,131.16,373.15,9.69;7,80.70,143.11,37.27,9.69">In contrast, we propose to use GCNs to learn unimodal high-level representations for each modality.</s><s xml:id="_3SNvRmy" coords="7,120.45,143.11,410.85,9.69;7,80.70,155.07,411.09,9.69">With a suitable adjacency matrix, GCNs enable parallel computation in the time dimension and is able to learn long-term temporal contextual information by identifying and connecting related time steps.</s></p><p xml:id="_YzH2rVs"><s xml:id="_x4uqtFG" coords="7,90.66,167.16,228.81,9.69">We deine the acoustic, visual, and language graphs as</s></p><formula xml:id="formula_4" coords="7,322.02,165.78,127.77,11.20">G a = (N a , E a ), G v = (N v , E v )</formula><p xml:id="_wp4s4yT"><s xml:id="_rdp9PZE" coords="7,449.79,167.16,30.32,9.69;7,480.61,165.78,2.12,6.82;7,488.06,165.78,34.58,11.20;7,523.03,165.78,9.37,10.74;7,80.70,179.11,51.55,9.69">, and G l = (N l , E l ), respectively.</s><s xml:id="_nsQPm3G" coords="7,135.11,179.11,182.26,9.69;7,318.71,177.73,3.89,6.82;7,327.13,177.73,22.59,11.20;7,345.94,184.28,3.38,7.08;7,350.55,179.48,8.91,9.32;7,359.71,177.73,3.89,6.82;7,359.82,184.28,3.38,7.08;7,364.43,179.48,21.42,9.32;7,386.10,177.73,3.89,6.82;7,385.80,184.79,7.62,6.82;7,395.05,179.11,120.41,9.69;7,515.95,177.73,15.05,10.36;7,80.75,191.96,6.64,9.33;7,88.72,190.20,22.46,11.08;7,112.51,190.20,3.89,6.82;7,119.93,191.59,275.52,9.69">Taking the acoustic graph as an example, N a = {n a 1 , n a 2 , ..., n a T a } is a set of acoustic nodes, E a ⊆ N a × N a refers to the set of edges that directly connects the acoustic nodes.</s><s xml:id="_NEt2p8s" coords="7,398.04,191.59,133.26,9.69;7,80.70,203.68,53.49,9.69;7,135.52,202.60,4.14,4.87;7,143.69,202.30,35.04,10.36;7,180.10,203.68,156.01,9.69">The acoustic node embedding is denoted as N a ∈ R T a ×d a , which is the input acoustic sequence.</s></p><p xml:id="_tQdMfFt"><s xml:id="_4Sun3nP" coords="7,90.66,215.64,440.64,9.69;7,80.70,227.59,79.11,9.69">However, unlike graph-structured data, a unimodal sequence does not have an adjacency matrix that determines the graph topology.</s><s xml:id="_32Sz74M" coords="7,162.30,227.59,369.00,9.69;7,80.70,239.55,282.39,9.69">Hence, one major problem is to deine the adjacency matrix in the unimodal sequence such that it efectively relects the connection between nodes (time steps).</s><s xml:id="_DzshYNq" coords="7,365.58,239.55,165.72,9.69;7,80.70,251.50,247.60,9.69">Intuitively, two nodes are assumed to be connected if they are close in terms of the feature embedding.</s><s xml:id="_wBK5AQF" coords="7,330.78,251.50,200.52,9.69;7,80.70,263.46,314.20,9.69">Therefore, we can measure the similarity between the embeddings of each two nodes to determine whether they are neighbors.</s><s xml:id="_f6HhX4P" coords="7,397.40,263.46,133.90,9.69;7,80.70,275.41,450.79,9.69;7,80.70,287.37,84.42,9.69">Here we use a simple cross-node attention mechanism to determine the correlation between nodes and thus deine the adjacency matrix for unimodal sequences.</s><s xml:id="_t3fJWDm" coords="7,167.61,287.37,300.15,9.69">The equations are shown below (taking acoustic modality as an example):</s></p><formula xml:id="formula_5" coords="7,197.35,300.58,333.95,32.06">Âa = f [f (QW a 1 ) f ((PW a 2 ) R )], A a i, j = Âa i, j v ∈N Âa i,v + ϵ<label>(3)</label></formula><p xml:id="_mz6Vu8h"><s xml:id="_pkvffRQ" coords="7,80.33,337.37,25.84,9.69">where</s></p><formula xml:id="formula_6" coords="7,108.27,335.98,412.57,13.62">Q = f (N a W a q ) ∈ R T a ×d , P = f (N a W a p ) ∈ R T a ×d , and W a 1 ∈ R d ×d ,W a 2 ∈ R d ×d ,W a q ∈ R d a ×d ,W a p ∈ R d a</formula><p xml:id="_Qj6xuXs"><s xml:id="_ewC6frX" coords="7,521.72,336.12,8.36,6.16;7,80.70,349.47,94.63,9.69">×d are learnable matrices.</s><s xml:id="_TMx3BV4" coords="7,179.22,349.47,352.08,9.69;7,80.70,361.42,237.75,9.69">f is the nonlinear activation function to increase the nonlinear expressive power of the model and R denotes the matrix transpose operation.</s><s xml:id="_JjpYzqd" coords="7,321.30,361.42,210.00,9.69;7,80.70,373.38,247.59,9.69">In the learned adjacency matrix, the values can be interpreted as the intensity of interactions between nodes.</s><s xml:id="_FA9Wrd7" coords="7,331.66,373.38,199.64,9.69;7,80.70,385.33,211.78,9.69">Therefore, the negative links relect little or no interaction between the corresponding two nodes.</s><s xml:id="_SHxeZ28" coords="7,295.65,385.33,235.66,9.69;7,80.70,397.29,237.92,9.69">We apply ReLU as our activation function such that the negative links between nodes can be efectively iltered out.</s><s xml:id="_w99vUFm" coords="7,321.06,397.29,210.24,9.69;7,80.70,409.24,271.00,9.69">The equations are the same for each modality except that the node embedding to learn the adjacency matrix is diferent.</s><s xml:id="_XsvX4hA" coords="7,354.17,409.24,177.39,9.69;7,80.70,421.20,450.61,9.69;7,80.70,433.15,450.61,9.69;7,80.70,445.11,232.54,9.69">This approach to constructing an adjacency matrix for a temporal sequence is learnable with parameters, and meanwhile it is instance-speciic as it considers the various relatedness among nodes for diferent utterances, as opposed to directly setting all the matrix elements as learnable parameters (we will discuss it in Section 4).</s><s xml:id="_dvRSHRj" coords="7,315.98,445.11,199.17,9.69">Hence, we term this approach indirect learning.</s><s xml:id="_XGfCvZN" coords="7,517.89,445.11,13.41,9.69;7,80.70,457.06,450.61,9.69;7,80.70,469.02,273.15,9.69">We claim that this instance-speciic and learnable approach can capture more relatedness information on the nodes and generate more favourable performance, as shown in Section 5.5.</s><s xml:id="_NYUQuST" coords="7,356.33,469.02,175.16,9.69;7,80.70,480.97,450.61,9.69;7,80.70,492.93,192.96,9.69">From this deinition of adjacency matrix for sequential data, it can be seen that even if the two nodes are distant apart in the time dimension, they can still be directly connected if they are considered related.</s><s xml:id="_WtTpPdk" coords="7,275.90,492.93,255.41,9.69;7,80.70,504.89,450.60,9.69;7,80.70,516.84,44.22,9.69">Therefore, compared to RNN variants, GCN can efectively learn long-term temporal dependency with fewer layers, which makes it a suitable alternative to model long temporal sequences.</s><s xml:id="_xDsNJhY" coords="7,127.88,516.84,403.42,9.69;7,80.70,528.80,326.47,9.69">Compared to TCN variants that use ixed convolution pattern to model sequences, the proposed method is more lexible to automatically identify and recognize related time steps.</s><s xml:id="_kYwX62n" coords="7,409.56,528.80,121.75,9.69;7,80.70,540.75,450.60,9.69;7,80.70,552.71,189.22,9.69">Although the indirect learning method has some similarity to Transformer <ref type="bibr" coords="7,257.23,540.75,16.24,9.69" target="#b50">[51]</ref> in avoiding recurrence and learning long-term dependency, this approach is diferent from it in several aspects.</s><s xml:id="_V46kE8x" coords="7,272.41,552.71,258.90,9.69;7,80.70,564.66,240.27,9.69">Firstly, Transformer uses so f tmax as activation function, which means that each time step is associated with all time steps.</s><s xml:id="_H9nWX3t" coords="7,323.45,564.66,207.85,9.69;7,80.70,576.62,450.80,9.69;7,80.70,588.57,43.05,9.69">In contrast, our method is better-targeted in that it can ilter out the time steps that have no direct connection, and automatically detect the one-hop neighbors for each node.</s><s xml:id="_44A5XfB" coords="7,126.24,588.57,405.06,9.69;7,80.70,600.53,450.61,9.69;7,80.70,612.48,450.61,9.69;7,80.70,624.44,319.14,9.69">Moreover, after inding the neighbors for each time step, we use a GCN such as Graph Isomorphism Network (GIN) <ref type="bibr" coords="7,141.99,600.53,16.22,9.69" target="#b55">[56]</ref> to aggregate the information of the neighbors and explore the inter-dependency between time steps, and this operation is quiet diferent from the feed forward network of Transformer which only operates at the feature dimension and cannot explore the connections between time steps.</s><s xml:id="_WAzjkFf" coords="8,90.66,107.25,440.64,9.69;8,80.70,119.20,273.70,9.69">Note that in common graph deinition, the elements in the adjacency matrix are often binary and restricted to either 0 or 1, which denotes no/one direct connection, respectively.</s><s xml:id="_R8zmA9C" coords="8,356.88,119.20,174.41,9.69;8,80.70,131.16,451.70,9.69;8,80.70,143.11,60.46,9.69">However, we dispense with this restriction and formulate the elements to be continuous, with a larger value indicating a closer relation between two nodes, and vice versa.</s><s xml:id="_pmgRajb" coords="8,143.66,143.11,387.89,9.69;8,80.70,155.07,29.05,9.69">This can be interpreted as multiplying an attention mask matrix to the conventional adjacency matrix.</s><s xml:id="_usrZGwe" coords="8,112.24,155.07,342.84,9.69">We justify the use of such continuous soft weights in adjacency matrix in Appendix.</s></p><p xml:id="_92rUHXw"><s xml:id="_Z96d9hz" coords="8,90.66,167.02,440.64,9.69;8,80.70,178.98,81.99,9.69">After obtaining the adjacency matrix, the deinition of COMBINE and AGGREGATE functions in GCN could have many choices.</s><s xml:id="_h37YnUj" coords="8,165.77,178.98,365.53,9.69;8,80.70,190.93,27.36,9.69">It is worth mentioning that the unimodal graphs are independent of the concrete GCN model.</s><s xml:id="_Aq8jQJG" coords="8,110.54,190.93,305.59,9.69">In other words, we can integrate any GCN model into our unimodal graph.</s><s xml:id="_GvkUttT" coords="8,418.61,190.93,112.69,9.69;8,80.70,202.89,450.61,9.69;8,80.70,214.84,145.37,9.69">In practice, we compare the performance of Graph Isomorphism Network (GIN) <ref type="bibr" coords="8,289.19,202.89,14.70,9.69" target="#b55">[56]</ref>, Graph Attention Network (GAT) <ref type="bibr" coords="8,443.26,202.89,14.70,9.69" target="#b51">[52]</ref>, GraphSAGE <ref type="bibr" coords="8,515.08,202.89,16.22,9.69" target="#b15">[16]</ref> and DifPool <ref type="bibr" coords="8,133.28,214.84,16.22,9.69" target="#b59">[60]</ref> in our experiment.</s><s xml:id="_jqMzwME" coords="8,228.54,214.84,302.76,9.69;8,80.70,226.94,164.55,9.69;8,245.92,225.55,7.38,6.82;8,256.72,226.94,165.26,9.69">Speciically, we use GraphSAGE with mean pooling <ref type="bibr" coords="8,436.75,214.84,16.22,9.69" target="#b15">[16]</ref> as the default GCN in this paper, and the equations for the k t h iteration of GraphSAGE is shown below:</s></p><formula xml:id="formula_7" coords="8,230.77,242.37,300.53,13.60">( N a ) k = f (D -1 (A a + I )(N a i ) k -1 W k a )<label>(4)</label></formula><formula xml:id="formula_8" coords="8,265.78,260.39,262.00,30.20">(N a i ) k = ( N a i ) k ||( N a i ) k || 2 (<label>5</label></formula><formula xml:id="formula_9" coords="8,527.78,269.99,3.52,9.69">)</formula><p xml:id="_4qjX3AU"><s xml:id="_Wmcvtpg" coords="8,80.33,294.00,379.65,9.69;8,461.32,292.92,4.14,4.87;8,460.22,299.39,2.23,6.82;8,466.78,292.61,7.08,10.74;8,477.66,294.00,53.64,9.69;8,80.70,306.98,278.11,9.69;8,360.49,305.60,3.77,6.82;8,358.69,311.67,3.89,6.82;8,367.58,306.98,94.69,9.69">where f is the non-linear activation function for which we use ReLU in our experiment, (N a i ) k is the hidden representation for node i of the acoustic modality at iteration k, and W k a is the parameter matrix.</s><s xml:id="_FZrzWZT" coords="8,464.28,306.98,67.02,9.69;8,80.70,318.94,76.76,9.69;8,157.25,317.86,4.14,4.87;8,164.94,318.94,72.50,11.07;8,237.23,317.55,3.89,6.82;8,237.48,324.32,5.19,6.82;8,246.30,318.94,77.60,10.55">D is the diagonal degree matrix of A a where D ii = j A a i j and D i j = 0 (i j).</s><s xml:id="_h8GHm4Q" coords="8,326.39,318.94,204.90,9.69;8,80.70,331.62,315.21,9.69;8,395.70,330.54,4.14,4.87;8,403.79,331.62,129.05,9.69">The diagonal degree matrix D is added to perform mean pooling, and the identity matrix I is added to the adjacency matrix A a to perform self-loop operation.</s><s xml:id="_hZJE8aX" coords="8,80.70,343.71,435.50,9.69;8,517.54,342.63,4.14,4.87;8,516.44,349.10,2.23,6.82;8,523.00,342.32,7.08,10.74;8,80.70,355.66,279.46,9.69">Normalization is done in Eq. 5. To obtain the inal unimodal representation, the hidden representations (N a i ) k for each layer is concatenated and sent to the fully-connected layers:</s></p><formula xml:id="formula_10" coords="8,242.96,371.05,288.34,13.60">N a i ← ⊕(N a i ) k , k ∈ [1, 2, .., r ]<label>(6)</label></formula><formula xml:id="formula_11" coords="8,261.37,388.27,269.93,13.60">N a i = f (N a i W a o + b a o )<label>(7)</label></formula><p xml:id="_g54PZbn"><s xml:id="_UBTVRUV" coords="8,80.33,404.74,34.76,9.69;8,116.42,403.66,4.14,4.87;8,115.33,410.13,2.23,6.82;8,124.11,404.74,407.20,9.69;8,80.70,416.70,55.13,9.69;8,137.56,415.31,3.89,6.82;8,135.63,421.39,3.59,6.82;8,144.87,416.70,22.13,9.69;8,167.55,415.31,3.89,6.82;8,167.11,421.39,3.59,6.82;8,174.86,416.70,301.31,9.69">where N a i is the inal representation of node i for acoustic modality, ⊕ denotes concatenation, r is the number of layers, and W a o and b a o are the weight matrix and bias for the fully connected layers, respectively.</s><s xml:id="_Ewr9rPR" coords="8,478.66,416.70,52.65,9.69;8,80.70,428.65,218.58,9.69">Note that we use the same convolution structure for each modality.</s></p><p xml:id="_cN7ryjQ"><s xml:id="_ZVmb63A" coords="8,90.66,440.61,308.89,9.69">With graph convolution, intra-modal interactions can be explored efectively.</s><s xml:id="_cbpRnQJ" coords="8,402.03,440.61,129.27,9.69;8,80.45,452.56,450.85,9.69;8,80.70,464.52,450.61,9.69;8,80.70,476.47,276.35,9.69">Unlike the commonly used RNN variants which are subject to a number of issues such as gradient vanishing and explosion, forgetting problem and slow inferring speed, GCN abandons the recurrence and can operate in parallel, which is more eicient in terms of time complexity and can learn longer temporal dependency.</s><s xml:id="_94Wgc48" coords="8,359.54,476.47,171.76,9.69;8,80.40,488.43,450.90,9.69;8,80.40,500.38,52.15,9.69">More importantly, it detects the immediate (one-hop) neighbors for each time step and ilters out the unrelated pairs, which is better-targeted compared to Transformer.</s><s xml:id="_RhhYpzc" coords="8,135.04,500.38,396.26,9.69;8,80.70,512.34,104.23,9.69">Extensive experiments are conducted in Section 5.4.1 to show the superior performance of GCN in modeling sequential data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3" xml:id="_ss6avJr">Graph Pooling Fusion Network (GPFN)</head><p xml:id="_T9SyB69"><s xml:id="_AQhBHN9" coords="8,80.35,549.46,450.96,9.69;8,80.70,561.42,78.56,9.69">After exploring intra-modal interactions, the second challenge is to model inter-modal dynamics and fuse the cross-modal nodes.</s><s xml:id="_2pEm8eQ" coords="8,161.75,561.42,369.55,9.69;8,80.70,573.37,51.68,9.69">Considering that we focus on unaligned sequences, the common word-level fusion cannot be achieved.</s><s xml:id="_TRPD7pv" coords="8,135.77,573.37,395.53,9.69;8,80.70,585.33,359.14,9.69">This means that our fusion network should learn the interactions among various nodes from multiple modalities, rather than fuse the features from three modalities at each time step.</s><s xml:id="_M6pthJJ" coords="8,442.32,585.33,88.98,9.69;8,80.70,597.28,164.35,9.69">To this end, we devise GPFN to fuse the unaligned sequences.</s><s xml:id="_KUFrh7T" coords="8,248.19,597.28,283.11,9.69;8,80.70,609.24,198.77,9.69">GPFN learns to aggregate the multimodal nodes to learn high-level and reined graph representations hierarchically.</s><s xml:id="_mqBAWTH" coords="8,281.97,609.24,249.34,9.69;8,80.70,621.19,342.03,9.69">Speciically, in GPFN, we introduce max/mean graph pooling and analyze their rationality, which are suitable for pooling the unimodal nodes.</s><s xml:id="_h2qVv9Q" coords="8,426.06,621.19,105.25,9.69;8,80.70,633.15,450.86,9.69;8,80.70,645.10,185.22,9.69">Additionally, we propose link similarity pooling to learn a cluster assignment matrix using the link (topology) information of adjacency matrix, which can fuse the cross-modal nodes.</s><s xml:id="_qSJM6mc" coords="8,268.41,645.10,263.99,9.69;9,80.70,386.19,450.61,9.69;9,80.70,398.15,37.00,9.69">Since features from diferent modalities are highly heterogeneous, the interactions between the cross-modal nodes are much more complex than those of the nodes from a single modality.</s><s xml:id="_dr9gRem" coords="9,119.72,398.15,411.58,9.69;9,80.70,410.10,359.48,9.69">By applying link similarity pooling, the model can automatically learn the complex interactions between the heterogeneous modalities by associating the related nodes between these modalities.</s></p><p xml:id="_zPT5RnD"><s xml:id="_mbaSjgQ" coords="9,90.66,422.06,441.73,9.69;9,80.70,434.01,437.69,9.69">As shown in Fig. <ref type="figure" coords="9,162.88,422.06,3.48,9.69" target="#fig_1">2</ref>, GPFN mainly consists of node sorting, adjacency matrix deinition, graph convolution, mean/max graph pooling, link similarity pooling, graph representation deinition and prediction inference.</s><s xml:id="_YghxVTf" coords="9,520.87,434.01,10.43,9.69;9,80.70,445.97,450.60,9.69;9,80.70,457.92,122.69,9.69">To retain consistence, the graph convolution framework and the adjacency matrix deinition method are the same as those in Unimodal Graphs.</s><s xml:id="_VGZhPkM" coords="9,206.02,457.92,325.28,9.69;9,80.70,469.88,413.48,9.69">In the following subsections, we will introduce node sorting, max/mean graph pooling, link similarity pooling, graph representation deinition and prediction inference respectively.</s></p><p xml:id="_az3AvvE"><s xml:id="_KCHFd4Z" coords="9,80.70,494.20,89.80,9.32">3.3.1 Node Sorting.</s><s xml:id="_PHwJ4pt" coords="9,173.99,493.83,357.32,9.69;9,80.70,505.78,223.51,9.69">Firstly, we need to arrange the nodes to determine the order of the nodes from the three Unimodal Graphs and obtain the multimodal sequence.</s><s xml:id="_JUUV2M5" coords="9,306.69,505.78,224.62,9.69;9,80.70,517.74,411.04,9.69">An intuition here is to sort nodes from three modalities according to time dimension such that the nodes from neighboring time steps are closer to each other.</s><s xml:id="_Dqzj5aw" coords="9,494.23,517.74,38.17,9.69;9,80.70,529.69,327.98,9.69">However, this requires that the time dimensions of diferent modalities are explicitly aligned.</s><s xml:id="_WdwA4QV" coords="9,410.96,529.69,120.34,9.69;9,80.70,541.65,450.61,9.69;9,80.70,553.69,89.47,9.69">Hence, we simply concatenate the nodes at the time dimension one modality after one modality, and let the model automatically learn to aggregate these nodes.</s><s xml:id="_M35ezPJ" coords="9,172.23,553.69,59.16,9.69;9,232.50,552.31,6.28,6.82;9,241.74,553.69,75.51,9.69">The node set N m can be described as</s></p><formula xml:id="formula_12" coords="9,80.95,552.31,450.36,28.52">N m = {n l 1 , n l 2 , ..., n l T l , n a 1 , n a 2 , ..., n a T a , n v 1 , n v 2 , ..., n v T v } = {n m 1 , n m 2 , ..., n m T l +T a +T v }.</formula><p xml:id="_ReSZ5sM"><s xml:id="_fPgBzTm" coords="9,175.46,566.85,262.00,11.08;9,440.92,568.24,91.48,9.69;9,80.70,581.02,301.73,11.08;9,384.12,582.40,136.32,10.67">For conciseness, we denote the multimodal sequence as N ∈ R T ×d in the rest of the paper, and the adjacency matrix for multimodal sequence is denoted as A ∈ R T ×T , where T is equal to T l + T a + T v .</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_ruE5c4d">3.3.2</head><p xml:id="_mwF8wqb"><s xml:id="_dXwFRcU" coords="9,108.36,606.72,119.24,9.33">Mean/Max Graph Pooling.</s><s xml:id="_2eTyXvR" coords="9,231.09,606.36,300.21,9.69;9,80.70,618.31,32.23,9.69">The initial adjacency matrix is computed in the same way as the Unimodal Graphs.</s><s xml:id="_vMeWksx" coords="9,115.42,618.31,416.25,9.69;9,80.70,630.27,137.04,9.69">With graph pooling, the related nodes are fused so that the interactions can be explored and the new adjacency matrix can be obtained.</s><s xml:id="_npVeUQG" coords="9,220.22,630.27,200.58,9.69">In the GPFN, we provide two pooling approaches.</s><s xml:id="_eunxD36" coords="9,423.29,630.27,108.02,9.69;9,80.70,642.22,129.45,9.69">The irst kind is the simple max pooling and mean pooling.</s><s xml:id="_r9VzafJ" coords="9,212.64,642.22,318.66,9.69;10,80.70,107.25,187.60,9.69">For the adjacency matrix, 2D mean/max pooling is applied, while for the node embeddings, 1D mean/max pooling is applied.</s><s xml:id="_eZ92Da6" coords="10,270.78,107.25,215.85,9.69">The equations for mean pooling are given as follows:</s></p><formula xml:id="formula_13" coords="10,268.03,123.86,263.28,27.65">N k i = s-1 д=0 N k -1 s •i-д s<label>(8)</label></formula><formula xml:id="formula_14" coords="10,244.22,158.25,287.08,27.65">A k i, j = s-1 д=0 s-1 m=0 A k -1 s •i-д,s •j-m s 2<label>(9)</label></formula><p xml:id="_y6vz36Q"><s xml:id="_F9huVsk" coords="10,80.70,188.35,85.00,9.69">and for max pooling:</s></p><formula xml:id="formula_15" coords="10,233.66,201.12,297.64,13.60">N k i = max({N k -1 s •i-д | 0 ≤ д ≤ s -1})<label>(10)</label></formula><formula xml:id="formula_16" coords="10,192.01,221.96,335.50,13.60">A k i, j = max({A k -1 s •i-д,s •j-m | 0 ≤ д ≤ s -1, 0 ≤ m ≤ s -1}) (<label>11</label></formula><formula xml:id="formula_17" coords="10,527.50,223.85,3.80,9.69">)</formula><p xml:id="_q7Kfj8y"><s xml:id="_kZKbMDs" coords="10,80.33,242.12,123.10,9.69;10,204.72,240.74,3.77,6.82;10,203.67,247.51,2.23,6.82;10,212.50,242.12,319.00,9.69;10,80.70,254.08,59.42,9.69">where s is the pooling size, N k i is the updated node embedding for node i at iteration k, and • denotes scalar multiplication.</s><s xml:id="_h75xZ2c" coords="10,142.60,254.08,256.52,9.69">Note that in Eq. 10, the max pooling operation is element-wise.</s></p><p xml:id="_wKYjW2m"><s xml:id="_NDXFAsB" coords="10,90.66,266.03,440.64,9.69;10,80.70,277.99,450.60,9.69;10,80.70,289.94,450.61,9.69;10,80.70,301.90,132.80,9.69">The mean/max pooling is meaningful because the nodes in multimodal sequence are concatenated according to the time dimension of each unimodal sequence, and thus the neighboring nodes are closely related in time dimension and considered to be fusible (but we need to carefully determine the pooling size s to avoid fusing the nodes from diferent modalities).</s><s xml:id="_vc8b2gV" coords="10,215.99,301.90,186.68,9.69">Moreover, we have the following observation:</s></p><p xml:id="_tM6SBZp"><s xml:id="_ASE3ZEx" coords="10,90.66,313.85,440.64,9.69;10,80.70,325.81,450.60,9.69;10,80.70,337.76,103.43,9.69">Observation 1: If nodes x and y are 1-hop neighbors (directly connected), then they are 1-hop or 0-hop neighbors after mean/max graph pooling, where 0-hop neighbors mean that x and y are merged into the same node after graph pooling.</s></p><p xml:id="_pMKsRaJ"><s xml:id="_2vmzES5" coords="10,90.66,349.72,88.97,9.69">Proof: See Appendix.</s></p><p xml:id="_CMCnWjX"><s xml:id="_C3En7Jg" coords="10,90.66,361.67,440.64,9.69;10,80.70,373.63,439.23,9.69">The above property of mean/max pooling suggests that they are reasonable approaches for graph pooling, and indicates a principle for us to manually design graph pooling algorithms: once neighbors, always neighbors.</s></p><p xml:id="_payrM2G"><s xml:id="_927e2UK" coords="10,90.66,385.58,440.64,9.69;10,80.33,397.54,450.97,9.69;10,80.70,409.49,250.60,9.69">Although the mean/max graph pooling may be efective in fusing nodes from the same modality given that we can utilize the time information in the unimodal sequence, it is not a desirable method for fusing nodes from diferent modalities in unaligned multimodal sequence.</s><s xml:id="_5PGPft8" coords="10,333.77,409.49,197.54,9.69;10,80.70,421.45,230.92,9.69">Therefore, we need a learnable method that can efectively cluster the heterogeneous cross-modal nodes.</s><s xml:id="_gvC26EX" coords="10,314.10,421.45,217.21,9.69;10,80.70,433.41,72.47,9.69">To this end, we propose link similarity pooling in the following section.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3" xml:id="_rDPEjGu">Link Similarity Pooling.</head><p xml:id="_BSVYjTz"><s xml:id="_G63WZzM" coords="10,219.42,452.32,311.88,9.69;10,80.70,464.28,450.86,9.69;10,80.70,476.23,204.74,9.69">Apart from the mean/max graph pooling, we devise a learnable graph pooling method, named link similarity pooling, to leverage the link information (topology information) in the adjacency matrix to learn the node cluster assignment matrix.</s><s xml:id="_neut9bF" coords="10,287.89,476.23,243.42,9.69;10,80.70,488.19,450.61,9.69;10,80.70,500.14,450.60,9.69;10,80.70,512.10,132.29,9.69">Diferent from other graph pooling methods such as DifPool <ref type="bibr" coords="10,80.70,488.19,16.25,9.69" target="#b59">[60]</ref> that mainly utilize the node embedding to learn a cluster assignment matrix <ref type="bibr" coords="10,406.83,488.19,14.92,9.69" target="#b59">[60,</ref><ref type="bibr" coords="10,424.25,488.19,11.19,9.69" target="#b61">62]</ref>, link similarity pooling uses the neighbor (topology) information in the adjacency matrix to learn a cluster assignment matrix, which is more intuitive and interpretable.</s><s xml:id="_qWuagus" coords="10,215.48,512.10,245.69,9.69">We deine the link similarity pooling in following equations:</s></p><formula xml:id="formula_18" coords="10,239.79,528.67,287.71,12.43">Z ′ = AA R , Z = f [(Z ′ + A)W z ] (<label>12</label></formula><formula xml:id="formula_19" coords="10,527.50,530.56,3.80,9.69">)</formula><p xml:id="_dg5AHFv"><s xml:id="_ha8fUEU" coords="10,80.33,546.33,205.46,13.21;10,289.12,546.33,196.08,12.36;10,488.53,549.00,42.78,9.69;10,80.70,560.95,248.30,9.69;10,330.69,559.71,2.52,6.16;10,335.98,560.95,34.14,9.69;10,371.80,559.71,2.52,6.16;10,377.10,560.95,135.85,9.69">where R denotes matrix transposition, W z ∈ R T ×T ′ is the transfer parametric matrix, and Z ∈ R T ×T ′ is the inal node cluster assignment matrix that maps the T nodes into T ′ nodes (T ′ is sequence length after pooling).</s><s xml:id="_4CWrXvt" coords="10,515.43,560.95,15.87,9.69;10,80.70,572.91,450.61,9.69;10,80.70,584.86,204.37,9.69">The irst equation measures the similarity score of two nodes by calculating the inner product of their respective association intensity to their common neighbors.</s><s xml:id="_9FgeckZ" coords="10,287.83,584.86,243.48,9.69;10,80.70,596.82,366.56,9.69">In this way, a greater extent to which a pair of nodes have similar distribution of association intensity on their shared neighbors leads to a larger score.</s><s xml:id="_Sb7cqaF" coords="10,449.60,596.82,81.96,9.69;10,80.70,608.77,450.61,9.69;10,80.70,620.73,39.39,9.69">It is also noteworthy that this score is weighted because the elements in the adjacency matrix are soft (continuous) and not restricted to be binary.</s><s xml:id="_d67aYCp" coords="10,122.58,620.73,408.73,9.69;10,80.70,632.68,111.76,9.69;10,193.90,631.44,2.52,6.16;10,192.87,637.38,10.63,6.82;10,207.36,632.68,35.67,9.69">For example, if nodes x and y have more common neighbors and their linked values with the common neighbor are larger, then Z ′ x,y is larger.</s><s xml:id="_ZMnjejs" coords="10,245.82,632.68,76.20,9.69;10,323.45,631.44,2.52,6.16;10,322.43,632.68,208.87,11.51;10,80.70,644.74,166.42,9.69">In Fact, we have Z ′ x,y = c ∈C N A x,c • A y,c where CN denotes the set of common neighbors between x and y.</s><s xml:id="_Gcgge58" coords="10,250.09,644.74,281.47,9.69;11,80.70,107.25,343.12,10.55">The second equation adds the original adjacency matrix to the link similarity information, which means that Z x,y is larger if x and y are 1-hop neighbors.</s><s xml:id="_M7FXvhG" coords="11,426.21,107.25,105.09,9.69;11,80.70,119.20,452.14,9.69">This is reasonable because if two nodes are neighbors, then they are considered to be similar and thereby can be fused with a high possibility.</s></p><p xml:id="_qePPTT4"><s xml:id="_6geMmqe" coords="11,90.66,131.16,440.64,9.69;11,80.70,143.11,69.69,9.69">After obtaining the node cluster assignment matrix Z , we use it to learn the updated adjacency matrix and node embedding.</s><s xml:id="_aQs7nyJ" coords="11,152.88,143.11,160.52,9.69">The equations are presented as follows:</s></p><formula xml:id="formula_20" coords="11,279.76,159.80,251.54,10.55">S = f (NW s )<label>(13)</label></formula><formula xml:id="formula_21" coords="11,240.03,175.41,287.47,12.44">N update = Z R S, A update = Z R AZ (<label>14</label></formula><formula xml:id="formula_22" coords="11,527.50,177.29,3.80,9.69">)</formula><p xml:id="_zyZdSck"><s xml:id="_AJJm77b" coords="11,80.33,191.60,231.70,11.93;11,315.81,190.32,196.42,13.23;11,516.01,192.99,15.29,9.69;11,80.40,203.38,64.52,13.23;11,148.24,206.04,293.19,9.69">where S is the transformed node embedding, W s ∈ R d ×d is a learnable parameter matrix, N update ∈ R T ′ ×d and A update ∈ R T ′ ×T ′ denote the updated node embedding and adjacency matrix, respectively.</s></p><p xml:id="_9G4MnED"><s xml:id="_su4q8Dv" coords="11,90.66,218.00,440.64,9.69;11,80.70,229.95,450.60,9.69;11,80.70,241.91,105.54,9.69">The reason why we do not use the node embedding is that the adjacency matrix is originally derived from the node embedding, and therefore adjacency matrix contains part of the information in node embedding as well as the topology information.</s><s xml:id="_jjjngHP" coords="11,188.72,241.91,342.58,9.69;11,80.70,253.86,58.92,9.69">Therefore, we assume that using the adjacency matrix is suicient to learn the node cluster matrix.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.4" xml:id="_ZbTSUfR">Graph Representation Definition and Prediction</head><p xml:id="_brkt2Rx"><s xml:id="_6V4XqDr" coords="11,328.49,272.43,45.04,9.32">Inference.</s><s xml:id="_bPymGkb" coords="11,377.01,272.06,154.29,9.69;11,80.70,284.01,364.44,9.69">After the graph convolution and graph pooling operation of GPFN, we average the node embeddings as the representation of GPFN.</s><s xml:id="_Sbek6TH" coords="11,447.19,284.01,84.11,9.69;11,80.70,295.97,450.61,9.69;11,80.70,307.92,151.70,9.69">Then we concatenate the representation of GPFN with the averaged node embedding from three Unimodal Graphs respectively to obtain the inal graph representation.</s><s xml:id="_Y637fPJ" coords="11,234.89,307.92,296.41,9.69;11,80.70,319.88,136.62,9.69">Several fully-connected layers are applied on the graph representation to infer the inal sentiment decision.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_XY2ffGC">DISCUSSION ON THE ADJACENCY MATRIX</head><p xml:id="_Q5Kn2sm"><s xml:id="_yr3hmbV" coords="11,80.70,357.00,450.60,9.69;11,80.70,368.95,123.76,9.69">Previously we deine an indirect learning method to construct the adjacency matrix for sequential data, which is instance-speciic and learnable.</s><s xml:id="_4XqGQeC" coords="11,206.57,368.95,326.27,9.69">In this section, we aim to explore other methods to construct the adjacency matrix.</s><s xml:id="_2gRYkAQ" coords="11,80.22,380.91,451.09,9.69;11,80.70,392.87,194.19,9.69">We present three other ways to deine the adjacency matrix, namely generalized diagonal matrix, KNN-based adjacency matrix, and a direct learning method.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_2NHnwHU">Generalized Diagonal Matrix</head><p xml:id="_UCbZ8zJ"><s xml:id="_5j7fBG6" coords="11,80.70,429.99,275.53,9.69">Intuitively, the neighboring time slices are more related to each other.</s><s xml:id="_P993dqW" coords="11,358.46,429.99,172.84,9.69;11,80.70,441.94,139.83,9.69">Therefore, we deine a generalized diagonal matrix (GDM) to relect this point:</s></p><formula xml:id="formula_23" coords="11,222.03,455.01,305.47,65.74">1 λ λ 2 ... λ n-1 0 ... 0 λ 1 λ ... λ n-2 λ n-1 ... 0 . . . . . . . . . . . . . . . . . . . . . . . . 0 ... 0 λ n-1 ... λ 1 λ 0 ... 0 0 ... λ 2 λ 1 (<label>15</label></formula><formula xml:id="formula_24" coords="11,527.50,484.02,3.80,9.69">)</formula><p xml:id="_uM5qDKy"><s xml:id="_fxvtsYK" coords="11,80.33,525.55,450.97,9.69;11,80.70,537.50,22.20,9.69">where λ is the attenuation factor which is set to 2 in our experiment, and n is the truncation factor which is set to 10.</s><s xml:id="_5bTfMKS" coords="11,105.38,537.50,350.72,9.69;11,460.25,535.56,3.38,7.08;11,459.85,543.17,3.86,6.82;11,467.80,537.50,63.51,9.69;11,80.70,549.46,373.54,9.69">The value on the diagonal of the GDM is 1, and each element is decayed by a factor of 1 λ centered on the diagonal, which means the correlation between them is reduced with the increase of distance.</s><s xml:id="_PpZUvxC" coords="11,456.70,549.46,74.60,9.69;11,80.70,561.41,450.79,9.69;11,80.70,573.37,93.79,9.69">When the distance to the diagonal element is larger than n, the value becomes zero, which means that the distant nodes no longer have direct connection.</s><s xml:id="_bBgBJKp" coords="11,176.99,573.37,354.67,9.69;11,80.70,585.32,89.80,9.69">Nevertheless, we can stack many layers so that each node can still have the overall view of the input sequence.</s><s xml:id="_tpBuXkd" coords="11,173.00,585.32,358.31,9.69;11,80.70,597.28,438.22,9.69">Obviously, for a sequence of length T , we need to stack ceil ((T -1)/(n -1)) layers such that each node can incorporate information from all the nodes, where ceil means rounding up to an integer.</s></p><p xml:id="_aRGReGP"><s xml:id="_bSkWDA7" coords="11,90.66,609.23,442.17,9.69">An advantage of GDM lies in that it eschews the complex computation for inding an adjacency matrix.</s><s xml:id="_ckHpvPg" coords="11,80.35,621.19,241.53,9.69">Actually, GDM functions like the kernels in TCN <ref type="bibr" coords="11,284.48,621.19,10.46,9.69" target="#b0">[1,</ref><ref type="bibr" coords="11,297.43,621.19,6.86,9.69" target="#b1">2,</ref><ref type="bibr" coords="11,306.78,621.19,11.32,9.69" target="#b40">41]</ref>.</s><s xml:id="_q3gVRAP" coords="11,324.36,621.19,206.93,9.69;11,80.70,633.15,361.41,9.69">But unlike TCN kernels, it is predeined instead of being obtained through learning (we leave the learning part to the parameters of GCN).</s><s xml:id="_uT3HCnx" coords="11,444.60,633.15,86.71,9.69;11,80.70,645.10,214.63,9.69">The main diferences between GCN and TCN will be discussed in Appendix.</s><s xml:id="_WnjhPjs" coords="11,297.36,645.10,234.19,9.69;12,80.70,107.25,229.31,9.69">In addition, we also implement a fully-connected adjacency matrix whose values are all ones to make a comparison.</s><s xml:id="_BzYXGx5" coords="12,312.50,107.25,218.80,9.69;12,80.70,119.20,450.79,9.69;12,80.70,131.16,39.38,9.69">Compared to the indirect learning method, this GDM method is intuitive and its the empirical pattern of sequence modeling, but it is neither instance-speciic nor learnable.</s><s xml:id="_9X7WMkB" coords="12,122.57,131.16,387.82,9.69">GDM serves as a reasonable baseline in the exploration of adjacency matrix for sequential data.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" xml:id="_xgKCCUR">KNN-based method</head><p xml:id="_v5D66Vc"><s xml:id="_mMpRvXH" coords="12,80.35,167.88,451.21,9.69;12,80.70,179.83,450.61,9.69;12,80.70,191.79,18.43,9.69">Another simple but efective non-parametric approach to inding an adjacency matrix in a sequence is to apply K-nearest neighbor (KNN) algorithm to deine the 1-hop neighbors of each node, which has also been evaluated in <ref type="bibr" coords="12,80.70,191.79,14.74,9.69" target="#b9">[10]</ref>.</s><s xml:id="_pBexFdB" coords="12,101.62,191.79,429.68,9.69;12,80.70,203.74,57.50,9.69">KNN is based on Euclidean distance, and we select the nodes with shortest distance as the 1-hop neighbors for each node.</s><s xml:id="_Uvd9NrW" coords="12,140.69,203.74,167.82,9.69">The equations can be described as below:</s></p><formula xml:id="formula_25" coords="12,248.03,217.42,283.27,53.09">d i j = 1 Eur(N j ; N i ) + ϵ (16) Âi,j = ReLU(d i j -α × j d i j T )<label>(17)</label></formula><formula xml:id="formula_26" coords="12,269.48,270.81,261.82,29.24">A i, j = Âi,j v ∈N Âi,v<label>(18)</label></formula><p xml:id="_XrsG8sm"><s xml:id="_QHpZw2J" coords="12,80.33,301.79,450.97,10.55;12,80.70,313.75,452.14,9.69">where Eur denotes the Euclidean distance, d i j denotes the 'similarity' of node j to node i, ϵ denotes a positive scalar to prevent division by zero, and α is a scalar that controls to what extent the weak links can be iltered out.</s><s xml:id="_YDrEFsj" coords="12,80.70,325.70,263.28,9.69">Eq. 17 ilters out the links that have no strong connection and Eq.</s><s xml:id="_5UK2jSZ" coords="12,346.48,325.70,132.34,9.69">18 denotes simple normalization.</s><s xml:id="_grMPEZn" coords="12,481.33,325.70,50.24,9.69;12,80.70,337.66,450.80,9.69;12,80.70,349.61,372.64,9.69">Note that by applying Eq. 17, we do not select an exact number of k neighbors for each node, but we allow a variable number of neighbors, as long as the links between the node and its neighbors meet the threshold.</s><s xml:id="_3wK8wsQ" coords="12,456.11,349.61,75.20,9.69;12,80.70,361.57,450.61,9.69;12,80.70,373.52,176.76,9.69">A disadvantage of KNN-based method is that it has to compute the Euclidean distance of each two nodes for each instance, making it more time-consuming compared to GDM.</s><s xml:id="_WETsqv4" coords="12,259.96,373.52,271.34,9.69;12,80.70,385.48,387.74,9.69">By deinition, KNN-based adjacency matrix is instance-speciic, but it is not learnable, serving as a reasonable comparative method to the indirect learning method.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" xml:id="_aUmCZpe">Direct Learning</head><p xml:id="_8BCqn7D"><s xml:id="_EnSwXyN" coords="12,80.35,422.20,452.49,9.69">Another intuitive way to construct an adjacency matrix is to learn the matrix directly in the training process.</s><s xml:id="_GvWwfhA" coords="12,80.70,432.42,350.48,12.20;12,432.88,434.93,98.42,9.69;12,80.70,445.15,452.14,12.32">In this method, the adjacency matrix is parameterized as a learnable matrix Â ∈ R T ×T , and a ReLU function is applied to activate the learned matrix: A = ReLU( Â) such that the linked values between nodes are non-negative.</s></p><p xml:id="_W9wFEeg"><s xml:id="_56KwCEc" coords="12,90.66,459.62,395.51,9.69">To obtain a more representative adjacency matrix, we add a regularization term, as shown below:</s></p><formula xml:id="formula_27" coords="12,218.36,473.91,312.94,22.77">ℓ = i ∈N ( j ∈N Âi,j ) 2 + i ∈N ( j ∈N ReLU( Âi,j ) -γ ) 2<label>(19)</label></formula><p xml:id="_2JCGAdx"><s xml:id="_47gday4" coords="12,80.33,500.92,450.97,9.69;12,80.70,512.87,450.61,9.69;12,80.70,524.83,450.60,9.69;12,80.70,536.78,29.63,9.69">where the irst term forces the sum of linked values of each node to equal zero, which can prevent the learned matrix from becoming a fully-connected matrix; the second term restricts the sum of the positive linked values of each node to approximate a given positive scalar γ so as to prevent it from degenerating into an all-zero matrix.</s><s xml:id="_9rA5vsS" coords="12,113.22,536.78,147.67,9.69">ℓ is optimized via gradient descent.</s><s xml:id="_AxHtaen" coords="12,263.83,536.78,267.48,9.69;12,80.40,548.74,450.90,9.69;12,80.70,560.69,162.26,9.69">Note that the learnt adjacency matrix is shared across instances (instance-independent), which may not be expressive enough compared to the instance-speciic indirect learning method but is computationally eicient.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_nePkHSF">EXPERIMENTS</head><p xml:id="_3NTxDdq"><s xml:id="_z8WHFXN" coords="12,80.70,597.41,325.55,9.69">Multimodal Graph is evaluated on three popular datasets for multimodal learning.</s><s xml:id="_geghzT3" coords="12,408.52,597.41,122.78,9.69;12,80.70,609.36,370.40,9.69">In this section, we focus on the following questions: 1) Does Multimodal Graph perform favorably to TCN and RNN variants?</s><s xml:id="_hVfxGYE" coords="12,453.23,609.36,78.08,9.69;12,80.70,621.32,416.21,9.69">2) Does Multimodal Graph achieve state-of-the-art performance on multimodal sentiment analysis and emotion recognition?</s><s xml:id="_a7BETNK" coords="12,499.34,621.32,31.96,9.69;12,80.70,633.28,415.75,9.69">3) What kind of GNNs performs best in our Multimodal Graph? 4) What kind of adjacent matrix performs best?</s><s xml:id="_PPRVjvv" coords="12,498.93,633.28,32.37,9.69;12,80.70,645.23,158.14,9.69">5) What are the attributes of adjacent matrices?</s><s xml:id="_cnNCBda" coords="13,216.46,122.19,257.99,9.69"><ref type="bibr" coords="13,216.46,122.19,16.30,9.69" target="#b66">[67]</ref> is a widely-used dataset for multimodal sentiment analysis.</s><s xml:id="_dGwWW2g" coords="13,476.95,122.19,54.35,9.69;13,80.45,134.15,279.18,9.69">It contains 93 videos in total, and each video is divided into 62 utterances at most.</s><s xml:id="_pSJVpUK" coords="13,362.11,134.15,169.19,9.69;13,80.70,146.10,363.65,9.69">The intensity of sentiment ranges within <ref type="bibr" coords="13,80.70,146.10,20.53,9.69">[-3,3]</ref>, where -3 indicates the strongest negative sentiment, and +3 the strongest positive.</s><s xml:id="_zPTrvpA" coords="13,446.85,146.10,84.45,9.69;13,80.70,158.06,450.86,9.69;13,80.70,170.01,450.61,9.69;13,80.70,181.97,252.58,9.69">We evaluate model's performance using various metrics, including 7-class accuracy (i.e., Acc7: sentiment score classiication), binary accuracy (i.e., Acc2: positive or negative sentiments), F1 score, mean absolute error (MAE) of the score, and the correlation of the model's prediction with humans (Corr).</s><s xml:id="_Cg4DDDG" coords="13,335.75,181.97,195.78,9.69;13,80.70,193.92,250.53,9.69">To be consistent with prior works, we use 1,284 utterances for training, 229 for validation, and 686 for testing.</s><s xml:id="_FG4erUr" coords="13,227.27,212.12,304.03,9.69;13,80.70,224.07,91.45,9.69"><ref type="bibr" coords="13,227.27,212.12,16.22,9.69" target="#b65">[66]</ref> is the largest multimodal language analysis dataset that contains a total number of 2928 videos.</s><s xml:id="_sK3A9FH" coords="13,174.32,224.07,356.99,9.69;13,80.70,236.03,359.39,9.69">The dataset has been segmented at the utterance level, and each utterance has been scored on two levels: sentiment ranging between [-3, 3], and emotion with six diferent values.</s><s xml:id="_RaVxbHn" coords="13,442.58,236.03,88.72,9.69;13,80.70,247.98,66.43,9.69">We use the sentiment label in our task.</s><s xml:id="_YNC9Wyx" coords="13,149.58,247.98,381.73,9.69;13,80.70,259.94,30.35,9.69">In our experiment, the evaluated metrics for CMU-MOSEI are the same as those for CMU-MOSI dataset.</s><s xml:id="_DKBBnzF" coords="13,113.35,259.94,417.96,9.69;13,80.70,271.89,13.68,9.69">We use 16,265 utterances as training set, 1,869 utterances as validation set, and 4,643 utterances as testing set.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1" xml:id="_4mK7989">Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1" xml:id="_QZNbBQS">CMU-MOSI. CMU-MOSI</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2" xml:id="_Mavsyq6">CMU-MOSEI. CMU-MOSEI</head><p xml:id="_qd5bfXq"><s xml:id="_tB9zhhg" coords="13,80.70,290.46,17.83,7.05">5.1.3</s><s xml:id="_hwtKbSN" coords="13,108.70,290.46,48.53,9.32">IEMOCAP.</s><s xml:id="_4fN7dyw" coords="13,160.71,290.09,370.59,9.69;13,80.47,302.05,115.99,9.69">IEMOCAP <ref type="bibr" coords="13,206.23,290.09,11.80,9.69" target="#b4">[5]</ref> is a multimodal emotion recognition dataset that contains a total number of 151 videos from 10 speakers.</s><s xml:id="_W4SUZRn" coords="13,198.96,302.05,214.50,9.69">The videos are segmented into about 10K utterances.</s><s xml:id="_3quxAEy" coords="13,415.95,302.05,115.36,9.69;13,80.70,314.00,362.00,9.69">IEMOCAP has the following labels: anger, happiness, sadness, neutral, excitement, frustration, fear, surprise and other.</s><s xml:id="_3m5tvZa" coords="13,445.17,314.00,86.32,9.69;13,80.70,325.96,164.57,9.69">We take the irst four emotions to compare with our baselines.</s><s xml:id="_PxTnMcX" coords="13,247.77,325.96,283.79,9.69;13,80.70,337.91,136.16,9.69">We follow previous works <ref type="bibr" coords="13,351.29,325.96,17.79,9.69" target="#b48">[49,</ref><ref type="bibr" coords="13,371.58,325.96,12.80,9.69" target="#b64">65]</ref> to report the classiication accuracy and the F1 score of each emotion.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" xml:id="_HKSDq3N">Baselines</head><p xml:id="_Y2QnHMA"><s xml:id="_etZttF8" coords="13,80.40,375.04,452.69,9.69;13,80.70,386.99,450.88,9.69;13,80.70,398.95,450.60,9.69;13,80.40,410.90,56.05,9.69">The baselines for comparison include Early Fusion LSTM (EF-LSTM), Late Fusion LSTM (LF-LSTM), Tensor Fusion Network (TFN) <ref type="bibr" coords="13,200.45,386.99,14.99,9.69" target="#b63">[64]</ref>, Memory Fusion Network (MFN) <ref type="bibr" coords="13,368.34,386.99,14.99,9.69" target="#b64">[65]</ref>, Multi-Fusion Residual Memory Network (MFRM) <ref type="bibr" coords="13,163.07,398.95,14.99,9.69" target="#b31">[32]</ref>, Multimodal Transformer (MulT) <ref type="bibr" coords="13,336.68,398.95,14.99,9.69" target="#b48">[49]</ref>, and Modal-Temporal Attention Graph (MTAG) <ref type="bibr" coords="13,118.03,410.90,14.73,9.69" target="#b56">[57]</ref>.</s><s xml:id="_dVMCynf" coords="13,138.94,410.90,394.05,9.69;13,80.70,422.86,450.61,9.69;13,80.70,434.81,42.62,9.69">Notably, for EF-LSTM, MFN, and MFRM, since they adopt word-level fusion, connectionist temporal classiication (CTC) <ref type="bibr" coords="13,177.64,422.86,16.48,9.69" target="#b14">[15]</ref> is performed to process the unaligned sequences to obtain approximately aligned sequences.</s><s xml:id="_cYhAusv" coords="13,125.79,434.81,407.19,9.69;13,80.70,446.77,33.48,9.69">The models are trained to optimize the CTC alignment objective and the prediction objective simultaneously.</s><s xml:id="_mKP8msx" coords="13,116.67,446.77,322.02,9.69">The detailed introduction of baselines are shown in Appendix for lack of space.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3" xml:id="_NGg8wz3">Experimental details</head><p xml:id="_YWZJTnV"><s xml:id="_K9DSyC3" coords="13,80.70,483.89,349.03,9.69">Hyperparameter Setting: We develop our model on Pytorch with RTX2080Ti as GPU.</s><s xml:id="_PZCGkw6" coords="13,431.89,483.89,99.41,9.69;13,80.70,495.85,452.14,9.69">We apply Mean Absolute Error (MAE) as loss function with Adam <ref type="bibr" coords="13,245.16,495.85,16.24,9.69" target="#b23">[24]</ref> as optimizer (the loss function for IEMOCAP is cross-entropy loss).</s></p><p xml:id="_SpqVmUT"><s xml:id="_ZwycsE3" coords="13,80.40,507.80,249.58,9.69">The defaulted adjacency matrix is the indirect learning one.</s><s xml:id="_z9H3j4Q" coords="13,332.82,507.80,198.48,9.69;13,80.40,519.76,31.32,9.69">For the hyper-parameter setting, please refer to Table <ref type="table" coords="13,104.90,519.76,3.41,9.69" target="#tab_2">3</ref>.</s></p><p xml:id="_KYTYCRZ"><s xml:id="_n6TdeZU" coords="13,90.66,531.71,440.64,9.69;13,80.70,543.67,450.61,9.69;13,80.70,555.62,427.50,9.69">Baseline Evaluation: To ensure a fair comparison, for each baseline, following Gkoumas et al. <ref type="bibr" coords="13,498.05,531.71,14.15,9.69" target="#b12">[13]</ref>, we reproduce the codes and determine the hyperparameters of the baseline by performing ifty-times random grid search on the hyperparameters, and the hyperparameter setting that reaches the best performance is saved.</s><s xml:id="_V59kTU6" coords="13,510.60,555.62,20.89,9.69;13,80.70,567.58,450.61,9.69;13,80.33,579.53,450.97,9.69;13,80.70,591.49,35.49,9.69">After the hyperparameters are determined, we train the model again with the best hyperparameters for ive times with diferent random seeds, and the inal results are obtained by calculating the mean results of the ive-time running.</s><s xml:id="_TtTSEzj" coords="13,118.69,591.49,273.49,9.69">Our method follows the same procedure to obtain the inal results.</s><s xml:id="_DazHHtp" coords="13,394.68,591.49,136.62,9.69;13,80.70,603.44,402.17,9.69">Feature Extraction: For feature extraction, to make a fair comparison with baselines, we follow the setting of CMU-MultimodalSDK<ref type="foot" coords="13,476.84,601.80,3.38,7.08" target="#foot_1">1</ref> .</s><s xml:id="_x645DNh" coords="13,484.98,603.44,46.33,9.69;13,80.70,615.40,328.58,9.69">GloVe word embeddings <ref type="bibr" coords="13,132.76,615.40,16.46,9.69" target="#b41">[42]</ref> are used to extract the features of the transcripts in the videos.</s><s xml:id="_2XEJHzA" coords="13,411.75,615.40,120.65,9.69;13,80.70,627.35,450.61,9.69;14,80.70,363.43,31.59,9.69">The Glove word embeddings, represent each word as a 300-dimensional vector, are trained on 840 billion tokens from the common crawl dataset.</s><s xml:id="_kqSE5Vh" coords="14,115.04,363.43,416.26,9.69;14,80.70,375.39,450.60,9.69;14,80.47,387.34,451.93,9.69;14,80.70,399.30,14.34,9.69">Facet<ref type="foot" coords="14,136.72,361.79,3.38,7.08" target="#foot_2">2</ref> is used to extract a sequence of visual features that are composed of facial action units, facial landmarks, head pose, and gaze tracking, etc. COVAREP <ref type="bibr" coords="14,310.03,375.39,11.66,9.69" target="#b7">[8]</ref> is utilized for extracting acoustic features including 12 Mel-frequency cepstral coeicients, pitch tracking, speech polarity, glottal closure instants, spectral envelope, etc.</s><s xml:id="_PZyX5GQ" coords="14,98.04,399.30,433.26,9.69;14,80.70,411.25,249.65,9.69">These acoustic features are extracted from the full audio clip of each utterance to form a sequence that represents variations in the tone of voice across the utterance.</s><s xml:id="_teyZ5En" coords="14,332.85,411.25,198.46,9.69;14,80.70,423.21,50.14,9.69">We refer the reader to <ref type="bibr" coords="14,423.80,411.25,16.28,9.69" target="#b48">[49]</ref> for more details about the features.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4" xml:id="_DmeAP6q">Comparative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_PyG2Xzb">5.4.1</head><p xml:id="_WGTkjS6"><s xml:id="_6rz9MPY" coords="14,108.44,460.65,147.70,9.32">Comparison with TCN and RNN.</s><s xml:id="_XkywcFv" coords="14,259.62,460.28,271.68,9.69;14,80.40,472.24,55.25,9.69">The mainstream approaches to processing sequences are RNN and TCN variants.</s><s xml:id="_F6zf6xK" coords="14,137.61,472.24,393.69,9.69;14,80.70,484.19,450.61,9.69;14,80.40,496.15,432.67,9.69">Here we compare the proposed Multimodal Graph with RNN and TCN variants where the Unimodal Graphs and GPFN are replaced by them so as to investigate the efectiveness of GNNs on modeling sequence (for Transformer <ref type="bibr" coords="14,151.22,496.15,14.84,9.69" target="#b50">[51]</ref>, please refer to Section ?? to see the comparison with Multimodal Transformer <ref type="bibr" coords="14,491.56,496.15,14.34,9.69" target="#b48">[49]</ref>).</s><s xml:id="_zAWxskM" coords="14,515.56,496.15,15.75,9.69;14,80.40,508.10,450.90,9.69;14,80.70,520.06,451.70,9.69;14,80.70,532.01,409.74,9.69">The TCN variants for comparison include the regular TCN that applies several 1-dimensional convolution layers to process the sequences <ref type="bibr" coords="14,179.73,520.06,12.94,9.69" target="#b1">[2]</ref>, the ResNet counterpart that applies 1-dimensional convolution (1D-ResNet) <ref type="bibr" coords="14,510.71,520.06,17.35,9.69" target="#b17">[18]</ref>, and the ResNet counterpart that applies 1-dimensional dilated convolution <ref type="bibr" coords="14,385.70,532.01,17.52,9.69" target="#b60">[61]</ref> (Dilated 1D-ResNet).</s><s xml:id="_neNXmB8" coords="14,492.93,532.01,38.38,9.69;14,80.45,543.97,230.93,9.69">The RNN variants for comparison include GRU <ref type="bibr" coords="14,234.24,543.97,11.73,9.69" target="#b6">[7]</ref> and LSTM <ref type="bibr" coords="14,292.83,543.97,14.84,9.69" target="#b18">[19]</ref>.</s><s xml:id="_mMvPWEF" coords="14,90.66,555.92,440.65,9.69;14,80.70,567.88,293.68,9.69">We can infer from Table <ref type="table" coords="14,193.70,555.92,4.73,9.69" target="#tab_3">4</ref> that GRU <ref type="bibr" coords="14,242.36,555.92,11.87,9.69" target="#b6">[7]</ref> and LSTM <ref type="bibr" coords="14,302.17,555.92,16.50,9.69" target="#b18">[19]</ref> perform competitively, and Multimodal Graph still outperforms GRU and LSTM across the majority of the evaluation metrics.</s><s xml:id="_uaXHsZF" coords="14,376.51,567.88,154.80,9.69;14,80.70,579.83,450.61,9.69;14,80.70,591.79,38.39,9.69">Speciically, Multimodal Graph reaches the best performance on binary accuracy, F1 score, MAE, and Corr metrics, and ranks second on the 7-class accuracy.</s><s xml:id="_yQpHtfC" coords="14,122.41,591.79,410.43,9.69">For the TCN variants, the regular TCN without residual learning obtains the worst performance.</s><s xml:id="_eNEyGfe" coords="14,80.47,603.74,450.83,9.69;14,80.70,615.70,450.61,9.69;14,80.70,627.65,72.91,9.69">1D-ResNet and Dilated 1D-ResNet obtain satisfactory results and outperform the regular TCN by a signiicant margin, demonstrating the efectiveness of residual learning on building up a deep convolutional network to model sequences.</s><s xml:id="_KwGBSeB" coords="14,156.71,627.65,375.69,9.69;15,147.67,178.78,16.75,9.93;15,196.89,178.78,304.43,9.93;15,125.72,191.02,39.94,9.93;15,196.89,191.02,305.55,9.93;15,122.01,203.26,47.35,9.93;15,196.89,203.26,16.47,9.93;15,80.70,387.61,202.48,9.69">Nevertheless, our Multimodal Graph still outperforms 1D-ResNet and Dilated 1D-ResNet,  <ref type="bibr" coords="15,147.67,178.78,16.75,9.93" target="#b63">[64]</ref> 77.9 32.4 75.0 1.040 0.616 79.5 49.3 78.9 0.613 0.673 MFN <ref type="bibr" coords="15,148.90,191.02,16.75,9.93" target="#b64">[65]</ref> 77.7 30.9 75.5 1.032 0.627 80.6 49.1 80.0 0.612 0.687 MFRM <ref type="bibr" coords="15,152.61,203.26,16.75,9.93" target="#b31">[32]</ref> 79. <ref type="bibr" coords="15,209.24,203.26,4.12,9.93" target="#b7">8</ref>  yielding over 1% improvements on Acc7 and Acc2.</s><s xml:id="_7uyZ49a" coords="15,285.66,387.61,245.63,9.69;15,80.70,399.56,85.16,9.69">These results demonstrate Multimodal Graph's superiority in modeling sequences.</s><s xml:id="_zwfsfPT" coords="15,168.34,399.56,363.22,9.69;15,80.33,411.52,273.48,9.69">This is partly because Multimodal Graph can automatically learn long-term dependency with a suitable adjacency matrix that links distant related time steps.</s><s xml:id="_bEDaZw3" coords="15,356.20,411.52,175.10,9.69;15,80.70,423.48,450.60,9.69;15,80.70,435.43,393.54,9.69">Moreover, diferent from the ixed modeling patterns of the TCNs and RNNs, for each time step, Multimodal Graph can identify and link various related time steps with it, which is more interpretable, lexible, and representative (see Appendix for detail).</s><s xml:id="_ekbUy69" coords="15,476.75,435.43,54.56,9.69;15,80.70,447.39,442.63,9.69">These results demonstrate GCN with an appropriate adjacency matrix as a novel and efective way of modeling sequences.</s></p><p xml:id="_83YMvua"><s xml:id="_jqqghWA" coords="15,90.66,459.34,442.32,9.69;15,80.70,471.30,393.84,9.69">Analysis of Training Time and Parameters: Intuitively, since graph convolution dispenses with the recurrence nature and allows parallel operation in the time dimension, it is faster than RNN networks.</s><s xml:id="_pC4YYvQ" coords="15,477.01,471.30,54.29,9.69;15,80.70,483.25,374.89,9.69">To verify this point, we report the training time per batch on the CMU-MOSEI dataset, as shown in Table <ref type="table" coords="15,448.87,483.25,3.36,9.69" target="#tab_3">4</ref>.</s><s xml:id="_mfy8Acv" coords="15,458.07,483.25,73.23,9.69;15,80.70,495.21,450.61,9.69;15,80.70,507.16,239.84,9.69">It can be seen that under the same experimental setting, training Multimodal Graph requires only 0.125s per batch, compared to 0.933s and 0.774s per batch for GRU and LSTM, respectively.</s><s xml:id="_eEmgFTZ" coords="15,322.90,507.16,208.41,9.69;15,80.70,519.12,352.12,9.69">Moreover, the training time of the three convolution networks (regular TCN, 1D-ResNet and Dilated 1D-ResNet) is close to that of our model.</s><s xml:id="_67835aH" coords="15,435.22,519.12,97.76,9.69;15,80.70,531.07,450.61,9.69;15,80.70,543.03,97.89,9.69">This empirically demonstrates that GCN is much more eicient than RNNs and is comparable to TCNs in terms of time complexity without sacriicing performance.</s><s xml:id="_wPRWxhD" coords="15,181.07,543.03,350.23,9.69;15,80.70,554.98,45.86,9.69">We also report the number of trainable parameters to evaluate the space complexity of the models.</s><s xml:id="_QG5xzc7" coords="15,129.05,554.98,402.44,9.69;15,80.70,566.94,46.45,9.69">It can be seen that GRU and LSTM require 1,018,811 and 917,831 parameters respectively, both fewer than GCNs.</s><s xml:id="_guAhsGq" coords="15,129.63,566.94,401.86,9.69;15,80.70,578.89,47.80,9.69">This is because we only implement three layers of GRU/LSTM which performs best according to our experiment.</s><s xml:id="_mhby6Dk" coords="15,130.99,578.89,400.30,9.69;15,80.70,590.85,278.35,9.69">When we stack more layers, the performance of GRU/LSTM decreases, which is reasonable because RNNs generally are more diicult to train when model grows deeper.</s><s xml:id="_TkgqnFF" coords="15,361.52,590.85,169.78,9.69;15,80.70,602.80,450.79,9.69;15,80.70,614.76,280.36,9.69">Additionally, the number of parameters of the three TCNs is slightly larger than that of our Multimodal Graph, demonstrating that the improvement of our model is not simply due to the increase in the number of parameters.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_kamGyJp">5.4.2</head><p xml:id="_mxeXEww"><s xml:id="_x4Y4dQV" coords="15,108.94,633.45,295.63,9.33">Comparison with Baselines for Multimodal Sentiment Analysis.</s><s xml:id="_6xmYEEU" coords="15,408.06,633.08,123.24,9.69;15,80.70,645.04,451.70,9.69">We compare our Multimodal Graph with the competitive baselines on two benchmark datasets CMU-MOSI <ref type="bibr" coords="15,416.61,645.04,16.50,9.69" target="#b66">[67]</ref> and CMU-MOSEI <ref type="bibr" coords="15,513.66,645.04,14.99,9.69" target="#b65">[66]</ref>,</s></p><p xml:id="_fuHb74s"><s xml:id="_9G38znP" coords="16,80.70,107.25,160.97,9.69">and the results are presented in Table <ref type="table" coords="16,234.87,107.25,3.40,9.69" target="#tab_4">5</ref>.</s><s xml:id="_g89PKxx" coords="16,244.14,107.25,287.15,9.69;16,80.70,119.20,96.44,9.69">It can be seen that Multimodal Graph yields best results on most of the metrics on two datasets.</s><s xml:id="_zXneRFQ" coords="16,179.62,119.20,351.68,9.69;16,80.70,131.16,247.90,9.69">Speciically, our model surpasses the state-of-the-art unaligned fusion method MulT <ref type="bibr" coords="16,512.30,119.20,19.00,9.69" target="#b48">[49]</ref> in terms of all metrics, except 7-class accuracy on CMU-MOSI.</s><s xml:id="_ebyyZfw" coords="16,330.95,131.16,200.35,9.69;16,80.70,143.11,450.60,9.69;16,80.70,155.07,34.62,9.69">The results demonstrate the efectiveness of graph convolution and graph pooling in learning sequential data, compared to Transformer <ref type="bibr" coords="16,432.34,143.11,18.31,9.69" target="#b50">[51]</ref> which is employed in MulT.</s><s xml:id="_ZTk92vB" coords="16,117.82,155.07,413.49,9.69;16,80.70,167.02,300.69,9.69">Moreover, our model also outperforms the graph-based method MTAG <ref type="bibr" coords="16,402.13,155.07,21.91,9.69" target="#b56">[57]</ref> across the majority of the evaluation metrics, further demonstrating the superiority of our method.</s><s xml:id="_mrkcGQg" coords="16,383.87,167.02,147.44,9.69;16,80.70,178.98,450.61,9.69;16,80.70,190.93,450.61,9.69;16,80.70,202.89,188.55,9.69">We argue that this is partly because compared to MTAG that averages the nodes in the graph to perform graph readout (fusion), our Multimodal Graph applies graph pooling and devises novel pooling method to aggregate the multimodal nodes and learn high-level graph representation hierarchically.</s></p><p xml:id="_jyEGGyw"><s xml:id="_BcU8Ecw" coords="16,90.66,214.84,440.64,9.69;16,80.70,226.80,202.33,9.69">Notably, the recent state-of-the-art aligned models <ref type="bibr" coords="16,290.77,214.84,14.89,9.69" target="#b16">[17,</ref><ref type="bibr" coords="16,307.50,214.84,11.41,9.69" target="#b34">35,</ref><ref type="bibr" coords="16,320.76,214.84,12.74,9.69" target="#b57">58]</ref> make great progress by using the large-pretrained BERT <ref type="bibr" coords="16,107.13,226.80,11.86,9.69" target="#b8">[9]</ref> to extract the language representation.</s><s xml:id="_UF79v8p" coords="16,285.79,226.80,247.20,9.69;16,80.70,238.75,349.82,9.69">In contrast, to make a fair comparison, we follow the stateof-the-art unaligned fusion models to use GloVe <ref type="bibr" coords="16,281.50,238.75,16.49,9.69" target="#b41">[42]</ref> to extract language embedding.</s><s xml:id="_zC9UbX3" coords="16,433.00,238.75,98.30,9.69;16,80.70,250.71,325.43,9.69">Our method can also be extended to the aligned setting and uses BERT <ref type="bibr" coords="16,276.29,250.71,11.87,9.69" target="#b8">[9]</ref> to reach remarkable results.</s><s xml:id="_AW74Btj" coords="16,408.74,250.71,122.56,9.69;16,80.70,262.67,450.61,9.69;16,80.70,274.62,250.80,9.69">In our experiment, our model achieves a binary accuracy of 85.9% and a 7-way accuracy of 48.7% on CMU-MOSI dataset under the aligned setting, which suppresses the state-of-the-art models <ref type="bibr" coords="16,298.94,274.62,15.03,9.69" target="#b16">[17,</ref><ref type="bibr" coords="16,316.47,274.62,11.27,9.69" target="#b57">58]</ref>.</s><s xml:id="_c2BzbBT" coords="16,334.00,274.62,198.40,9.69;16,80.33,286.58,325.56,9.69">Since our focus is unaligned fusion in this paper, we do not present the detailed results of aligned setting due to the lack of space.</s></p><p xml:id="_ut3THfH"><s xml:id="_9WjSnxS" coords="16,90.66,298.53,440.64,9.69;16,80.70,310.49,103.38,9.69">Additionally, we discover that our Multimodal Graph performs better on larger dataset (CMU-MOSEI) than smaller one (CMU-MOSI).</s><s xml:id="_hgpJGT2" coords="16,186.52,310.49,344.97,9.69;16,80.70,322.44,118.39,9.69">Similarly, the model performs better on IEMOCAP (see Section 5.4.3), which is a larger dataset, than on CMU-MOSI.</s><s xml:id="_g9byqGJ" coords="16,201.57,322.44,329.73,9.69;16,80.70,334.40,34.86,9.69">These results show good generalizability of our model, which is scalable to large datasets.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_SuYjG6u">5.4.3</head><p xml:id="_gu4BTNF"><s xml:id="_sNszdCd" coords="16,108.87,354.41,299.39,9.32">Comparison with Baselines for Multimodal Emotion Recognition.</s><s xml:id="_DSDjJXV" coords="16,411.74,354.04,119.55,9.69;16,80.70,365.99,450.61,9.69;16,80.70,377.95,143.24,9.69">We additionally evaluate the proposed method on the task of multimodal emotion recognition to justify the generalization ability of the model to other multimodal learning task.</s><s xml:id="_eJCUWtk" coords="16,226.75,377.95,263.91,9.69">The widely-used dataset IEMOCAP is evaluated in this section.</s><s xml:id="_VmBmXY5" coords="16,493.48,377.95,37.82,9.69;16,80.70,389.90,450.61,9.69;16,80.70,401.86,450.61,9.69;16,80.70,413.81,294.97,9.69">From the results presented in Table <ref type="table" coords="16,189.90,389.90,3.48,9.69" target="#tab_5">6</ref>, it can be seen that the Multimodal Graph outperforms the baselines in the tasks of recognizing the 'Happy', 'Angry' and 'Neutral' emotions, yielding about 3.5% improvements compared with the best results of baselines on the recognizing of the 'Neutral' emotion.</s><s xml:id="_AcUrPeF" coords="16,378.15,413.81,153.15,9.69;16,80.70,425.77,450.61,9.69;16,80.70,437.72,122.87,9.69">More importantly, Multimodal Graph outperforms the baselines in terms of the average performance, yielding over 1% improvements on the average accuracy and average F1 score.</s><s xml:id="_FWx6wMm" coords="16,206.02,437.72,325.29,9.69;16,80.70,449.68,450.61,9.69;16,80.70,461.63,171.94,9.69">In addition to the task of multimodal sentiment analysis, the extra experiments of the more challenging multimodal emotion recognition task have proven the efectiveness and generalization ability of the proposed Multimodal Graph.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4" xml:id="_45n7tTM">Ablation Studies.</head><p xml:id="_N5WcT3Y"><s xml:id="_PQCN2tF" coords="16,189.97,481.27,341.34,9.69;16,80.70,493.23,199.28,9.69">In this section, we perform ablation studies to investigate the efectiveness of the proposed graph convolution and graph pooling.</s><s xml:id="_qDFm2GP" coords="16,282.69,493.23,248.61,9.69;16,80.70,505.18,450.61,9.69;16,80.70,517.14,135.35,9.69">From the results in Table <ref type="table" coords="16,389.42,493.23,3.48,9.69">7</ref>, we discover that the introduced graph convolution and graph pooling are both beneicial to the performance of the model, without which the performance drops considerably.</s><s xml:id="_77qBmXN" coords="16,218.52,517.14,313.88,9.69;16,80.70,529.09,415.61,9.69">The graph convolution brings greater improvement than the graph pooling, indicating the importance of learning more expressive and discriminative unimodal representations.</s><s xml:id="_SRErV8G" coords="16,498.90,529.09,32.41,9.69;16,80.70,541.05,450.60,9.69;16,79.90,553.00,122.38,9.69">We also investigate the inluence of the proposed link similarity pooling by removing it from the GPFN (see the case of 'W/O LSP in GPFN' in Table <ref type="table" coords="16,192.69,553.00,3.20,9.69">7</ref>).</s><s xml:id="_3rdGs8H" coords="16,204.31,553.00,326.98,9.69;16,80.70,564.96,450.60,9.69;16,80.70,576.92,148.56,9.69">The results suggest that the link similarity pooling is efective, which demonstrates the importance of a learnable graph pooling algorithm to automatically identify and cluster the related nodes in the multimodal graph hierarchically.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.5" xml:id="_MFzVta5">Analysis of Model Complexity.</head><p xml:id="_bMaECPx"><s xml:id="_RT3bwEV" coords="16,252.11,596.56,279.19,9.69;16,80.70,608.51,450.61,9.69;16,80.70,620.47,202.35,9.69">To analyze the model complexity of Multimodal Graph, we use the number of trainable parameters as the proxy for its space complexity, and compare it with the state-of-the-art unaligned fusion methods, as reported in Table <ref type="table" coords="16,276.16,620.47,3.45,9.69" target="#tab_6">8</ref>.</s><s xml:id="_tMmmSYR" coords="16,285.53,620.47,245.77,9.69;16,80.70,632.42,362.49,9.69">It can be seen that our Multimodal Graph requires 1,225,400 trainable parameters on CMU-MOSEI, which is 64.46% of the number of parameters of MulT.</s><s xml:id="_WRcYQjU" coords="16,445.09,632.42,86.21,9.69;16,80.70,644.38,450.90,9.69;17,80.46,105.60,168.91,8.80">Although Multimodal Graph requires fewer trainable parameters than the current state-of-the-art model MulT, it still outperforms MulT Table <ref type="table" coords="17,102.61,105.60,3.13,8.80">7</ref>. Ablation studies on CMU-MOSEI.</s><s xml:id="_KGWMcTP" coords="17,251.60,105.60,279.70,8.80;17,80.70,116.56,37.80,8.80">In the case of 'W/O Graph Pooling', we remove the graph pooling layers in the GPFN.</s><s xml:id="_6VHEYuN" coords="17,120.63,116.56,410.84,8.80;17,80.70,127.52,262.71,8.80">In the case of 'W/O Graph Convolution', we replace the graph convolution network with the fully connected layer to map the feature dimensionality of diferent modalities to be the same.</s><s xml:id="_R2Bw542" coords="17,345.60,127.52,185.93,8.80;17,80.70,138.48,130.59,8.80">'W/O LSP in GPFN' denotes that the link similarity pooling is removed from the GPFN.</s><s xml:id="_nRzURhf" coords="17,185.23,585.45,346.66,9.69;17,80.70,597.41,18.72,9.69"><ref type="bibr" coords="17,185.23,585.45,16.22,9.69" target="#b15">[16]</ref> with Graph Attention Network (GAT) <ref type="bibr" coords="17,355.65,585.45,16.22,9.69" target="#b51">[52]</ref> and Graph Isomorphism Network (GIN) <ref type="bibr" coords="17,80.70,597.41,14.97,9.69" target="#b55">[56]</ref>.</s><s xml:id="_JWSuBSZ" coords="17,101.91,597.41,429.39,9.69;17,80.70,609.36,25.78,9.69">From Table <ref type="table" coords="17,151.06,597.41,4.72,9.69" target="#tab_7">9</ref> it can be seen that GraphSAGE <ref type="bibr" coords="17,288.70,597.41,16.49,9.69" target="#b15">[16]</ref> reaches the best performance among all the compared GCNs.</s><s xml:id="_NXDrYAU" coords="17,108.93,609.36,422.63,9.69;17,80.70,621.32,88.21,9.69">In addition, the results of GIN <ref type="bibr" coords="17,230.39,609.36,16.22,9.69" target="#b55">[56]</ref> and GAT <ref type="bibr" coords="17,287.35,609.36,16.22,9.69" target="#b51">[52]</ref> are also satisfactory, indicating the generalization ability of Multimodal Graph.</s></p><p xml:id="_HPQqUbY"><s xml:id="_7tBCxp9" coords="17,90.66,633.28,440.64,9.69;17,80.70,645.23,81.39,9.69">As for the graph pooling methods, we implement and evaluate DifPool <ref type="bibr" coords="17,383.72,633.28,16.37,9.69" target="#b59">[60]</ref> as the baseline to compare with the proposed GPFN.</s><s xml:id="_Sq8NTxN" coords="17,164.58,645.23,318.61,9.69">DifPool <ref type="bibr" coords="17,200.55,645.23,16.33,9.69" target="#b59">[60]</ref> achieves a relatively good result and is still inferior to our model.</s><s xml:id="_SfunSHY" coords="17,485.70,645.23,45.61,9.69">To be more</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5" xml:id="_nRVZeYq">Discussion of Adjacency Matrices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1" xml:id="_KCydNSf">The Comparison of Diferent Adjacency Matrices.</head><p xml:id="_f2cyjJs"><s xml:id="_5ESpqu8" coords="18,335.92,302.29,195.38,9.69;18,80.70,314.24,450.61,9.69;18,80.70,326.20,27.25,9.69">To analyze the performance of diferent kinds of adjacency matrices, we conduct an experiment where diferent types of adjacency matrix are used in Multimodal Graph.</s><s xml:id="_gnGVSGE" coords="18,110.28,326.20,422.12,9.69;18,80.70,338.15,450.86,9.69;18,80.70,350.11,293.20,9.69">To show the efectiveness of the proposed types of adjacency matrix, i.e., indirect learning, direct learning, GDM and KNN-based adjacency matrices, we additionally implement one comparative trial where the adjacency matrix is an all-one matrix which corresponds to a fully-connected graph.</s><s xml:id="_FKbfqVv" coords="18,376.23,350.11,155.07,9.69;18,80.70,362.06,450.61,9.69;18,80.70,374.02,452.14,9.69">We can infer from Table <ref type="table" coords="18,474.40,350.11,9.08,9.69" target="#tab_8">10</ref> that among all the adjacency matrices, the all-one matrix performs worst, which is understandable because the GCN in this case will reduce into a fully-connected graph with no ability to discern various relatedness between nodes.</s><s xml:id="_Ec3uDaS" coords="18,80.70,385.97,450.61,9.69;18,80.70,397.93,366.17,9.69">Besides, the KNN-based adjacency matrix reaches a relatively low performance, indicating that the Euclidean distance is not a perfect choice to determine the correlation between heterogeneous nodes.</s><s xml:id="_fX9cWmP" coords="18,449.38,397.93,81.93,9.69;18,80.70,409.88,263.97,9.69">In contrast, both the indirect learning method and the GDM achieve satisfactory results.</s><s xml:id="_SBjv9Gw" coords="18,346.65,409.88,184.65,9.69;18,80.70,421.84,388.60,9.69">This is because GDM is in line with the general pattern of sequence modeling which sticks out the present time step and dilutes the past ones.</s><s xml:id="_fZXHTem" coords="18,471.79,421.84,59.52,9.69;18,80.70,433.79,381.13,9.69">And the direct learning method can learn such pattern in the absence of prior knowledge by gradient descent.</s><s xml:id="_aJz4Xjg" coords="18,464.32,433.79,66.98,9.69;18,80.70,445.75,246.61,9.69">Additionally, the indirect learning method, produces best results on all metrics.</s><s xml:id="_fY7m3Wb" coords="18,329.79,445.75,201.52,9.69;18,80.70,457.70,450.84,9.69;18,80.70,469.66,106.43,9.69">One possible reason is that it is both learnable and instance-speciic, and therefore it is more representative and can be optimized to capture more subtle and complex relatedness among nodes.</s><s xml:id="_sN7J8BR" coords="18,189.60,469.66,341.70,9.69;18,80.70,481.61,450.61,9.69;18,80.70,493.57,450.60,9.69;18,80.70,505.53,450.60,9.69;18,80.70,517.48,26.06,9.69">To sum up, the comparative results that diferent types of adjacency matrices yield suggest that: (1) the proposed adjacency matrices are helpful in capturing useful information on the relatedness among nodes, compared to an all-one adjacency matrix that contains no such information, and (2) a learnable and meanwhile instance-speciic adjacency matrix is crucial for capturing more relatedness information among nodes.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.2" xml:id="_RQxcYPF">Visualization of Adjacency Matrices.</head><p xml:id="_gQ4rBB4"><s xml:id="_PKEqctm" coords="18,279.87,536.89,251.69,9.69;18,80.70,548.85,419.60,9.69">To analyze the attributes of the indirect learning adjacency matrices, we provide a visualization of the unimodal and multimodal indirect learning adjacency matrices.</s><s xml:id="_sFzwtR9" coords="18,502.45,548.85,28.85,9.69;18,80.70,560.80,450.61,9.69;18,80.70,572.76,404.73,9.69">Instead of merely analyzing a few instances, we average the adjacency matrices of all the testing instances in CMU-MOSEI to reveal the general patterns of the adjacency matrices, which is more convincing and informative.</s></p><p xml:id="_k27Pr8g"><s xml:id="_94uaTRf" coords="18,90.66,584.71,440.64,9.69;18,80.70,596.67,450.86,9.69;18,80.70,608.62,450.61,9.69;18,80.70,620.58,367.50,9.69">Unimodal Adjacency Matrices: As can be inferred from Fig. <ref type="figure" coords="18,349.45,584.71,3.48,9.69" target="#fig_2">3</ref>, for the visual and acoustic modalities, the latter portion of nodes have much more impact than their counterparts in that they have more intensive link association (the nodes tends to have more connections with the latter portion of nodes), indicting that the model heavily relies on the latter portion of nodes for prediction and they are more informative.</s><s xml:id="_tVFe32b" coords="18,450.68,620.58,80.88,9.69;18,80.70,632.54,450.61,9.69;18,80.70,644.49,293.00,9.69">In contrast, the link association between diferent nodes in language modality is more even, indicting that the connections between diferent words are distributed evenly at nearly all temporal positions.</s><s xml:id="_r7NWsyU" coords="18,376.52,644.49,155.04,9.69;19,80.46,562.82,423.61,8.80">Interestingly, the language adjacency  The first 50, the middle 500 and the later 500 nodes belong to language, acoustic, and visual modality, respectively.</s></p><p xml:id="_AHCAynK"><s xml:id="_AsKCejz" coords="19,80.70,585.37,290.47,9.69">matrix suggests that the language nodes have fewer self-connections.</s><s xml:id="_3MQC9Ax" coords="19,374.05,585.37,157.25,9.69;19,80.70,597.33,450.60,9.69;19,80.70,609.28,450.60,9.69;19,80.70,621.24,56.17,9.69">This is reasonable because during the graph convolution, we add self-loop operation such that the nodes can connect with themselves (see Eq. 4), so the adjacency matrix does not need to learn a strong self-connection value on the diagonal, otherwise it would be redundant.</s><s xml:id="_GubJbYS" coords="19,90.66,633.19,440.88,9.69;19,80.70,645.15,437.40,9.69">Cross-modal Adjacency Matrices: We also conduct visualization of the indirect learning adjacency matrix for multimodal sequence, and we provide the visualization of KNN-based adjacency matrix for comparison.</s><s xml:id="_ZjbumVy" coords="19,520.58,645.15,10.72,9.69;20,80.70,107.25,450.61,9.69;20,80.70,119.20,450.61,9.69;20,80.70,131.16,60.65,9.69">As can be inferred from Fig. <ref type="figure" coords="20,184.08,107.25,3.43,9.69" target="#fig_3">4</ref>, for KNN-based method, the nodes from diferent modalities have no direct or close interactions with each other, and these nodes only interact with their neighboring nodes that come from the same modality.</s><s xml:id="_2pNhRsq" coords="20,143.84,131.16,387.47,9.69;20,80.70,143.11,364.80,9.69">This means that by using the KNN-based adjacency matrix, the cross-modal interactions cannot be efectively explored, which may explain why it performs worse than other methods.</s><s xml:id="_9QxC4Ue" coords="20,448.38,143.11,82.92,9.69;20,80.70,155.07,450.87,9.69;20,80.70,167.02,249.74,9.69">The visualization of the KNN-based adjacency matrix also suggests that the distribution gap between diferent modalities actually exists and we need to handle it during modality fusion <ref type="bibr" coords="20,311.70,167.02,14.99,9.69" target="#b30">[31]</ref>.</s><s xml:id="_Bh45v2f" coords="20,333.21,167.02,198.10,9.69;20,80.70,178.98,450.61,9.69;20,80.70,190.93,450.61,9.69;20,80.70,202.89,178.73,9.69">In contrast, for adjacency matrix of the indirect learning method, interestingly, the mean adjacency matrix for multimodal sequence suggests that the nodes tend to connect with the language nodes (i.e., the irst 50 nodes) more closely, which indicates that the language nodes are more important and informative.</s><s xml:id="_fNZzQy3" coords="20,261.95,202.89,269.35,9.69;20,80.70,214.84,266.21,9.69">This scenario can be partly explained by the fact that language is more important than the other modalities, as revealed in <ref type="bibr" coords="20,314.32,214.84,15.05,9.69" target="#b29">[30,</ref><ref type="bibr" coords="20,331.86,214.84,11.28,9.69" target="#b42">43]</ref>.</s><s xml:id="_usvuWup" coords="20,349.39,214.84,181.92,9.69;20,80.70,226.80,450.61,9.69;20,80.70,238.75,268.84,9.69">Additionally, as the adjacency matrix for the language nodes (the irst 50 nodes) has the darkest color, our visualization suggests the language nodes have the strongest connection with each other in the multimodal sequence.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" xml:id="_zgDJEkb">CONCLUSION</head><p xml:id="_7e8HhG6"><s xml:id="_RQPhZza" coords="20,80.22,277.61,451.08,9.69;20,80.70,289.56,154.53,9.69">We employed popular GNNs to process multimodal sequences, which was free of the recurrent structure and proved more eicient and competitive.</s><s xml:id="_BYXSN4T" coords="20,237.72,289.56,293.58,9.69;20,80.70,301.52,452.14,9.69">Speciically, we developed a unimodal graph for each modality to explore intra-modal dynamics, and a graph pooling fusion network over Unimodal Graphs to learn inter-modal dynamics.</s><s xml:id="_vawy9a6" coords="20,80.22,313.47,316.00,9.69">We proposed multiple ways to construct an adjacency matrix for a sequence.</s><s xml:id="_W6WR8ef" coords="20,398.73,313.47,133.75,9.69;20,80.47,325.43,451.93,9.69;20,80.33,337.38,450.98,9.69;20,80.70,349.34,450.61,9.69;20,80.70,361.30,450.61,9.69;20,80.70,373.25,450.60,9.69;20,80.70,385.21,322.27,9.69">The experiments suggested that: 1) the proposed GCN-based model outperformed RNN and TCN variants with high computational eiciency, which indicated a new research direction in modeling sequences; 2) the proposed learnable and instance-speciic methods to deine an adjacency matrix performed best; 3) our method outperformed the transformer-based model MulT, which indicated the efectiveness of graph-based models; 4) the visualization results suggested that our model can identify important intra-and inter-modal interactions; 5) the proposed GPFN outperformed the classical graph pooling method DifPool in fusing cross-modal information.</s><s xml:id="_A94ZzmJ" coords="20,405.46,385.21,125.84,9.69;20,80.70,397.16,285.31,9.69">However, Multimodal Graph is non-causal, and thus not applicable to some language processing tasks.</s><s xml:id="_e47ZNhW" coords="20,368.50,397.16,162.80,9.69;20,80.70,409.12,163.46,9.69">In the future, we aim to develop a causal GCN-based model to process sequences.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" xml:id="_Fe8KP6a">ACKNOWLEDGMENT</head><p xml:id="_qUzdYhd"><s xml:id="_yEeEsSK" coords="20,80.40,447.97,415.78,9.69">This work was supported by the National Natural Science Foundation of China under Grant 62076262.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="5,80.70,311.95,450.61,8.80;5,80.70,323.24,293.78,8.47"><head>Fig. 1 .</head><label>1</label><figDesc><div><p xml:id="_YrrUecQ"><s xml:id="_xTxKGWA" coords="5,80.70,311.95,21.10,8.80">Fig. 1.</s><s xml:id="_9TuuHHw" coords="5,106.28,312.28,187.67,8.47">The Schematic Diagram of Multimodal Graph.</s><s xml:id="_fTVQmm2" coords="5,296.00,312.28,235.31,8.47;5,80.70,323.24,168.12,8.47">Multimodal Graph consists of three Unimodal Graphs and a Graph Pooling Fusion Network (GPFN).</s><s xml:id="_KxfygFU" coords="5,251.06,323.24,123.42,8.47">Adj denotes adjacency matrix.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="9,80.70,337.78,450.61,8.81;9,80.33,349.07,200.08,8.47"><head>Fig. 2 .</head><label>2</label><figDesc><div><p xml:id="_6gShQDm"><s xml:id="_zPe3b6g" coords="9,80.70,337.78,156.96,8.80">Fig. 2. The Detailed Structure of GPFN.</s><s xml:id="_zd6wuwa" coords="9,239.49,338.11,91.21,8.48">s denotes pooling size.</s><s xml:id="_Y2j7tSh" coords="9,332.90,338.11,198.41,8.47;9,80.33,349.07,200.08,8.47">The arrow in the figure indicates the direction in which the layers are stacked, from botom to top.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="19,80.70,271.02,450.61,8.80;19,80.70,281.98,451.59,8.80;19,80.70,292.94,450.61,8.80;19,80.70,303.90,230.70,8.80"><head>Fig. 3 .</head><label>3</label><figDesc><div><p xml:id="_5XXr2nm"><s xml:id="_bagrsfg" coords="19,80.70,271.02,20.79,8.80">Fig.3.</s><s xml:id="_45CAGuf" coords="19,105.97,271.35,261.47,8.47">Visualization of Unimodal Indirect Learning Adjacency Matrices.</s><s xml:id="_8jMkUx6" coords="19,369.19,271.02,162.11,8.80;19,80.70,281.98,146.49,8.80">Each grid in the figure reflects the interaction between each corresponding two nodes.</s><s xml:id="_ByENsjG" coords="19,229.43,281.98,302.85,8.80;19,80.70,292.94,52.68,8.80">A darker color reflect a stronger interaction between the corresponding two nodes, and vice versa.</s><s xml:id="_V4nqNtT" coords="19,135.49,292.94,395.81,8.80;19,80.70,303.90,230.70,8.80">For indirect learning, since the adjacency matrices are instance-specific, we average the adjacency matrices of the testing instances and visualize the mean adjacency matrix.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="19,80.70,529.95,452.09,8.80;19,80.70,540.91,450.60,8.81;19,80.70,551.86,451.99,8.80;19,80.46,562.82,423.61,8.80"><head>Fig. 4 .</head><label>4</label><figDesc><div><p xml:id="_gMxZw7r"><s xml:id="_mFEnAG2" coords="19,80.70,529.95,21.46,8.80">Fig.<ref type="bibr" coords="19,96.08,529.95,3.04,8.80" target="#b3">4</ref>.</s><s xml:id="_xKgThg8" coords="19,106.64,530.28,275.53,8.47">Visualization of Cross-modal Indirect Learning Adjacency Matrices.</s><s xml:id="_w79NtXq" coords="19,384.40,529.95,148.38,8.80;19,80.70,540.91,81.12,8.80">A darker color indicates a stronger interaction, and vice versa.</s><s xml:id="_XECnBHx" coords="19,164.05,540.91,158.40,8.81">The labels of x and y axes are both 'nodes'.</s><s xml:id="_UYaCzXe" coords="19,324.69,540.91,206.61,8.80;19,80.70,551.86,451.99,8.80">Since the KNN-based and indirect learning methods are both instance-specific, we average the adjacency matrices of the testing instances and visualize the mean adjacency matrices.</s><s xml:id="_Mx9qQwm" coords="19,80.46,562.82,423.61,8.80">The first 50, the middle 500 and the later 500 nodes belong to language, acoustic, and visual modality, respectively.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" coords="6,107.96,409.64,204.89,101.23"><head>Table 1 .</head><label>1</label><figDesc><div><p xml:id="_ByyEa5j"><s xml:id="_x2p3HgS" coords="6,193.53,409.64,67.87,8.80">Table of Notations</s></p></div></figDesc><table coords="6,107.96,423.91,149.39,15.23"><row><cell>Notations</cell><cell>Descriptions</cell></row><row><cell>G</cell><cell></cell></row></table><note coords="6,113.83,430.93,3.36,3.89;6,120.09,432.25,14.74,6.97;6,136.05,430.93,3.36,3.89;6,139.89,436.66,2.01,1.79;6,143.92,432.31,4.12,6.82;6,148.63,430.93,3.36,3.89;6,153.06,432.25,113.90,6.88;6,268.18,430.93,3.36,3.89;6,274.17,432.28,31.51,6.86;6,306.27,430.93,3.36,3.89;6,125.84,438.90,11.25,6.56;6,177.89,440.25,122.55,6.86;6,125.69,448.25,5.19,6.82;6,131.93,446.87,5.45,3.89;6,190.77,448.22,97.24,6.79;6,128.95,456.22,4.20,6.82;6,171.50,456.19,135.79,6.79;6,129.06,464.50,5.48,4.87;6,214.20,464.16,50.38,6.79;6,129.72,472.16,2.44,6.82;6,197.19,472.13,84.39,6.79;6,129.76,480.13,3.86,6.82;6,165.92,480.10,146.94,6.79;6,129.75,488.10,3.32,6.82;6,167.44,488.07,143.89,6.79;6,129.43,496.07,4.37,6.82;6,195.51,496.04,87.76,6.79;6,130.00,504.04,2.91,6.82;6,189.65,504.01,99.48,6.79"><p xml:id="_Rn7bjca"><s xml:id="_bXwWafA" coords="6,113.83,430.93,3.36,3.89;6,120.09,432.25,14.74,6.97;6,136.05,430.93,3.36,3.89;6,139.89,436.66,2.01,1.79;6,143.92,432.31,4.12,6.82;6,148.63,430.93,3.36,3.89;6,153.06,432.25,113.90,6.88;6,268.18,430.93,3.36,3.89;6,274.17,432.28,31.51,6.86;6,306.27,430.93,3.36,3.89;6,125.84,438.90,11.25,6.56;6,177.89,440.25,122.55,6.86;6,125.69,448.25,5.19,6.82;6,131.93,446.87,5.45,3.89;6,190.77,448.22,97.24,6.79;6,128.95,456.22,4.20,6.82;6,171.50,456.19,135.79,6.79;6,129.06,464.50,5.48,4.87;6,214.20,464.16,50.38,6.79;6,129.72,472.16,2.44,6.82;6,197.19,472.13,84.39,6.79;6,129.76,480.13,3.86,6.82;6,165.92,480.10,146.94,6.79;6,129.75,488.10,3.32,6.82;6,167.44,488.07,143.89,6.79;6,129.43,496.07,4.37,6.82;6,195.51,496.04,87.76,6.79;6,130.00,504.04,2.91,6.82;6,189.65,504.01,99.48,6.79">a = (N a , E a ) Graph for modality a with node N a and edge E a N a Sequence (node embedding) for modality a N m Node set for multimodal sequence T The number of nodes (time steps) in a sequence A Adjacency matrix f Nonlinear activation function λ Attenuation factor for Generalized Diagonal Matrix ϵ A small negative scalar to prevent division by zero R Matrix transposition operation s Pooling size for mean/max pooling</s></p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="6,348.77,409.64,149.03,101.16"><head>Table 2 .</head><label>2</label><figDesc><div><p xml:id="_k86xFR6"><s xml:id="_zXE6pHz" coords="6,388.17,409.64,78.61,8.80">Table of Abbreviation</s></p></div></figDesc><table coords="6,348.77,423.91,149.03,86.89"><row><cell>Abbreviation</cell><cell>Full Name</cell></row><row><cell>GPFN</cell><cell>Graph Pooling Fusion Network</cell></row><row><cell>GCN</cell><cell>Graph Convolution Network</cell></row><row><cell>Adj</cell><cell>Adjacency Matrix</cell></row><row><cell>LSP</cell><cell>Link Similarity Pooling</cell></row><row><cell>GDM</cell><cell>Generalized Diagonal Matrix</cell></row><row><cell>TCN</cell><cell>Temporal Convolution Network</cell></row><row><cell>KNN</cell><cell>K-Nearest Neighbor</cell></row><row><cell>RNN</cell><cell>Recurrent Neural Network</cell></row><row><cell>GRU</cell><cell>Gated Recurrent Units</cell></row><row><cell>LSTM</cell><cell>Long Short-Term Memory Network</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="14,163.01,105.60,283.50,111.94"><head>Table 3 .</head><label>3</label><figDesc><div><p xml:id="_PY4qtPZ"><s xml:id="_wVK4hve" coords="14,248.23,105.60,147.72,8.80">Hyperparameters of Multimodal Graph.</s></p></div></figDesc><table coords="14,163.01,128.89,283.50,88.65"><row><cell></cell><cell cols="3">CMU-MOSI CMU-MOSEI IEMOCAP</cell></row><row><cell>Batch Size</cell><cell>50</cell><cell>50</cell><cell>24</cell></row><row><cell>Initial Learning Rate</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell></row><row><cell>Training Epochs</cell><cell>50</cell><cell>20</cell><cell>15</cell></row><row><cell>Gradient Clip</cell><cell>0.8</cell><cell>0.8</cell><cell>0.8</cell></row><row><cell>Pooling Sequential Length (T ′ )</cell><cell>70</cell><cell>75</cell><cell>50</cell></row><row><cell>Feature Dimensionality (d)</cell><cell>50</cell><cell>40</cell><cell>80</cell></row><row><cell>Convolution Layers (r )</cell><cell>2</cell><cell>2</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="14,80.46,224.14,450.85,127.53"><head>Table 4 .</head><label>4</label><figDesc><div><p xml:id="_Nu9TyTB"><s xml:id="_sR3dZTY" coords="14,115.90,224.47,169.13,8.47">Comparison with RNN and TCN variants.</s><s xml:id="_taBQuGa" coords="14,287.41,224.14,209.94,8.80">The GRU and LSTM models used here are bidirectional.</s><s xml:id="_SPXmUQh" coords="14,499.72,224.14,31.59,8.80;14,80.46,235.10,337.93,8.80">Training Time means training time of the model per batch (the batch size is the same for all models).</s></p></div></figDesc><table coords="14,96.35,258.66,416.82,93.02"><row><cell></cell><cell>Acc2 Acc7 F1</cell><cell cols="3">MAE Corr Training Time (s) Parameters</cell></row><row><cell>GRU</cell><cell cols="2">80.4 50.5 80.5 0.611 0.669</cell><cell>0.933</cell><cell>1,018,811</cell></row><row><cell>LSTM</cell><cell cols="2">80.7 48.3 81.3 0.623 0.662</cell><cell>0.774</cell><cell>917,831</cell></row><row><cell>Regular TCN</cell><cell cols="2">62.9 41.4 77.2 0.838 0.007</cell><cell>0.120</cell><cell>1,307,071</cell></row><row><cell>1D-ResNet</cell><cell cols="2">79.8 48.4 80.1 0.626 0.646</cell><cell>0.134</cell><cell>1,367,561</cell></row><row><cell cols="3">Dilated 1D-ResNet 80.4 48.0 80.7 0.636 0.644</cell><cell>0.127</cell><cell>1,367,561</cell></row><row><cell cols="3">Multimodal Graph 81.4 49.7 81.7 0.608 0.675</cell><cell>0.125</cell><cell>1,225,400</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" coords="15,126.16,105.60,375.16,83.11"><head>Table 5 .</head><label>5</label><figDesc><div><p xml:id="_AxzpKwB"><s xml:id="_B5FuBcq" coords="15,166.33,105.93,313.76,8.47">Performance of Multimodal Graph on CMU-MOSI and CMU-MOSEI datasets.</s></p></div></figDesc><table coords="15,126.16,129.00,375.16,59.71"><row><cell></cell><cell cols="2">CMU-MOSI</cell><cell cols="2">CMU-MOSEI</cell></row><row><cell>Methods</cell><cell>Acc2 Acc7 F1</cell><cell cols="2">MAE Corr Acc2 Acc7 F1</cell><cell>MAE Corr</cell></row><row><cell>LF-LSTM</cell><cell cols="4">74.5 31.3 74.3 1.042 0.608 79.5 48.0 79.6 0.632 0.650</cell></row><row><cell>EF-LSTM</cell><cell cols="4">73.7 32.2 73.5 1.038 0.594 65.3 41.7 76.0 0.799 0.265</cell></row><row><cell>TFN</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" coords="15,107.08,203.26,394.24,172.07"><head>Table 6 .</head><label>6</label><figDesc><div><p xml:id="_QvN3w5F"><s xml:id="_tVV9rQP" coords="15,150.06,260.75,346.31,8.47">Comparison between Multimodal Graph and other approaches on IEMOCAP dataset.</s><s xml:id="_NvcVujg" coords="15,268.12,354.14,210.94,9.21;15,129.88,366.12,350.00,9.21">79.5 72.8 76.2 69.8 57.8 56.9 74.8 69.8 Multimodal Graph 86.0 81.3 79.4 72.3 76.8 71.9 61.4 59.8 75.9 71.3</s></p></div></figDesc><table coords="15,107.08,203.26,394.24,160.08"><row><cell></cell><cell cols="10">34.7 79.4 0.956 0.673 80.3 48.9 80.6 0.617 0.669</cell></row><row><cell>MulT [49]</cell><cell cols="10">80.6 35.3 79.3 0.972 0.681 80.1 49.0 80.9 0.630 0.664</cell></row><row><cell>MTAG [57]</cell><cell cols="10">80.5 31.9 80.4 0.941 0.692 79.1 48.2 75.9 0.645 0.614</cell></row><row><cell cols="11">Multimodal Graph 80.6 32.1 80.5 0.933 0.684 81.4 49.7 81.7 0.608 0.675</cell></row><row><cell></cell><cell cols="2">Happy</cell><cell>Sad</cell><cell></cell><cell cols="2">Angry</cell><cell cols="2">Neutral</cell><cell cols="2">Average</cell></row><row><cell>Models</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell></row><row><cell>MFN [65]</cell><cell cols="10">79.7 78.9 73.1 71.7 72.8 66.9 57.9 56.5 70.9 68.5</cell></row><row><cell>TFN [67]</cell><cell cols="10">77.1 76.8 68.9 63.2 71.7 66.8 51.0 51.2 67.2 64.5</cell></row><row><cell>MulT [49]</cell><cell cols="10">85.7 79.4 79.3 70.5 75.8 65.4 51.8 52.2 73.2 66.9</cell></row><row><cell>MFRM [32]</cell><cell cols="10">85.9 79.6 76.7 71.4 76.0 68.9 55.1 53.2 73.4 68.3</cell></row><row><cell>MTAG [57]</cell><cell cols="2">85.8 79.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" coords="17,80.46,162.11,452.33,137.83"><head>Table 8 .</head><label>8</label><figDesc><div><p xml:id="_ZD8dGSX"><s xml:id="_Zpgtu7G" coords="17,112.90,240.60,226.01,8.47">The Comparison of Model Complexity on CMU-MOSEI.</s><s xml:id="_kAXFzah" coords="17,341.14,240.27,191.65,8.80;17,80.70,251.23,83.32,8.80">We implement three widely-used baselines for comparison in this section.</s></p></div></figDesc><table coords="17,118.92,162.11,371.67,137.83"><row><cell></cell><cell></cell><cell cols="2">Acc2 Acc7 F1</cell><cell>MAE Corr</cell></row><row><cell cols="4">W/O Graph Convolution 79.7 48.2 80.1 0.639 0.645</cell></row><row><cell cols="2">W/O Graph Pooling</cell><cell cols="2">80.4 49.6 80.5 0.612 0.671</cell></row><row><cell cols="2">W/O LSP in GPFN</cell><cell cols="2">80.8 49.2 81.1 0.610 0.674</cell></row><row><cell cols="2">Multimodal Graph</cell><cell cols="2">81.4 49.7 81.7 0.608 0.675</cell></row><row><cell>Methods</cell><cell cols="3">TFN [64] MulT [49] MFN [65] Multimodal Graph</cell></row><row><cell cols="3">Number of Parameters 1,002,471 1,901,161</cell><cell>963,777</cell><cell>1,225,400</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" coords="17,80.40,309.02,450.91,286.12"><head>Table 9 .</head><label>9</label><figDesc><div><p><s xml:id="_5qzUV9A" coords="17,115.67,309.02,415.64,8.80;17,80.70,319.98,450.61,8.80;17,80.70,330.94,450.61,8.80;17,80.70,341.90,32.28,8.80;17,189.17,471.88,342.13,9.69;17,80.70,483.84,450.61,9.69;17,80.40,495.79,450.90,9.69;17,80.70,507.75,450.61,9.69;17,80.70,519.70,450.61,9.69;17,80.70,531.66,322.41,9.69;17,80.70,549.96,244.96,9.33">Discussion on the Concrete Graph Neural Networks on CMU-MOSEI.For the case of 'GAT' and 'GIN', we replace the defaulted graph convolution model GraphSAGE in Unimodal Graphs and GPFN with the corresponding GAT and GIN model.For the case of 'DifPool', we replace the max/mean graph pooling and link similarity pooling in GPFN with DifPool.This advantage in space complexity is signiicant because it shows that in addition to being efective in modeling sequential data, our Multimodal Graph is also more eicient than the variant of Transformer which is one dominant sequence learning method.This further validates GCN as a novel way of modeling sequential data.Compared to TFN and MFN, the proposed Multimodal Graph has more parameters.To sum up, given the high empirical performance of Multimodal Graph, the space complexity of Multimodal Graph is moderate compared to state-of-the-art unaligned sequence analysis methods.5.4.6Discussion of Diferent Graph Neural Networks.</s><s xml:id="_cEV3uaa" coords="17,329.14,549.59,202.16,9.69;17,80.70,561.54,343.28,9.69">Since our Multimodal Graph is independent of the concrete GCN structure, we can choose any GCN structure to implement our model.</s><s xml:id="_4z8P3dD" coords="17,426.46,561.54,104.84,9.69;17,80.70,573.50,351.42,9.69">This subsection compares diferent current GCN structures to analyze which kind of GCN structures performs best.</s><s xml:id="_DtgG3AX" coords="17,434.24,573.50,97.07,9.69;17,80.70,585.45,102.40,9.69">Speciically, we compare the defaulted GraphSAGE</s></p></div></figDesc><table coords="17,80.70,365.78,375.05,115.80"><row><cell>Models</cell><cell>Acc2 Acc7 F1</cell><cell>MAE Corr</cell></row><row><cell>GAT [52]</cell><cell cols="2">80.3 49.0 80.4 0.627 0.650</cell></row><row><cell>GIN [56]</cell><cell cols="2">81.1 49.0 81.6 0.615 0.676</cell></row><row><cell cols="3">GraphSAGE [16] 81.4 49.7 81.7 0.608 0.675</cell></row><row><cell>DifPool [60]</cell><cell cols="2">81.1 49.8 81.3 0.611 0.670</cell></row><row><cell>GPFN</cell><cell cols="2">81.4 49.7 81.7 0.608 0.675</cell></row><row><cell>as shown in Section 5.4.2.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" coords="18,80.70,105.60,450.61,168.04"><head>Table 10 .</head><label>10</label><figDesc><div><p xml:id="_vGfRYBa"><s xml:id="_pUCQbCy" coords="18,213.21,105.93,221.93,8.47">Discussion on the adjacency matrices on CMU-MOSEI.</s><s xml:id="_R5q6EH5" coords="18,115.24,240.03,416.06,9.69;18,80.70,251.99,185.17,9.69">DifPool outperforms our GPFN in terms of 7-class accuracy by 0.1 points, while our model outperforms DifPool on the rest of the evaluation metrics.</s><s xml:id="_8weW8je" coords="18,268.36,251.99,262.93,9.69;18,80.70,263.95,33.16,9.69">These results demonstrate the efectiveness of our graph pooling method.</s></p></div></figDesc><table coords="18,80.70,129.45,375.13,120.27"><row><cell></cell><cell>Acc2 Acc7 F1</cell><cell>MAE Corr</cell></row><row><cell>All-one Matrix</cell><cell cols="2">78.8 49.0 79.3 0.644 0.623</cell></row><row><cell>KNN</cell><cell cols="2">80.3 45.8 80.6 0.659 0.625</cell></row><row><cell>GDM</cell><cell cols="2">81.1 49.3 81.2 0.617 0.666</cell></row><row><cell>Direct Learning</cell><cell cols="2">80.7 49.3 81.2 0.618 0.659</cell></row><row><cell cols="3">Indirect Learning 81.4 49.7 81.7 0.608 0.675</cell></row><row><cell>speciic,</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="3,367.56,670.61,163.74,7.75"><p xml:id="_NAV3jdR"><s xml:id="_tJPwZFq" coords="3,367.56,670.61,39.48,7.75">ACM Trans.</s><s xml:id="_XGDEv8M" coords="3,409.38,670.61,67.55,7.75">Multimedia Comput.</s><s xml:id="_CgVjxtw" coords="3,479.24,670.61,32.06,7.75">Commun.</s><s xml:id="_qDmQWSz" coords="3,513.63,670.61,17.67,7.75">Appl.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1" coords="13,83.93,646.56,166.94,7.75"><p xml:id="_n7cNVAk"><s xml:id="_AKtKhv7" coords="13,83.93,646.56,166.94,7.75">https://github.com/A2Zadeh/CMU-MultimodalSDK</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2" coords="14,86.06,646.59,119.80,7.75"><p xml:id="_eRDysGC"><s xml:id="_VjUFawG" coords="14,86.06,646.59,47.56,7.75">iMotions 2017.</s><s xml:id="_RqVpBN9" coords="14,135.61,646.59,70.25,7.75">https://imotions.com/</s></p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct coords="20,97.77,486.30,433.53,7.75;20,97.77,496.47,81.16,7.54" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="20,253.50,486.30,129.19,7.75" xml:id="_WZZfGQA">Trellis Networks for Sequence Modeling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Shaojie Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladlen</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_V8B9SWd" coord="20,396.99,486.51,134.31,7.54;20,97.77,496.47,77.98,7.54">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shaojie Bai, J. Kolter, and Vladlen Koltun. 2019. Trellis Networks for Sequence Modeling. In Proceedings of International Conference on Learning Representations.</note>
</biblStruct>

<biblStruct coords="20,97.77,506.22,433.69,7.75;20,97.77,516.18,193.33,7.75" xml:id="b1">
	<monogr>
		<title level="m" type="main" coord="20,271.38,506.22,260.08,7.75;20,97.77,516.18,61.62,7.75" xml:id="_BSe5ZVW">An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling</title>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Shaojie Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladlen</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Koltun</surname></persName>
		</author>
		<idno>Arxiv: 1803.01271</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Arxiv preprint</note>
	<note type="raw_reference">Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2018. An Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence Modeling. Arxiv preprint Arxiv: 1803.01271 (2018).</note>
</biblStruct>

<biblStruct coords="20,97.77,526.15,433.53,7.75;20,97.77,536.11,417.38,7.75" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="20,330.76,526.15,179.09,7.75" xml:id="_pHTtknd">Multimodal Machine Learning: A Survey and Taxonomy</title>
		<author>
			<persName coords=""><forename type="first">Tadas</forename><surname>Baltrušaitis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2798607</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2798607" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_wkST42y" coord="20,516.67,526.36,14.63,7.54;20,97.77,536.33,184.20,7.54">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="443" />
			<date type="published" when="2019-02">2019. Feb 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Tadas Baltrušaitis, Chaitanya Ahuja, and Louis-Philippe Morency. 2019. Multimodal Machine Learning: A Survey and Taxonomy. IEEE Transactions on Pattern Analysis and Machine Intelligence 41, 2 (Feb 2019), 423ś443. https://doi.org/10.1109/TPAMI.2018.2798607</note>
</biblStruct>

<biblStruct coords="20,97.77,546.07,433.53,7.75;20,97.77,556.04,119.96,7.75" xml:id="b3">
	<analytic>
		<title level="a" type="main" coord="20,237.74,546.07,219.22,7.75" xml:id="_6RN7GqK">Learning long-term dependencies with gradient descent is diicult</title>
		<author>
			<persName coords=""><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Simard</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6h5WBYr" coord="20,463.48,546.29,67.83,7.54;20,97.77,556.25,52.95,7.54">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Y Bengio, P Simard, and P Frasconi. 1994. Learning long-term dependencies with gradient descent is diicult. IEEE Transactions on Neural Networks 5, 2 (1994), 157ś166.</note>
</biblStruct>

<biblStruct coords="20,97.77,566.00,433.53,7.75;20,97.77,575.96,434.41,7.75;20,97.59,585.92,57.38,7.75" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="20,196.82,575.96,207.07,7.75" xml:id="_Tg22Raa">IEMOCAP: interactive emotional dyadic motion capture database</title>
		<author>
			<persName coords=""><forename type="first">Carlos</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Murtaza</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chi</forename><forename type="middle">Chun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Abe</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Emily</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Samuel</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeannette</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sungbok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shrikanth</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_BeYmgZw" coord="20,410.01,576.18,111.16,7.54">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="359" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Carlos Busso, Murtaza Bulut, Chi Chun Lee, Abe Kazemzadeh, Emily Mower, Samuel Kim, Jeannette N. Chang, Sungbok Lee, and Shrikanth S. Narayanan. 2008. IEMOCAP: interactive emotional dyadic motion capture database. Language Resources and Evaluation 42, 4 (2008), 335ś359.</note>
</biblStruct>

<biblStruct coords="20,97.77,595.82,433.53,7.82;20,97.77,605.85,434.76,7.75;20,97.59,615.81,28.36,7.75" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="20,458.05,595.89,73.25,7.75;20,97.77,605.85,191.13,7.75" xml:id="_vjpwaRn">Multimodal sentiment analysis with word-level fusion and reinforcement learning</title>
		<author>
			<persName coords=""><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tadas</forename><surname>Baltrus Ǎitis</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><forename type="middle">Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ftJ6qF5" coord="20,302.74,606.06,223.47,7.54">19th ACM International Conference on Multimodal Interaction (ICMI&apos;17</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="163" to="171" />
		</imprint>
	</monogr>
	<note type="raw_reference">Minghai Chen, Sen Wang, Paul Pu Liang, Tadas Baltrus ǎitis, Amir Zadeh, and Louis Philippe Morency. 2017. Multimodal sentiment analysis with word-level fusion and reinforcement learning. In 19th ACM International Conference on Multimodal Interaction (ICMI&apos;17). 163ś171.</note>
</biblStruct>

<biblStruct coords="20,97.77,625.77,434.76,7.75;20,97.77,635.74,433.54,7.75;20,97.77,645.70,247.38,7.75" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="20,118.15,635.74,324.06,7.75" xml:id="_MPwGzmH">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</title>
		<author>
			<persName coords=""><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bart</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_t4Xpf5U" coord="20,456.68,635.95,74.63,7.54;20,97.77,645.91,206.51,7.54">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014. 1724ś1734</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. 1724ś1734.</note>
</biblStruct>

<biblStruct coords="21,97.77,108.71,433.53,7.75;21,97.77,118.68,408.25,7.75" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="21,389.18,108.71,142.11,7.75;21,97.77,118.68,115.04,7.75" xml:id="_u9K9qz3">COVAREP: A Collaborative Voice Analysis Repository for Speech Technologies</title>
		<author>
			<persName coords=""><forename type="first">Gilles</forename><surname>Degottex</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">John</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Drugman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tuomo</forename><surname>Raitio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefan</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_wTyShzg" coord="21,226.91,118.89,245.65,7.54">2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">964</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Gilles Degottex, John Kane, Thomas Drugman, Tuomo Raitio, and Stefan Scherer. 2014. COVAREP: A Collaborative Voice Analysis Repository for Speech Technologies. In 2019 IEEE International Conference on Acoustics, Speech and Signal Processing. 960ś964.</note>
</biblStruct>

<biblStruct coords="21,97.77,128.64,433.53,7.75;21,97.77,138.60,433.54,7.75;21,97.77,148.56,434.41,7.75;21,97.77,158.53,199.33,7.75" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="21,348.26,128.64,183.04,7.75;21,97.77,138.60,91.42,7.75" xml:id="_vwbMqVz">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://doi.org/10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_DrBk5sX" coord="21,203.43,138.82,327.87,7.54;21,97.77,148.78,134.96,7.54">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s" xml:id="_n2DehAr" coord="21,272.58,148.78,67.74,7.54">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Association for Computational Linguistics, Minneapolis, Minnesota, 4171ś4186. https://doi.org/10.18653/v1/N19-1423</note>
</biblStruct>

<biblStruct coords="21,97.77,168.49,433.53,7.75;21,97.77,178.45,249.91,7.75" xml:id="b9">
	<analytic>
		<title level="a" type="main" coord="21,338.72,168.49,180.34,7.75" xml:id="_PsPtHDf">Learning Discrete Structures for Graph Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiao</forename><surname>He</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1903.11960" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_BMVanjw" coord="21,97.77,178.67,141.54,7.54">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Luca Franceschi, Mathias Niepert, Massimiliano Pontil, and Xiao He. 2019. Learning Discrete Structures for Graph Neural Networks. In International conference on machine learning. http://arxiv.org/abs/1903.11960</note>
</biblStruct>

<biblStruct coords="21,97.77,188.41,433.53,7.75;21,97.77,198.38,434.76,7.75;21,97.77,208.34,101.61,7.75" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="21,397.95,188.41,133.35,7.75;21,97.77,198.38,146.01,7.75" xml:id="_wRDHDaH">Dynamic Graph Learning Convolutional Networks for Semi-Supervised Classiication</title>
		<author>
			<persName coords=""><forename type="first">Sichao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weifeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weili</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yicong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dapeng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Changsheng</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3412846</idno>
		<ptr target="https://doi.org/10.1145/3412846" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_WkNgCP6" coord="21,249.94,198.59,159.52,7.54">ACM Trans. Multimedia Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2021-03">2021. mar 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sichao Fu, Weifeng Liu, Weili Guan, Yicong Zhou, Dapeng Tao, and Changsheng Xu. 2021. Dynamic Graph Learning Convolutional Networks for Semi-Supervised Classiication. ACM Trans. Multimedia Comput. Commun. Appl. 17, 1s, Article 4 (mar 2021), 13 pages. https://doi.org/10.1145/3412846</note>
</biblStruct>

<biblStruct coords="21,97.77,218.30,433.53,7.75;21,97.53,228.26,337.84,7.75" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="21,277.36,218.30,253.94,7.75;21,97.53,228.26,12.75,7.75" xml:id="_3zFECb8">Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene Text</title>
		<author>
			<persName coords=""><forename type="first">Difei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_fgunHJ5" coord="21,132.49,228.48,234.22,7.54">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020. 2020</date>
			<biblScope unit="page" from="12743" to="12753" />
		</imprint>
	</monogr>
	<note type="raw_reference">Difei Gao, Ke Li, R. Wang, S. Shan, and X. Chen. 2020. Multi-Modal Graph Neural Network for Joint Reasoning on Vision and Scene Text. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (2020), 12743ś12753.</note>
</biblStruct>

<biblStruct coords="21,97.77,238.23,433.53,7.75;21,97.77,248.19,291.91,7.75" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="21,343.78,238.23,187.52,7.75;21,97.77,248.19,162.63,7.75" xml:id="_PsUc8KZ">What makes the diference? An empirical comparison of fusion strategies for multimodal language analysis</title>
		<author>
			<persName coords=""><forename type="first">Dimitris</forename><surname>Gkoumas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiuchi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yijun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Da</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Song</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9gJDT4p" coord="21,266.37,248.40,60.21,7.54">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="184" to="197" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Dimitris Gkoumas, Qiuchi Li, C. Lioma, Yijun Yu, and Da wei Song. 2021. What makes the diference? An empirical comparison of fusion strategies for multimodal language analysis. Information Fusion 66 (2021), 184ś197.</note>
</biblStruct>

<biblStruct coords="21,97.77,258.15,434.76,7.75;21,97.77,268.12,188.40,7.75" xml:id="b13">
	<analytic>
		<title level="a" type="main" coord="21,304.23,258.15,224.77,7.75" xml:id="_Xqxb3WT">First-order versus second-order single-layer recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">M W</forename><surname>Goudreau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">C L</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S T</forename><surname>Chakradhar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_HnGeG5r" coord="21,97.77,268.33,121.39,7.54">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="511" to="513" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
	<note type="raw_reference">M W Goudreau, C L Giles, S T Chakradhar, and . Chen, D. 1994. First-order versus second-order single-layer recurrent neural networks. IEEE Transactions on Neural Networks 5, 3 (1994), 511ś513.</note>
</biblStruct>

<biblStruct coords="21,97.77,278.08,433.53,7.75;21,97.77,288.04,434.76,7.75;21,97.77,298.00,28.36,7.75" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="21,375.13,278.08,156.17,7.75;21,97.77,288.04,197.38,7.75" xml:id="_PxhNExC">Connectionist temporal classiication: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName coords=""><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Santiago</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CTfkjQq" coord="21,309.57,288.26,219.78,7.54">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
	<note type="raw_reference">Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. 2006. Connectionist temporal classiication: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning. 369ś376.</note>
</biblStruct>

<biblStruct coords="21,97.77,307.97,433.53,7.75;21,97.77,317.93,98.19,7.75" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="21,266.16,307.97,154.83,7.75" xml:id="_vjBfGy9">Inductive representation learning on large graphs</title>
		<author>
			<persName coords=""><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ARd8aPz" coord="21,434.08,308.18,97.22,7.54;21,97.77,318.14,57.17,7.54">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017. 1024ś1034</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation learning on large graphs. In Advances in neural information processing systems. 1024ś1034.</note>
</biblStruct>

<biblStruct coords="21,97.77,327.89,433.53,7.75;21,97.77,337.85,310.18,7.75" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="21,305.90,327.89,225.40,7.75;21,97.77,337.85,61.64,7.75" xml:id="_6QhsSEk">MISA: Modality-Invariant and -Speciic Representations for Multimodal Sentiment Analysis</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Devamanyu Hazarika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soujanya</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_mJJf9CG" coord="21,165.59,338.07,219.00,7.54">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Devamanyu Hazarika, R. Zimmermann, and Soujanya Poria. 2020. MISA: Modality-Invariant and -Speciic Representations for Multimodal Sentiment Analysis. Proceedings of the 28th ACM International Conference on Multimedia (2020).</note>
</biblStruct>

<biblStruct coords="21,97.77,347.82,433.53,7.75;21,97.77,357.78,162.61,7.75" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="21,306.72,347.82,149.67,7.75" xml:id="_Q4MZEHJ">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName coords=""><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_p6VKAzG" coord="21,470.41,348.03,60.89,7.54;21,97.77,357.99,129.04,7.54">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">770</biblScope>
			<biblScope unit="page">778</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In IEEE Conference on Computer Vision and Pattern Recognition. 770ś778.</note>
</biblStruct>

<biblStruct coords="21,97.77,367.74,386.48,7.76" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="21,255.63,367.74,82.44,7.75" xml:id="_2YAV8wn">Long Short-Term Memory</title>
	</analytic>
	<monogr>
		<title level="m" xml:id="_v9Ps7EZ" coord="21,97.77,367.74,132.72,7.75">Sepp Hochreiter and J1rgen Schmidhuber</title>
		<imprint>
			<date type="published" when="1997">1997. 1997. 1735ś1780</date>
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Sepp Hochreiter and J1rgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9, 8 (1997), 1735ś1780.</note>
</biblStruct>

<biblStruct coords="21,97.77,377.70,433.68,7.75;21,97.77,387.67,286.46,7.75" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="21,354.17,377.70,177.28,7.75;21,97.77,387.67,62.45,7.75" xml:id="_2tCB5pA">Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling</title>
		<author>
			<persName coords=""><forename type="first">Ming</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiajia</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianhai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wanzeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qibin</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Dr3ADhS" coord="21,174.24,387.88,161.46,7.54">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019. 12113ś12122</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Ming Hou, Jiajia Tang, Jianhai Zhang, Wanzeng Kong, and Qibin Zhao. 2019. Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling. In Advances in Neural Information Processing Systems. 12113ś12122.</note>
</biblStruct>

<biblStruct coords="21,97.77,397.63,433.53,7.75;21,97.49,407.59,415.18,7.75" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="21,302.12,397.63,229.18,7.75;21,97.49,407.59,26.52,7.75" xml:id="_Dyung5z">Attention-Based Modality-Gated Networks for Image-Text Sentiment Analysis</title>
		<author>
			<persName coords=""><forename type="first">Feiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kaimin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3388861</idno>
		<ptr target="https://doi.org/10.1145/3388861" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_K8Ygs9g" coord="21,130.20,407.81,157.21,7.54">ACM Trans. Multimedia Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<date type="published" when="2020-07">2020. jul 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Feiran Huang, Kaimin Wei, Jian Weng, and Zhoujun Li. 2020. Attention-Based Modality-Gated Networks for Image-Text Sentiment Analysis. ACM Trans. Multimedia Comput. Commun. Appl. 16, 3, Article 79 (jul 2020), 19 pages. https://doi.org/10.1145/3388861</note>
</biblStruct>

<biblStruct coords="21,97.77,417.55,433.53,7.75;21,97.77,427.52,401.58,7.75" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="21,291.24,417.55,240.07,7.75;21,97.77,427.52,69.13,7.75" xml:id="_35M6pUr">Cross-Modality Microblog Sentiment Prediction via Bi-Layer Multimodal Hypergraph Learning</title>
		<author>
			<persName coords=""><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fuhai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2018.2867718</idno>
		<ptr target="https://doi.org/10.1109/TMM.2018.2867718" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_9caaZtc" coord="21,173.21,427.73,105.23,7.54">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1062" to="1075" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Rongrong Ji, Fuhai Chen, Liujuan Cao, and Yue Gao. 2019. Cross-Modality Microblog Sentiment Prediction via Bi-Layer Multimodal Hypergraph Learning. IEEE Transactions on Multimedia 21, 4 (2019), 1062ś1075. https://doi.org/10.1109/TMM.2018.2867718</note>
</biblStruct>

<biblStruct coords="21,97.77,437.48,433.68,7.75;21,97.77,447.44,433.53,7.75;21,97.28,457.62,78.69,7.54" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="21,341.57,437.48,189.88,7.75;21,97.77,447.44,144.37,7.75" xml:id="_eDxgvyt">Investigating Audio, Visual, and Text Fusion Methods for End-to-End Automatic Personality Prediction</title>
		<author>
			<persName coords=""><forename type="first">Onno</forename><surname>Kampman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Elham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dario</forename><surname>Barezi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pascale</forename><surname>Bertero</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Fung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_SWTrUrM" coord="21,255.86,447.66,275.44,7.54">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Onno Kampman, Elham J. Barezi, Dario Bertero, and Pascale Fung. 2018. Investigating Audio, Visual, and Text Fusion Methods for End-to-End Automatic Personality Prediction. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers).</note>
</biblStruct>

<biblStruct coords="21,97.77,467.37,433.53,7.75;21,97.77,477.55,103.79,7.54" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="21,232.58,467.37,147.90,7.75" xml:id="_paFH2hY">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName coords=""><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_f23qxRK" coord="21,394.75,467.58,136.55,7.54;21,97.77,477.55,100.59,7.54">Proceedings of International Conference on Learning Representations (ICLR)</title>
		<meeting>International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Diederik P Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In Proceedings of International Conference on Learning Representations (ICLR).</note>
</biblStruct>

<biblStruct coords="21,97.77,487.29,433.53,7.75;21,97.77,497.47,127.54,7.54" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="21,223.87,487.29,207.87,7.75" xml:id="_qT3dWR9">Semi-supervised classiication with graph convolutional networks</title>
		<author>
			<persName coords=""><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_7fc2v8N" coord="21,445.36,487.51,85.95,7.54;21,97.77,497.47,124.36,7.54">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Thomas N Kipf and Max Welling. 2016. Semi-supervised classiication with graph convolutional networks. In Proceedings of International Conference on Learning Representations.</note>
</biblStruct>

<biblStruct coords="21,97.77,507.22,433.54,7.75;21,97.77,517.18,146.71,7.75" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="21,344.59,507.22,186.72,7.75;21,97.77,517.18,24.84,7.75" xml:id="_Qh95NrK">Quantum-inspired multimodal fusion for video sentiment analysis</title>
		<author>
			<persName coords=""><forename type="first">Qiuchi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dimitris</forename><surname>Gkoumas</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christina</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Massimo</forename><surname>Melucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_pHph4hw" coord="21,128.58,517.40,60.21,7.54">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="58" to="71" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Qiuchi Li, Dimitris Gkoumas, Christina Lioma, and Massimo Melucci. 2021. Quantum-inspired multimodal fusion for video sentiment analysis. Information Fusion 65 (2021), 58ś71.</note>
</biblStruct>

<biblStruct coords="21,97.77,527.14,433.53,7.75;21,97.77,537.11,433.53,7.76;21,97.77,547.07,171.79,7.75" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="21,501.55,527.14,29.75,7.75;21,97.77,537.11,269.95,7.75" xml:id="_BASfqrH">Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization</title>
		<author>
			<persName coords=""><forename type="first">Paul Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qibin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_PhXr4Ns" coord="21,382.29,537.32,149.00,7.54;21,97.77,547.28,131.05,7.54">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019. 1569ś1576</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Paul Pu Liang, Zhun Liu, Yao-Hung Hubert Tsai, Qibin Zhao, Ruslan Salakhutdinov, and Louis-Philippe Morency. 2019. Learning Representations from Imperfect Time Series Data via Tensor Rank Regularization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 1569ś1576.</note>
</biblStruct>

<biblStruct coords="21,97.77,557.03,433.53,7.75;21,97.77,566.99,348.89,7.75" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="21,340.26,557.03,191.04,7.75;21,97.77,566.99,20.11,7.75" xml:id="_vFActVq">Multimodal Language Analysis with Recurrent Multistage Fusion</title>
		<author>
			<persName coords=""><forename type="first">Paul Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ziyin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><forename type="middle">Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_ahk7CxW" coord="21,131.90,567.21,281.31,7.54">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">161</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Paul Pu Liang, Ziyin Liu, Amir Zadeh, and Louis Philippe Morency. 2018. Multimodal Language Analysis with Recurrent Multistage Fusion. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. 150ś161.</note>
</biblStruct>

<biblStruct coords="21,97.77,576.96,433.53,7.75;21,97.77,586.92,413.74,7.75" xml:id="b28">
	<analytic>
		<title level="a" type="main" coord="21,385.91,576.96,145.39,7.75;21,97.77,586.92,81.25,7.75" xml:id="_xnnRzjJ">Eicient Low-rank Multimodal Fusion with Modality-Speciic Factors</title>
		<author>
			<persName coords=""><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><forename type="middle">Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_zP48ZXr" coord="21,192.85,587.13,277.91,7.54">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2247ś2256</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhun Liu, Ying Shen, Paul Pu Liang, Amir Zadeh, and Louis Philippe Morency. 2018. Eicient Low-rank Multimodal Fusion with Modality-Speciic Factors. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. 2247ś2256.</note>
</biblStruct>

<biblStruct coords="21,97.77,596.88,433.53,7.75;21,97.77,606.84,433.53,7.75;21,97.77,616.81,66.10,7.75" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="21,257.36,596.88,273.94,7.75;21,97.77,606.84,179.93,7.75" xml:id="_fp9x5Cf">Divide, Conquer and Combine: Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Afective Computing</title>
		<author>
			<persName coords=""><forename type="first">Sijie</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haifeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Songlong</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_v6S3gbA" coord="21,292.07,607.06,239.24,7.54;21,97.77,617.02,32.77,7.54">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="481" to="492" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sijie Mai, Haifeng Hu, and Songlong Xing. 2019. Divide, Conquer and Combine: Hierarchical Feature Fusion Network with Local and Global Perspectives for Multimodal Afective Computing. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 481ś492.</note>
</biblStruct>

<biblStruct coords="21,97.77,626.77,433.53,7.75;21,97.77,636.73,388.65,7.75" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="21,254.99,626.77,276.31,7.75;21,97.77,636.73,125.68,7.75" xml:id="_JyfVPCb">Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion</title>
		<author>
			<persName coords=""><forename type="first">Sijie</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haifeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Songlong</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Yjy6BZn" coord="21,237.48,636.95,190.11,7.54">Proceedings of the AAAI Conference on Artiicial Intelligence</title>
		<meeting>the AAAI Conference on Artiicial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">172</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Sijie Mai, Haifeng Hu, and Songlong Xing. 2020. Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion. In Proceedings of the AAAI Conference on Artiicial Intelligence, Vol. 34. 164ś172.</note>
</biblStruct>

<biblStruct coords="22,97.77,108.71,434.76,7.75;22,97.77,118.68,172.67,7.75" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="22,226.97,108.71,301.66,7.75" xml:id="_6SyfkzY">Multi-Fusion Residual Memory Network for Multimodal Human Sentiment Comprehension</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_CHb2VTY" coord="22,97.77,118.89,133.51,7.54">IEEE Transactions on Afective Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Mai, H. Hu, J. Xu, and S. Xing. 2020. Multi-Fusion Residual Memory Network for Multimodal Human Sentiment Comprehension. IEEE Transactions on Afective Computing (2020), 1ś1.</note>
</biblStruct>

<biblStruct coords="22,97.77,128.64,433.53,7.75;22,97.77,138.60,217.24,7.75" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="22,202.43,128.64,328.87,7.75;22,97.77,138.60,34.71,7.75" xml:id="_6jBkvDv">Locally Conined Modality Fusion Network With a Global Perspective for Multimodal Human Afective Computing</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_6Vj8wm8" coord="22,139.21,138.82,105.23,7.54">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="122" to="137" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Mai, S. Xing, and H. Hu. 2020. Locally Conined Modality Fusion Network With a Global Perspective for Multimodal Human Afective Computing. IEEE Transactions on Multimedia 22, 1 (2020), 122ś137.</note>
</biblStruct>

<biblStruct coords="22,97.77,148.56,433.53,7.75;22,97.53,158.53,346.60,7.75" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="22,256.13,148.56,275.16,7.75;22,97.53,158.53,102.33,7.75" xml:id="_5m8jVpv">Analyzing Multimodal Language via Acoustic-and Visual-LSTM with Channel-aware Temporal Convolution Network</title>
		<author>
			<persName coords=""><forename type="first">Sijie</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Songlong</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haifeng</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_Ham2euK" coord="22,206.53,158.74,213.96,7.54">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sijie Mai, Songlong Xing, and Haifeng Hu. 2021. Analyzing Multimodal Language via Acoustic-and Visual-LSTM with Channel-aware Temporal Convolution Network. IEEE/ACM Transactions on Audio, Speech, and Language Processing (2021).</note>
</biblStruct>

<biblStruct coords="22,97.77,168.49,433.53,7.75;22,97.77,178.45,222.31,7.75" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="22,300.89,168.49,230.41,7.75;22,97.77,178.45,59.20,7.75" xml:id="_uDPJ6CN">Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis</title>
		<author>
			<persName coords=""><forename type="first">Sijie</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuangjia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haifeng</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_C3w8Jxe" coord="22,162.94,178.67,133.51,7.54">IEEE Transactions on Afective Computing</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sijie Mai, Ying Zeng, Shuangjia Zheng, and Haifeng Hu. 2022. Hybrid contrastive learning of tri-modal representation for multimodal sentiment analysis. IEEE Transactions on Afective Computing (2022).</note>
</biblStruct>

<biblStruct coords="22,97.77,188.41,434.76,7.75;22,97.77,198.38,228.34,7.75" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="22,316.39,188.41,212.73,7.75" xml:id="_JJpQETB">Communicative message passing for inductive relation reasoning</title>
		<author>
			<persName coords=""><forename type="first">Sijie</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuangjia</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuedong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Haifeng</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_kHgdHpt" coord="22,97.77,198.59,179.94,7.54">Association for the Advancement of Artiicial Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Sijie Mai, Shuangjia Zheng, Yuedong Yang, and Haifeng Hu. 2021. Communicative message passing for inductive relation reasoning. Association for the Advancement of Artiicial Intelligence (AAAI) (2021).</note>
</biblStruct>

<biblStruct coords="22,97.77,208.34,434.41,7.75;22,97.77,218.30,34.17,7.75" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="22,154.14,208.34,211.09,7.75" xml:id="_tZBBPdF">Neural Network for Graphs: A Contextual Constructive Approach</title>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_9HqzU9N" coord="22,371.81,208.55,120.34,7.54">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="498" to="511" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Micheli. 2009. Neural Network for Graphs: A Contextual Constructive Approach. IEEE Transactions on Neural Networks 20, 3 (2009), p.498ś511.</note>
</biblStruct>

<biblStruct coords="22,97.77,228.26,434.76,7.75;22,97.77,238.23,92.78,7.75" xml:id="b37">
	<monogr>
		<title level="m" type="main" coord="22,358.07,228.26,170.73,7.75" xml:id="_hEVYjxU">Multi-Modal Retrieval using Graph Neural Networks</title>
		<author>
			<persName coords=""><forename type="first">Aashish</forename><surname>Kumar Misraa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ajinkya</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pranav</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Aminian</surname></persName>
		</author>
		<idno>ArXiv abs/2010.01666</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Aashish Kumar Misraa, Ajinkya Kale, Pranav Aggarwal, and A. Aminian. 2020. Multi-Modal Retrieval using Graph Neural Networks. ArXiv abs/2010.01666 (2020).</note>
</biblStruct>

<biblStruct coords="22,97.77,248.19,434.88,7.75;22,97.77,258.15,321.90,7.75" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="22,405.81,248.19,126.84,7.75;22,97.77,258.15,47.93,7.75" xml:id="_gpuvYrW">Deep multimodal fusion for persuasiveness prediction</title>
		<author>
			<persName coords=""><forename type="first">Behnaz</forename><surname>Nojavanasghari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deepak</forename><surname>Gopinath</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jayanth</forename><surname>Koushik</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><forename type="middle">Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_YRdwCAj" coord="22,159.56,258.37,226.77,7.54">Proceedings of ACM International Conference on Multimodal Interaction</title>
		<meeting>ACM International Conference on Multimodal Interaction</meeting>
		<imprint>
			<date type="published" when="2016">2016. 284ś288</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Behnaz Nojavanasghari, Deepak Gopinath, Jayanth Koushik, and Louis Philippe Morency. 2016. Deep multimodal fusion for persuasive- ness prediction. In Proceedings of ACM International Conference on Multimodal Interaction. 284ś288.</note>
</biblStruct>

<biblStruct coords="22,97.77,268.12,434.41,7.75;22,97.77,278.08,28.36,7.75" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="22,163.33,268.12,228.26,7.75" xml:id="_fcauP2w">From Utterance to Text: The Bias of Language in Speech and Writing</title>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Olson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_5YHqF42" coord="22,398.39,268.33,92.17,7.54">Harvard Educational Review</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">281</biblScope>
			<date type="published" when="1977">1977. 1977</date>
		</imprint>
	</monogr>
	<note type="raw_reference">David Olson. 1977. From Utterance to Text: The Bias of Language in Speech and Writing. Harvard Educational Review 47, 3 (1977), 257ś281.</note>
</biblStruct>

<biblStruct coords="22,97.77,288.04,433.54,7.75;22,97.77,298.00,359.38,7.75" xml:id="b40">
	<analytic>
		<title level="a" type="main" coord="22,242.48,288.04,288.82,7.75;22,97.77,298.00,39.01,7.75" xml:id="_7mX6bcM">TCNN: Temporal convolutional neural network for real-time speech enhancement in the time domain</title>
		<author>
			<persName coords=""><forename type="first">Ashutosh</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_MnRZQg3" coord="22,151.19,298.22,245.66,7.54">2019 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6875" to="6879" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ashutosh Pandey and DeLiang Wang. 2019. TCNN: Temporal convolutional neural network for real-time speech enhancement in the time domain. In 2019 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 6875ś6879.</note>
</biblStruct>

<biblStruct coords="22,97.77,307.97,433.53,7.75;22,97.77,317.93,283.36,7.75" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="22,330.49,307.97,150.27,7.75" xml:id="_qPzCJz6">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName coords=""><forename type="first">Jefrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_FC5GVE5" coord="22,494.74,308.18,36.56,7.54;22,97.77,318.14,242.49,7.54">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014">2014. 1532ś1543</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jefrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing. 1532ś1543.</note>
</biblStruct>

<biblStruct coords="22,97.77,327.89,433.53,7.75;22,97.64,337.85,431.47,7.75" xml:id="b42">
	<analytic>
		<title level="a" type="main" coord="22,407.82,327.89,123.48,7.75;22,97.64,337.85,206.60,7.75" xml:id="_DxuGBYC">Found in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities</title>
		<author>
			<persName coords=""><forename type="first">Hai</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><surname>Manzini</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Poczǒs</forename><surname>Barnabǎs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Vmr3MUY" coord="22,318.05,338.07,170.91,7.54">Thirty-Third AAAI Conference on Artiicial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">6892</biblScope>
			<biblScope unit="page">6899</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Hai Pham, Paul Pu Liang, Thomas Manzini, Louis Philippe Morency, and Poczǒs Barnabǎs. 2019. Found in Translation: Learning Robust Joint Representations by Cyclic Translations Between Modalities. In Thirty-Third AAAI Conference on Artiicial Intelligence. 6892ś6899.</note>
</biblStruct>

<biblStruct coords="22,97.77,347.82,434.88,7.75;22,97.77,357.78,433.53,7.75;22,97.77,367.74,66.10,7.75" xml:id="b43">
	<analytic>
		<title level="a" type="main" coord="22,503.45,347.82,29.20,7.75;22,97.77,357.78,181.39,7.75" xml:id="_ZFtjUYS">Context-Dependent Sentiment Analysis in User-Generated Videos</title>
		<author>
			<persName coords=""><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><forename type="middle">Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_baMdBXS" coord="22,292.97,357.99,238.33,7.54;22,97.77,367.96,32.77,7.54">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
	<note type="raw_reference">Soujanya Poria, Erik Cambria, Devamanyu Hazarika, Navonil Majumder, Amir Zadeh, and Louis Philippe Morency. 2017. Context- Dependent Sentiment Analysis in User-Generated Videos. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. 873ś883.</note>
</biblStruct>

<biblStruct coords="22,97.77,377.70,433.53,7.75;22,97.77,387.67,327.12,7.75" xml:id="b44">
	<analytic>
		<title level="a" type="main" coord="22,325.13,377.70,206.17,7.75;22,97.77,387.67,61.64,7.75" xml:id="_d4UuCvV">Convolutional MKL Based Multimodal Emotion Recognition and Sentiment Analysis</title>
		<author>
			<persName coords=""><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Iti</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_YCNJNnp" coord="22,173.40,387.88,217.49,7.54">Proceedings of IEEE International Conference on Data Mining (ICDM)</title>
		<meeting>IEEE International Conference on Data Mining (ICDM)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="439" to="448" />
		</imprint>
	</monogr>
	<note type="raw_reference">Soujanya Poria, Iti Chaturvedi, Erik Cambria, and Amir Hussain. 2016. Convolutional MKL Based Multimodal Emotion Recognition and Sentiment Analysis. In Proceedings of IEEE International Conference on Data Mining (ICDM). 439ś448.</note>
</biblStruct>

<biblStruct coords="22,97.77,397.63,433.53,7.75;22,97.77,407.59,116.26,7.75" xml:id="b45">
	<analytic>
		<title level="a" type="main" coord="22,346.73,397.63,111.05,7.75" xml:id="_f7RFata">The Graph Neural Network Model</title>
		<author>
			<persName coords=""><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_duHkMRP" coord="22,464.37,397.84,66.93,7.54;22,97.77,407.81,52.95,7.54">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note type="raw_reference">F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini. 2009. The Graph Neural Network Model. IEEE Transactions on Neural Networks 20, 1 (2009), 61ś80.</note>
</biblStruct>

<biblStruct coords="22,97.77,417.55,434.76,7.75;22,97.77,427.52,256.52,7.75" xml:id="b46">
	<monogr>
		<title level="m" type="main" coord="22,308.76,417.55,220.47,7.75" xml:id="_eU7u7A9">Hostśparasite: graph LSTM-in-LSTM for group activity recognition</title>
		<author>
			<persName coords=""><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Liyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yunlian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="663" to="674" />
		</imprint>
	</monogr>
	<note type="raw_reference">Xiangbo Shu, Liyan Zhang, Yunlian Sun, and Jinhui Tang. 2020. Hostśparasite: graph LSTM-in-LSTM for group activity recognition. IEEE transactions on neural networks and learning systems 32, 2 (2020), 663ś674.</note>
</biblStruct>

<biblStruct coords="22,97.77,437.48,433.69,7.75;22,97.77,447.44,370.18,7.75" xml:id="b47">
	<analytic>
		<title level="a" type="main" coord="22,336.92,437.48,194.54,7.75;22,97.77,447.44,87.94,7.75" xml:id="_XDGqbGE">Social anchor-unit graph regularized tensor completion for large-scale image retagging</title>
		<author>
			<persName coords=""><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zechao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_TZ8maDJ" coord="22,191.86,447.66,197.95,7.54">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<date type="published" when="2019">2019. 2019. 2027ś2034</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jinhui Tang, Xiangbo Shu, Zechao Li, Yu-Gang Jiang, and Qi Tian. 2019. Social anchor-unit graph regularized tensor completion for large-scale image retagging. IEEE transactions on pattern analysis and machine intelligence 41, 8 (2019), 2027ś2034.</note>
</biblStruct>

<biblStruct coords="22,97.77,457.41,433.53,7.75;22,97.53,467.37,433.77,7.75;22,97.77,477.33,73.51,7.75" xml:id="b48">
	<analytic>
		<title level="a" type="main" coord="22,493.94,457.41,37.36,7.75;22,97.53,467.37,188.36,7.75" xml:id="_FBHUv7B">Multimodal Transformer for Unaligned Multimodal Language Sequences</title>
		<author>
			<persName coords=""><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shaojie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Pu Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">Zico</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_rf6ECmT" coord="22,298.69,467.58,232.61,7.54;22,97.77,477.55,32.77,7.54">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6558" to="6569" />
		</imprint>
	</monogr>
	<note type="raw_reference">Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang, J. Zico Kolter, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2019. Multimodal Transformer for Unaligned Multimodal Language Sequences. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 6558ś6569.</note>
</biblStruct>

<biblStruct coords="22,97.77,487.29,434.47,7.75;22,97.77,497.26,433.53,7.75;22,97.77,507.22,139.84,7.75" xml:id="b49">
	<analytic>
		<title level="a" type="main" coord="22,462.82,487.29,69.42,7.75;22,97.77,497.26,245.46,7.75" xml:id="_XS49QJQ">Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis</title>
		<author>
			<persName coords=""><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Muqiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_tBNrqkX" coord="22,356.41,497.47,174.89,7.54;22,97.77,507.43,98.97,7.54">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020. 1823ś1833</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Yao-Hung Hubert Tsai, Martin Ma, Muqiao Yang, Ruslan Salakhutdinov, and Louis-Philippe Morency. 2020. Multimodal Routing: Improving Local and Global Interpretability of Multimodal Language Analysis. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. 1823ś1833.</note>
</biblStruct>

<biblStruct coords="22,97.77,517.18,434.76,7.75;22,97.49,527.14,293.36,7.75" xml:id="b50">
	<analytic>
		<title level="a" type="main" coord="22,97.49,527.14,78.73,7.75" xml:id="_uG4RgGw">Attention is all you need</title>
		<author>
			<persName coords=""><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NTscme4" coord="22,190.36,527.14,196.91,7.75">Advances in neural information processing systems. 5998ś</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">6008</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems. 5998ś6008.</note>
</biblStruct>

<biblStruct coords="22,97.77,537.11,434.76,7.75;22,97.77,547.07,226.26,7.75" xml:id="b51">
	<analytic>
		<title level="a" type="main" coord="22,448.04,537.11,80.98,7.75" xml:id="_rGG6bKu">Graph attention networks</title>
		<author>
			<persName coords=""><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_RKVqtQy" coord="22,106.45,547.28,214.41,7.54">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. 2018. Graph attention networks. In Proceedings of International Conference on Learning Representations.</note>
</biblStruct>

<biblStruct coords="22,97.77,557.03,433.53,7.75;22,97.77,566.99,257.71,7.75" xml:id="b52">
	<analytic>
		<title level="a" type="main" coord="22,233.15,557.03,225.55,7.75" xml:id="_ZAMDpqv">Sparse Multigraph Embedding for Multimodal Feature Representation</title>
		<author>
			<persName coords=""><forename type="first">Shiping</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wenzhong</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2017.2663324</idno>
		<ptr target="https://doi.org/10.1109/TMM.2017.2663324" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_D2BhZ4v" coord="22,464.94,557.25,66.36,7.54;22,97.77,567.21,36.79,7.54">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1454" to="1466" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Shiping Wang and Wenzhong Guo. 2017. Sparse Multigraph Embedding for Multimodal Feature Representation. IEEE Transactions on Multimedia 19, 7 (2017), 1454ś1466. https://doi.org/10.1109/TMM.2017.2663324</note>
</biblStruct>

<biblStruct coords="22,97.77,576.96,433.74,7.75;22,97.49,586.92,435.04,7.75;22,97.59,596.88,35.77,7.75" xml:id="b53">
	<analytic>
		<title level="a" type="main" coord="22,430.61,576.96,100.90,7.75;22,97.49,586.92,198.71,7.75" xml:id="_c8QRfyK">Words Can Shift: Dynamically Adjusting Word Representations Using Nonverbal Behaviors</title>
		<author>
			<persName coords=""><forename type="first">Yansen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_PSZAEkp" coord="22,310.47,587.13,193.14,7.54">Proceedings of the AAAI Conference on Artiicial Intelligence</title>
		<meeting>the AAAI Conference on Artiicial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Yansen Wang, Ying Shen, Zhun Liu, Paul Pu Liang, Amir Zadeh, and Louis-Philippe Morency. 2019. Words Can Shift: Dynamically Adjusting Word Representations Using Nonverbal Behaviors. In Proceedings of the AAAI Conference on Artiicial Intelligence, Vol. 33. 7216ś7223.</note>
</biblStruct>

<biblStruct coords="22,97.77,606.84,433.53,7.75;22,97.77,616.81,350.95,7.75" xml:id="b54">
	<analytic>
		<title level="a" type="main" coord="22,502.99,606.84,28.31,7.75;22,97.77,616.81,205.37,7.75" xml:id="_kxNxQjP">YouTube Movie Reviews: Sentiment Analysis in an Audio-Visual Context</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Wollmer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Felix</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tobias</forename><surname>Knaup</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Bjorn</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Congkai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kenji</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><forename type="middle">Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_KMUr2zT" coord="22,309.47,617.02,75.95,7.54">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Martin Wollmer, Felix Weninger, Tobias Knaup, Bjorn Schuller, Congkai Sun, Kenji Sagae, and Louis Philippe Morency. 2013. YouTube Movie Reviews: Sentiment Analysis in an Audio-Visual Context. IEEE Intelligent Systems 28, 3 (2013), 46ś53.</note>
</biblStruct>

<biblStruct coords="22,97.77,626.77,433.53,7.75;22,97.77,636.95,170.71,7.54" xml:id="b55">
	<analytic>
		<title level="a" type="main" coord="22,326.55,626.77,142.65,7.75" xml:id="_rsGGAuX">How powerful are graph neural networks?</title>
		<author>
			<persName coords=""><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CRrKq44" coord="22,484.91,626.99,46.39,7.54;22,97.77,636.95,167.53,7.54">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2019. How powerful are graph neural networks?. In Proceedings of International Conference on Learning Representations.</note>
</biblStruct>

<biblStruct coords="23,97.77,108.71,434.76,7.75;23,97.77,118.68,433.53,7.75;23,97.77,128.64,76.74,7.75" xml:id="b56">
	<monogr>
		<title level="m" type="main" coord="23,97.77,118.68,380.36,7.75" xml:id="_rbbXFca">MTGAT: Multimodal Temporal Graph Attention Networks for Unaligned Human Multimodal Language Sequences</title>
		<author>
			<persName coords=""><forename type="first">Jianing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yongxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ruitao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yuying</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Azaan</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11985</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note type="raw_reference">Jianing Yang, Yongxin Wang, Ruitao Yi, Yuying Zhu, Azaan Rehman, Amir Zadeh, Soujanya Poria, and Louis-Philippe Morency. 2020. MTGAT: Multimodal Temporal Graph Attention Networks for Unaligned Human Multimodal Language Sequences. arXiv preprint arXiv:2010.11985 (2020).</note>
</biblStruct>

<biblStruct coords="23,97.77,138.60,433.53,7.75;23,97.77,148.56,176.80,7.75" xml:id="b57">
	<analytic>
		<title level="a" type="main" coord="23,238.75,138.60,208.05,7.75" xml:id="_MrtyRRa">CM-BERT: Cross-Modal BERT for Text-Audio Sentiment Analysis</title>
		<author>
			<persName coords=""><forename type="first">Kaicheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_HmMAFYF" coord="23,460.48,138.82,70.82,7.54;23,97.77,148.78,142.95,7.54">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page">528</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Kaicheng Yang, Hua Xu, and Kai Gao. 2020. CM-BERT: Cross-Modal BERT for Text-Audio Sentiment Analysis. In Proceedings of the 28th ACM International Conference on Multimedia. 521ś528.</note>
</biblStruct>

<biblStruct coords="23,97.77,158.53,433.53,7.75;23,97.77,168.49,320.28,7.75" xml:id="b58">
	<analytic>
		<title level="a" type="main" coord="23,295.36,158.53,235.94,7.75;23,97.77,168.49,26.56,7.75" xml:id="_FknBwN4">Image-text Multimodal Emotion Classiication via Multi-view Attentional Network</title>
		<author>
			<persName coords=""><forename type="first">Xiaocui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yifei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2020.3035277</idno>
		<ptr target="https://doi.org/10.1109/TMM.2020.3035277" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_G43PHr7" coord="23,131.00,168.70,105.23,7.54">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Xiaocui Yang, Shi Feng, Daling Wang, and Yifei Zhang. 2020. Image-text Multimodal Emotion Classiication via Multi-view Attentional Network. IEEE Transactions on Multimedia (2020), 1ś1. https://doi.org/10.1109/TMM.2020.3035277</note>
</biblStruct>

<biblStruct coords="23,97.77,178.45,433.53,7.75;23,97.77,188.41,328.81,7.75" xml:id="b59">
	<analytic>
		<title level="a" type="main" coord="23,420.68,178.45,110.62,7.75;23,97.77,188.41,114.33,7.75" xml:id="_4JRzm3k">Hierarchical graph representation learning with diferentiable pooling</title>
		<author>
			<persName coords=""><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9PdjXjn" coord="23,226.10,188.41,196.91,7.75">Advances in neural information processing systems. 4800ś</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">4810</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec. 2018. Hierarchical graph representation learning with diferentiable pooling. In Advances in neural information processing systems. 4800ś4810.</note>
</biblStruct>

<biblStruct coords="23,97.77,198.38,433.53,7.75;23,97.77,208.55,90.84,7.54" xml:id="b60">
	<analytic>
		<title level="a" type="main" coord="23,215.48,198.38,178.20,7.75" xml:id="_qcVnaJU">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName coords=""><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XBPP4hp" coord="23,407.63,198.59,123.67,7.54;23,97.77,208.55,87.66,7.54">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Fisher Yu and Vladlen Koltun. 2016. Multi-scale context aggregation by dilated convolutions. In Proceedings of International Conference on Learning Representations.</note>
</biblStruct>

<biblStruct coords="23,97.77,218.30,433.53,7.75;23,97.77,228.48,127.54,7.54" xml:id="b61">
	<analytic>
		<title level="a" type="main" coord="23,207.83,218.30,222.00,7.75" xml:id="_SZSrYRf">StructPool: Structured Graph Pooling via Conditional Random Fields</title>
		<author>
			<persName coords=""><forename type="first">Hao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_UHR2PgD" coord="23,443.42,218.52,87.88,7.54;23,97.77,228.48,124.36,7.54">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Hao Yuan and Shuiwang Ji. 2020. StructPool: Structured Graph Pooling via Conditional Random Fields. In Proceedings of International Conference on Learning Representations.</note>
</biblStruct>

<biblStruct coords="23,97.77,238.23,434.41,7.75;23,97.77,248.19,128.51,7.75" xml:id="b62">
	<analytic>
		<title level="a" type="main" coord="23,228.59,238.23,144.21,7.75" xml:id="_nRcDNJc">Speaker identiication on the SCOTUS corpus</title>
		<author>
			<persName coords=""><forename type="first">Jiahong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Liberman</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.2935783</idno>
		<ptr target="https://doi.org/10.1121/1.2935783" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_FwxQX6Y" coord="23,378.98,238.44,117.03,7.54">Acoustical Society of America Journal</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page">3878</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jiahong Yuan and Mark Liberman. 2008. Speaker identiication on the SCOTUS corpus. Acoustical Society of America Journal 123 (2008), 3878. https://doi.org/10.1121/1.2935783</note>
</biblStruct>

<biblStruct coords="23,97.77,258.15,433.53,7.75;23,97.77,268.12,397.80,7.75" xml:id="b63">
	<analytic>
		<title level="a" type="main" coord="23,404.10,258.15,127.20,7.75;23,97.77,268.12,61.64,7.75" xml:id="_qx7Qe3b">Tensor Fusion Network for Multimodal Sentiment Analysis</title>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><forename type="middle">Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_y5RxDUq" coord="23,173.40,268.33,281.30,7.54">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017">2017. 1114ś1125</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis Philippe Morency. 2017. Tensor Fusion Network for Multimodal Sentiment Analysis. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. 1114ś1125.</note>
</biblStruct>

<biblStruct coords="23,97.77,278.08,433.53,7.75;23,97.77,288.04,431.41,7.75" xml:id="b64">
	<analytic>
		<title level="a" type="main" coord="23,259.12,278.08,193.49,7.75;23,478.76,278.08,52.54,7.75;23,97.77,288.04,142.03,7.75" xml:id="_DyTkCP8">Soujanya Poria, Erik Cambria, and Louis Philippe Morency</title>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Navonil</forename><surname>Mazumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Y6Q4zKK" coord="23,253.79,288.26,189.30,7.54">Proceedings of the AAAI Conference on Artiicial Intelligence</title>
		<meeting>the AAAI Conference on Artiicial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="5634" to="5641" />
		</imprint>
	</monogr>
	<note>Memory Fusion Network for Multi-view Sequential Learning</note>
	<note type="raw_reference">Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, and Louis Philippe Morency. 2018. Memory Fusion Network for Multi-view Sequential Learning. In Proceedings of the AAAI Conference on Artiicial Intelligence (Vol. 32, No. 1). 5634ś5641.</note>
</biblStruct>

<biblStruct coords="23,97.77,298.00,433.53,7.75;23,97.77,307.97,433.53,7.75;23,97.77,317.93,279.84,7.75" xml:id="b65">
	<analytic>
		<title level="a" type="main" coord="23,147.14,307.97,334.89,7.75" xml:id="_BeEHEDj">Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph</title>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Paul</forename><forename type="middle">Pu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jonathan</forename><surname>Vanbriesen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Edmund</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Minghai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><forename type="middle">Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CuFWDCg" coord="23,495.22,308.18,36.09,7.54;23,97.77,318.14,239.10,7.54">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2236ś2246</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Amir Zadeh, Paul Pu Liang, Jonathan Vanbriesen, Soujanya Poria, Edmund Tong, Erik Cambria, Minghai Chen, and Louis Philippe Morency. 2018. Multimodal Language Analysis in the Wild: CMU-MOSEI Dataset and Interpretable Dynamic Fusion Graph. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. 2236ś2246.</note>
</biblStruct>

<biblStruct coords="23,97.77,327.89,433.54,7.75;23,97.77,337.85,251.23,7.75" xml:id="b66">
	<analytic>
		<title level="a" type="main" coord="23,341.87,327.89,189.44,7.75;23,97.77,337.85,96.09,7.75" xml:id="_7eKq5C5">Multimodal Sentiment Intensity Analysis in Videos: Facial Gestures and Verbal Messages</title>
		<author>
			<persName coords=""><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Eli</forename><surname>Pincus</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Louis</forename><forename type="middle">Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jPEC2Jn" coord="23,200.35,338.07,75.95,7.54">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="82" to="88" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Amir Zadeh, Rowan Zellers, Eli Pincus, and Louis Philippe Morency. 2016. Multimodal Sentiment Intensity Analysis in Videos: Facial Gestures and Verbal Messages. IEEE Intelligent Systems 31, 6 (11 2016), 82ś88.</note>
</biblStruct>

<biblStruct coords="23,97.77,347.82,433.53,7.75;23,97.77,357.78,64.65,7.75" xml:id="b67">
	<analytic>
		<title level="a" type="main" coord="23,221.39,347.82,157.47,7.75" xml:id="_jPuC62k">Link prediction based on graph neural networks</title>
		<author>
			<persName coords=""><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_qX3MGke" coord="23,393.29,348.03,138.01,7.54;23,97.77,357.78,61.08,7.75">Advances in Neural Information Processing Systems. 5165ś</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">5175</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural networks. In Advances in Neural Information Processing Systems. 5165ś5175.</note>
</biblStruct>

<biblStruct coords="23,97.77,367.74,434.76,7.75;23,97.77,377.70,189.22,7.75" xml:id="b68">
	<analytic>
		<title level="a" type="main" coord="23,324.51,367.74,204.95,7.75" xml:id="_WVTc2ds">An end-to-end deep learning architecture for graph classiication</title>
		<author>
			<persName coords=""><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_z6uwRFe" coord="23,106.45,377.92,177.64,7.54">Thirty-Second AAAI Conference on Artiicial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Muhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. 2018. An end-to-end deep learning architecture for graph classiication. In Thirty-Second AAAI Conference on Artiicial Intelligence.</note>
</biblStruct>

<biblStruct coords="23,97.77,387.67,434.88,7.75;23,97.77,397.63,434.76,7.75;23,97.77,407.59,101.61,7.75" xml:id="b69">
	<analytic>
		<title level="a" type="main" coord="23,393.15,387.67,139.50,7.75;23,97.77,397.63,131.24,7.75" xml:id="_usNwSud">Afective Computing for Large-Scale Heterogeneous Multimedia Data: A Survey</title>
		<author>
			<persName coords=""><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Shangfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhiraj</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1145/3363560</idno>
		<ptr target="https://doi.org/10.1145/3363560" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_yzWkJeG" coord="23,237.89,397.84,163.62,7.54">ACM Trans. Multimedia Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">93</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2019-12">2019. dec 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Sicheng Zhao, Shangfei Wang, Mohammad Soleymani, Dhiraj Joshi, and Qiang Ji. 2019. Afective Computing for Large-Scale Het- erogeneous Multimedia Data: A Survey. ACM Trans. Multimedia Comput. Commun. Appl. 15, 3s, Article 93 (dec 2019), 32 pages. https://doi.org/10.1145/3363560</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
