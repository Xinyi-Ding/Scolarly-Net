<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main" xml:id="_Pb4HaEu" coord="1,45.94,81.42,374.62,12.93;1,45.94,98.35,52.69,12.93">Online Application Guidance for Heterogeneous Memory Systems</title>
				<funder ref="#_2CcysVT">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_8zFyFF5">
					<orgName type="full">Exascale Computing Project</orgName>
				</funder>
				<funder>
					<orgName type="full">U.S. Department of Energy Office of Science</orgName>
				</funder>
				<funder>
					<orgName type="full">National Nuclear Security Administration</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName coords="1,123.67,131.72,84.66,8.12"><roleName>USA</roleName><forename type="first">Intel</forename><surname>Corporation</surname></persName>
							<idno type="ORCID">0000-0003-1131-5967</idno>
						</author>
						<author>
							<persName coords="1,205.20,143.43,88.98,9.82"><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Jantz</surname></persName>
							<email>mrjantz@utk.edu</email>
							<idno type="ORCID">0000-0003-4460-1206</idno>
						</author>
						<author>
							<persName coords="1,110.22,531.18,39.21,7.22"><forename type="first">M</forename><forename type="middle">Ben</forename><surname>Olson</surname></persName>
						</author>
						<author>
							<persName coords="1,166.61,531.18,36.73,7.22"><forename type="first">Kshitij</forename><forename type="middle">A</forename><surname>Doshi</surname></persName>
							<email>kshitij.a.doshi@intel.com</email>
						</author>
						<author>
							<persName coords="1,288.64,531.18,40.23,7.22"><forename type="first">W</forename><surname>Chandler</surname></persName>
						</author>
						<author>
							<persName coords="1,331.47,531.18,61.60,7.22"><roleName>AZ</roleName><forename type="first">Blvd</forename><surname>Chandler</surname></persName>
						</author>
						<author>
							<persName coords="1,188.15,541.15,57.17,7.22"><forename type="first">Brandon</forename><surname>Kammerdiener</surname></persName>
						</author>
						<author>
							<persName coords="2,345.24,114.57,41.55,8.12"><forename type="first">Terry</forename><surname>Jones</surname></persName>
							<email>trjones@ornl.gov.</email>
						</author>
						<author>
							<persName coords="2,415.50,114.57,24.82,8.12"><surname>Online</surname></persName>
						</author>
						<author>
							<affiliation>
								<orgName>University of Tennessee, </orgName>
								<address><addrLine>USA Min H Kao Bldg, Room 605 1520 Middle Drive Knoxville, TN 37996 5200, 1 Bethel Valley Road Oak Ridge, TN 37830</addrLine></address>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> University of Tennessee, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> T. Jones, Oak Ridge National Laboratory, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> Application Guidance for Heterogeneous Memory Systems.</orgName>
							</affiliation>
						</author>
						<title level="a" type="main" xml:id="_V9gbfax" coord="1,45.94,81.42,374.62,12.93;1,45.94,98.35,52.69,12.93">Online Application Guidance for Heterogeneous Memory Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A07D8B1D4CF45629CDE33EDDA4D54468</idno>
					<idno type="DOI">10.1145/3533855</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-03-13T15:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term xml:id="_vW67ZBW">Profiling</term>
					<term xml:id="_SZkrBYE">analysis</term>
					<term xml:id="_2ZSxNWx">runtime systems</term>
					<term xml:id="_v3JUGu8">heterogeneous memory management</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_QbYKgDT"><p xml:id="_A3bgDG8"><s xml:id="_nBpvPXX" coords="1,45.63,200.13,394.82,8.12;1,45.94,211.10,271.28,8.12">As scaling of conventional memory devices has stalled, many high-end computing systems have begun to incorporate alternative memory technologies to meet performance goals.</s><s xml:id="_8SQ5Da8" coords="1,320.04,211.10,120.45,8.12;1,45.94,222.05,394.51,8.12;1,45.94,233.01,394.51,8.12;1,45.94,243.97,78.33,8.12">Since these technologies present distinct advantages and tradeoffs compared to conventional DDR* SDRAM, such as higher bandwidth with lower capacity or vice versa, they are typically packaged alongside conventional SDRAM in a heterogeneous memory architecture.</s><s xml:id="_BX7U3C4" coords="1,125.89,243.97,314.56,8.12;1,45.94,254.93,278.69,8.12">To utilize the different types of memory efficiently, new data management strategies are needed to match application usage to the best available memory technology.</s><s xml:id="_5BAUmUZ" coords="1,326.80,254.93,113.86,8.12;1,45.94,265.89,394.49,8.12;1,45.94,276.85,394.54,8.12;1,45.94,287.81,352.63,8.12">However, current proposals for managing heterogeneous memories are limited, because they either (1) do not consider high-level application behavior when assigning data to different types of memory or (2) require separate program execution (with a representative input) to collect information about how the application uses memory resources.</s></p><p xml:id="_NYmZv5w"><s xml:id="_dmMBB4R" coords="1,55.91,298.77,384.74,8.12;1,45.94,309.72,109.41,8.12">This work presents a new data management toolset to address the limitations of existing approaches for managing complex memories.</s><s xml:id="_nZz8UC2" coords="1,157.48,309.72,284.52,8.12;1,45.94,320.68,394.77,8.12;1,45.94,331.65,223.46,8.12">It extends the application runtime layer with automated monitoring and management routines that assign application data to the best tier of memory based on previous usage, without any need for source code modification or a separate profiling run.</s><s xml:id="_9vuqq3G" coords="1,271.53,331.65,168.97,8.12;1,45.94,342.60,394.54,8.12;1,45.94,353.56,372.26,8.12">It evaluates this approach on a state-of-the-art server platform with both conventional DDR4 SDRAM and non-volatile Intel Optane DC memory, using both memory-intensive high-performance computing (HPC) applications as well as standard benchmarks.</s><s xml:id="_r2ujw52" coords="1,420.85,353.56,21.13,8.12;1,45.94,364.52,394.50,8.12;1,45.94,375.48,283.40,8.12">Overall, the results show that this approach improves program performance significantly compared to a standard unguided approach across a variety of workloads and system configurations.</s><s xml:id="_tWW6eU4" coords="1,331.89,375.48,108.58,8.12;1,45.94,386.44,278.69,8.12">The HPC applications exhibit the largest benefits, with speedups ranging from 1.4× to 7× in the best cases.</s><s xml:id="_Vn2xQgr" coords="1,326.72,386.44,113.75,8.12;1,45.94,397.40,394.51,8.12;1,45.94,408.36,287.32,8.12">Additionally, we show that this approach achieves similar performance as a comparable offline profiling-based approach after a short startup period, without requiring separate program execution or offline analysis steps.</s><s xml:id="_8E92Jdv" coords="1,45.94,425.00,396.12,8.45;1,45.94,435.95,143.83,8.45;1,45.95,461.45,394.51,7.22;1,45.95,471.42,394.52,7.22;1,45.95,481.38,165.79,7.22">CCS Concepts: • Software and its engineering → Runtime environments; • Computer systems organization → Heterogeneous (hybrid) systems; Extension of Conference Paper: While not a true extension, this work builds upon prior contributions published at the IEEE International Conference on Networking, Architecture, and Storage (Best Paper Award) [60] and the ACM/IEEE International Symposium on Memory Systems [58].</s><s xml:id="_pnmJbmk" coords="1,213.26,481.38,227.22,7.22;1,45.95,491.34,39.84,7.22">The tools and approach developed for these earlier works are described in Section 3.</s><s xml:id="_ux9SfkR" coords="1,87.78,491.34,195.73,7.22">All other sections present original results and contributions.</s></p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<facsimile>
		<surface n="1" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="2" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="3" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="4" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="5" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="6" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="7" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="8" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="9" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="10" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="11" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="12" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="13" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="14" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="15" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="16" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="17" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="18" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="19" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="20" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="21" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="22" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="23" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="24" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="25" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="26" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
		<surface n="27" ulx="0.0" uly="0.0" lrx="486.0" lry="720.0"/>
	</facsimile>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1" xml:id="_jtaSzh4">INTRODUCTION</head><p xml:id="_U3QJAar"><s xml:id="_EMJmh29" coords="2,45.77,201.49,394.52,9.03;2,45.77,213.45,74.56,9.03">Recent market and architectural trends have placed enormous strain on the memory system of modern machines.</s><s xml:id="_Z4EmWHD" coords="2,122.64,213.45,317.67,9.03;2,45.77,225.40,396.22,9.03;2,45.77,237.36,241.41,9.03">The popularity of artificial intelligence and other types of data-driven decision making, which often require frequent and detailed analyses of very large datasets, have driven demands for in-memory processing to unprecedented heights.</s><s xml:id="_UvVYdd4" coords="2,289.35,237.36,150.94,9.03;2,45.77,249.32,394.81,9.03;2,45.47,261.27,324.47,9.03">Unfortunately, high energy costs <ref type="bibr" coords="2,423.95,237.36,16.33,9.03" target="#b43">[44]</ref> and other technological constraints <ref type="bibr" coords="2,197.46,249.32,15.00,9.03" target="#b36">[37,</ref><ref type="bibr" coords="2,216.31,249.32,12.82,9.03" target="#b53">54]</ref> have limited dynamic random access memory (DRAM) capacity and bandwidth improvements for several product generations.</s><s xml:id="_EhNPEvE" coords="2,371.79,261.27,69.62,9.03;2,45.78,273.23,396.21,9.03;2,45.78,285.18,71.65,9.03">At the same time, CPU core counts still continue to rise, and many applications now rely on multiprocessing for performance scaling.</s><s xml:id="_j2ZMYmU" coords="2,120.10,285.18,320.20,9.03;2,45.78,297.13,293.08,9.03">The end result is that the access and allocation demands of the processor have significantly outpaced the capabilities of conventional memory systems.</s></p><p xml:id="_3EfYu9q"><s xml:id="_zJQfaPV" coords="2,55.74,309.09,386.27,9.03;2,45.78,321.05,394.54,9.03;2,45.78,333.00,22.08,9.03">To address this challenge, many high-end computing systems have begun to incorporate multiple types of memory hardware, distinct from conventional DRAM, within the same compute node.</s><s xml:id="_WTgpbbz" coords="2,70.44,332.95,370.97,9.08;2,45.41,344.96,394.90,9.03;2,45.78,356.91,168.32,9.03">Such heterogeneous memory architectures organize main memory in a hierarchical fashion, where each layer of the hierarchy corresponds to a different memory technology with distinct performance and capacity characteristics.</s><s xml:id="_j7CDHY4" coords="2,216.39,356.91,223.90,9.03;2,44.65,368.86,395.91,9.03;2,45.41,380.82,122.71,9.03">For example, Intel's latest Xeon processors (codenamed "Cascade Lake") support access to conventional DRAM as well as non-volatile Optane DC memory within the same address space.</s><s xml:id="_aww8JA7" coords="2,169.94,380.82,272.04,9.03;2,45.78,392.78,311.74,9.03">While this configuration greatly expands the capacity of main memory, access to the non-volatile tier has limited bandwidth and longer latencies.</s><s xml:id="_3AnqdU7" coords="2,359.47,392.78,80.84,9.03;2,45.78,404.73,396.17,9.03;2,45.41,416.69,395.47,9.03;2,45.78,428.64,379.96,9.03">Other systems, such as the (now defunct) Intel Knights Landing, and many GPU-based platforms package high bandwidth (but lower capacity) memories (commonly known as "on-package" or "die-stacked" RAMs) alongside conventional memory to enable better performance for a portion of main memory.</s><s xml:id="_E6qadcS" coords="2,428.48,428.64,13.49,9.03;2,45.78,440.59,395.13,9.03;2,45.78,452.56,356.95,9.03">Future memory systems are expected to be even more complex as architectures with three (or more) types of memory and more flexible operating modes have already been announced <ref type="bibr" coords="2,384.18,452.56,14.84,9.03" target="#b13">[14]</ref>.</s></p><p xml:id="_uqeqYtb"><s xml:id="_Yt8fx3g" coords="2,55.74,464.51,384.55,9.03;2,45.78,476.47,88.65,9.03">Despite their potential benefits, heterogeneous memory architectures present new challenges for data management.</s><s xml:id="_6avtssd" coords="2,136.77,476.47,305.20,9.03;2,45.78,488.42,395.17,9.03;2,45.78,500.37,395.28,9.03;2,45.78,512.33,130.27,9.03">Computing systems have traditionally viewed memory as a single homogeneous address space, sometimes divided into different non-uniform memory access (NUMA) domains, but consisting entirely of the same storage medium (i.e., double data rate (DDR)* synchronous DRAM (SDRAM)).</s><s xml:id="_yZSPmH2" coords="2,178.96,512.33,263.03,9.03;2,45.78,524.29,396.21,9.03;2,45.78,536.24,238.97,9.03">To utilize heterogeneous resources efficiently, alternative strategies are needed to match data to the appropriate technology in consideration of hardware capabilities, application usage, and in some cases, NUMA domain.</s></p><p xml:id="_A5eJbpv"><s xml:id="_U8W8Rub" coords="2,55.74,548.20,384.55,9.03;2,45.78,560.15,396.23,9.03;2,45.78,572.10,21.55,9.03">Spurred by this problem, the architecture and systems communities have proposed a range of hardware and software techniques to manage data efficiently on heterogeneous memory systems.</s><s xml:id="_TxUWbNx" coords="2,70.19,572.10,370.13,9.03;2,45.78,584.06,394.51,9.03;2,45.78,596.02,394.55,9.03;2,45.78,607.97,394.54,9.03;2,45.78,619.93,42.10,9.03">The existing solutions exhibit various advantages, disadvantages, and tradeoffs, with most hardware-based techniques offering more ease of use and software transparency at the expense of flexibility and efficiency, while software-based solutions provide more fine-grained control of data placement (and, thus, better performance) in exchange for additional effort from developers and users.</s><s xml:id="_hdEJxaK" coords="2,91.56,619.93,311.17,9.03">Section 2 provides a more detailed overview of these existing approaches.</s><s xml:id="_hQXqdx8" coords="2,406.40,619.93,35.62,9.03;2,45.78,631.88,394.53,9.03;2,45.78,643.83,392.86,9.03">Unfortunately, there is currently no silver bullet, as the more flexible and more efficient software-based approaches still require significant efforts (and, in many cases, expert knowledge) to be effective.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_RhuEemU">Online Application Guidance for Heterogeneous Memory Systems 45:3</head><p xml:id="_KH5FzM8"><s xml:id="_AGRUvCY" coords="3,55.91,82.72,384.83,9.08;3,45.94,94.68,234.82,9.08">To fill this gap, we began developing a hybrid data management solution for complex memory systems based on automated application guidance <ref type="bibr" coords="3,248.69,94.73,15.01,9.03" target="#b56">[58,</ref><ref type="bibr" coords="3,265.76,94.73,11.25,9.03" target="#b58">60]</ref>.</s><s xml:id="_3vYEqY3" coords="3,282.83,94.73,157.64,9.03;3,45.95,106.63,394.54,9.08;3,45.95,118.63,186.19,9.03">Our previous approach employs source code analysis and offline architectural profiling to collect information about how applications use different regions in their virtual address space.</s><s xml:id="_e33Ea2T" coords="3,234.16,118.63,206.29,9.03;3,45.95,130.60,394.53,9.03;3,45.95,142.55,189.12,9.03">It also includes a recommendation engine, based on sorting and bin-packing heuristics, to decide which memory tier to use for data allocated during subsequent executions of the same application.</s><s xml:id="_gqs2xMY" coords="3,237.15,142.55,205.01,9.03;3,45.95,154.50,288.37,9.03">While this approach can significantly improve performance for many applications, it still has some significant limitations.</s><s xml:id="_SBvU4YT" coords="3,336.48,154.50,103.97,9.03;3,45.95,166.46,394.53,9.03;3,45.95,178.36,394.52,9.08;3,45.94,190.37,198.47,9.03">Specifically, (1) it requires earlier execution with a representative input to collect information about how the application uses program data objects, and (2) it only provides static placement recommendations and cannot adjust data-tier assignments as application usage shifts.</s></p><p xml:id="_wJQrcNq"><s xml:id="_afTbEmD" coords="3,55.91,202.33,384.57,9.03;3,45.94,214.23,394.54,9.08;3,45.94,226.19,299.65,9.08">This work addresses these limitations by extending our previous approach and toolset with online components that are able to collect and apply application-level memory tiering guidance during production execution and without the need for a separate profile run.</s><s xml:id="_vp3CRSM" coords="3,347.64,226.24,92.83,9.03;3,45.95,238.19,396.22,9.03;3,45.95,250.14,394.77,9.03;3,45.95,262.11,153.25,9.03">We evaluate our online approach using high-performance computing (HPC) as well as standard (SPEC CPU) computing benchmarks on an Intel Cascade Lake platform with two tiers of memory: conventional DDR4 SDRAM and non-volatile Optane DC.</s><s xml:id="_PJcdB9z" coords="3,201.88,262.11,240.29,9.03;3,45.95,274.06,394.73,9.03;3,45.95,286.01,340.16,9.03">Our experiments show that our updated toolset can generate effective tiering guidance with very low overhead and typically achieves performance similar to our previous offline profiling-based approach after a short initial startup period.</s><s xml:id="_4yMJcAB" coords="3,389.04,286.01,51.69,9.03;3,45.95,297.97,123.94,9.03">The primary contributions of this work are:</s></p><p xml:id="_tgGqjuz"><s xml:id="_gaZqVZB" coords="3,55.80,314.90,302.28,9.03;3,358.09,313.07,3.38,6.59;3,365.44,314.90,75.40,9.03;3,70.35,326.86,284.41,9.03">(1) We extend the Simplified Interface to Complex Memory (SICM) <ref type="foot" coords="3,358.09,313.07,3.38,6.59" target="#foot_1">1</ref> runtime with new techniques for profiling memory usage during production execution.</s><s xml:id="_YJ44ePK" coords="3,357.96,326.86,82.52,9.03;3,70.35,338.81,370.13,9.03;3,70.35,350.77,343.87,9.03">For the benchmarks in this study, our approach is able to collect detailed data-tiering guidance with negligible execution time overhead in most cases and less than 10% overhead in the worst case.</s><s xml:id="_weCxERC" coords="3,55.80,362.72,384.70,9.03;3,70.35,374.68,371.68,9.03"><ref type="bibr" coords="3,55.80,362.72,10.57,9.03" target="#b1">(2)</ref> We design and implement an online data tiering solution that leverages this application feedback to steer data allocation and placement across a heterogeneous memory hierarchy.</s><s xml:id="_ZaqenyZ" coords="3,70.35,386.64,370.12,9.03;3,69.99,398.59,336.63,9.03">Our approach, inspired by solutions to the classical ski rental problem, only migrates data when the expected cost of doing so is outweighed by the cost of leaving it in place.</s><s xml:id="_HrUqXgB" coords="3,55.80,410.54,386.34,9.03;3,70.35,422.50,296.33,9.03"><ref type="bibr" coords="3,55.80,410.54,10.57,9.03" target="#b2">(3)</ref> We demonstrate the effectiveness of this approach on a state-of-the-art heterogeneous memory system with both conventional DRAM and large-capacity NVRAM.</s><s xml:id="_kVaPXvN" coords="3,369.96,422.50,70.88,9.03;3,70.35,434.46,370.15,9.03;3,70.35,446.41,291.15,9.03">The results show that it significantly outperforms unguided execution on average and achieves speedups ranging from 1.4× to more than 7× for our selected HPC workloads.</s><s xml:id="_quRbGsT" coords="3,365.29,446.41,76.88,9.03;3,70.35,458.37,370.13,9.03;3,70.35,470.32,28.44,9.03">Additionally, it attains speedups similar to a comparable offline profiling-based approach after a short startup period.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2" xml:id="_SF4CDhz">RELATED WORK 2.1 Data Management Strategies for Conventional Systems</head><p xml:id="_84RkFA9"><s xml:id="_25eScfE" coords="3,45.95,524.05,343.52,9.03">Data placement is a long-standing and well-studied problem in computer science.</s><s xml:id="_DcsZfQS" coords="3,393.20,524.05,47.49,9.03;3,45.58,536.01,394.92,9.03;3,45.95,547.96,182.98,9.03">Many prior works have successfully used program profiling and analysis to improve data management across the cache, memory, and storage hierarchies.</s><s xml:id="_88PRTfu" coords="3,232.35,547.96,208.16,9.03;3,45.58,559.92,394.90,9.03;3,45.95,571.88,257.34,9.03">Some researchers have proposed static techniques with offline profiling and/or source code analysis to allocate hot fields and objects closer together in the heap, thereby improving caching efficiency <ref type="bibr" coords="3,235.90,571.88,10.36,9.03" target="#b8">[9,</ref><ref type="bibr" coords="3,248.18,571.88,11.45,9.03" target="#b27">28,</ref><ref type="bibr" coords="3,261.54,571.88,11.46,9.03" target="#b32">33,</ref><ref type="bibr" coords="3,274.91,571.88,11.45,9.03" target="#b41">42,</ref><ref type="bibr" coords="3,288.28,571.88,11.26,9.03" target="#b63">65]</ref>.</s><s xml:id="_3aGFJyZ" coords="3,305.20,571.88,136.95,9.03;3,45.95,583.83,394.49,9.03;3,45.95,595.79,394.34,9.03">Others have combined online profiling with high-level language features, such as object indirection and garbage collection, to enable similar benefits transparently, and in an adaptive runtime environment <ref type="bibr" coords="3,337.25,595.79,99.29,9.03">[11, 12, 25-27, 67, 71, 74]</ref>.</s></p><p xml:id="_NXmd2j2"><s xml:id="_69ECpeX" coords="4,55.74,82.77,384.56,9.03;4,45.77,94.73,162.69,9.03">A number of other works integrate application-level guidance with physical data management in the operating system and hardware.</s><s xml:id="_txshUNq" coords="4,212.13,94.73,228.18,9.03;4,45.77,106.68,394.53,9.03;4,45.77,118.63,127.95,9.03">Some projects developed frameworks to expose kernel resources to applications <ref type="bibr" coords="4,151.40,106.68,10.38,9.03" target="#b6">[7,</ref><ref type="bibr" coords="4,165.03,106.68,12.82,9.03" target="#b19">20]</ref> or to facilitate communication between user-and system-level data management <ref type="bibr" coords="4,121.48,118.63,10.37,9.03" target="#b4">[5,</ref><ref type="bibr" coords="4,134.71,118.63,6.83,9.03" target="#b5">6,</ref><ref type="bibr" coords="4,144.39,118.63,11.46,9.03" target="#b31">32,</ref><ref type="bibr" coords="4,158.72,118.63,11.25,9.03" target="#b37">38]</ref>.</s><s xml:id="_Tp7d7gN" coords="4,176.59,118.63,263.71,9.03;4,45.40,130.60,394.91,9.03;4,45.77,142.55,394.71,9.03;4,45.77,154.50,117.11,9.03">More recent efforts have combined these cross-layer approaches with automated collection of high-level guidance to address a variety of issues, including: DRAM energy <ref type="bibr" coords="4,76.39,142.55,15.00,9.03" target="#b30">[31,</ref><ref type="bibr" coords="4,93.89,142.55,11.25,9.03" target="#b57">59]</ref>, cache pollution <ref type="bibr" coords="4,176.35,142.55,14.83,9.03" target="#b23">[24]</ref>, NUMA traffic congestion <ref type="bibr" coords="4,302.14,142.55,14.84,9.03" target="#b14">[15]</ref>, and data movement costs for non-uniform caches <ref type="bibr" coords="4,130.20,154.50,15.00,9.03" target="#b52">[53,</ref><ref type="bibr" coords="4,147.89,154.50,11.25,9.03" target="#b68">70]</ref>.</s><s xml:id="_3JtZdcG" coords="4,165.58,154.50,274.72,9.03;4,45.77,166.46,390.70,9.03">While these works evince some of the benefits of integrating usage feedback during data management, their purposes and goals are very different from this project.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2" xml:id="_93ZDtww">Data Management Strategies for Heterogeneous Memory Systems</head><p xml:id="_bXsdkrB"><s xml:id="_xrJ6YYx" coords="4,45.77,202.33,395.59,9.03;4,45.77,214.28,396.08,9.03">Propelled by the simultaneous growth of data analytics and stalling of conventional DRAM scaling, research interest in alternative memory technologies has grown significantly over the past decade.</s><s xml:id="_WRAPfTQ" coords="4,45.48,226.24,394.82,9.03;4,45.77,238.19,394.52,9.03;4,45.77,250.14,110.48,9.03">The shifting landscape has pushed the architecture, systems, and high-performance computing communities to propose new strategies, tools, and techniques for mapping application data across heterogeneous device tiers.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_q5d5nWb">2.2.1</head><p xml:id="_TPFwCD5"><s xml:id="_vaBeMev" coords="4,83.35,268.13,139.13,8.98">Hardware-managed DRAM Caches.</s><s xml:id="_rtmRysn" coords="4,225.97,268.08,214.51,9.03;4,45.77,280.03,196.96,9.03">One common strategy is to exercise the faster, smaller capacity tier(s) as a hardware-managed cache.</s><s xml:id="_dJRrnkc" coords="4,247.24,280.03,193.06,9.03;4,44.65,291.98,397.34,9.03;4,45.77,303.95,143.67,9.03">For example, Intel's Cascade Lake includes a "memory-mode" option, which applies this approach with DDR4 as a direct-mapped cache to storage class Optane DC memory <ref type="bibr" coords="4,170.89,303.95,14.84,9.03" target="#b28">[29]</ref>.</s><s xml:id="_EKVJwFz" coords="4,192.26,303.95,248.03,9.03;4,45.77,315.90,394.53,9.03;4,45.77,327.86,215.11,9.03">While hardware-managed caching provides some immediate advantages, such as software-transparency and backwards compatibility, it is inflexible, often less efficient, and reduces the system's available capacity.</s></p><p xml:id="_C2ktzVf"><s xml:id="_xUu2zQU" coords="4,55.74,339.81,386.23,9.03;4,45.77,351.76,394.50,9.03;4,45.48,363.72,394.81,9.03;4,45.78,375.68,114.31,9.03">Some works have proposed architectural strategies to address these issues, for example, by colocating tags and data in DRAM to increase efficiency <ref type="bibr" coords="4,266.14,351.76,15.02,9.03" target="#b46">[47,</ref><ref type="bibr" coords="4,283.62,351.76,11.26,9.03" target="#b50">51]</ref>, keeping track of cache contents in TLBs and page tables to reduce metadata traffic <ref type="bibr" coords="4,236.89,363.72,14.99,9.03" target="#b29">[30,</ref><ref type="bibr" coords="4,253.90,363.72,11.45,9.03" target="#b42">43,</ref><ref type="bibr" coords="4,267.36,363.72,11.26,9.03">73]</ref>, or swapping data lines out of the cache to preserve capacity <ref type="bibr" coords="4,128.18,375.68,15.00,9.03" target="#b12">[13,</ref><ref type="bibr" coords="4,145.07,375.68,11.26,9.03" target="#b66">68]</ref>.</s><s xml:id="_shaKJDv" coords="4,161.97,375.68,279.85,9.03">Mittal and Vetter provide a modern (2016) survey of this research <ref type="bibr" coords="4,423.27,375.68,14.84,9.03" target="#b51">[52]</ref>.</s><s xml:id="_b359wqe" coords="4,45.78,387.63,394.76,9.03;4,45.78,399.59,396.20,9.03;4,45.78,411.54,79.70,9.03">In contrast to these works, this work extends and develops techniques to increase efficiency solely through software-driven data placement, without relying on architectural modifications or nonstandard hardware.</s><s xml:id="_pSGFu3t" coords="4,129.17,411.54,311.13,9.03;4,45.78,423.44,394.53,9.08;4,45.78,435.45,219.83,9.03">Some recent work has also shown that profile guidance can enhance data management on systems that support hardware-directed caching and OS paging simultaneously but for different portions of their address space <ref type="bibr" coords="4,247.06,435.45,14.84,9.03" target="#b17">[18]</ref>.</s><s xml:id="_sXpPfsN" coords="4,269.00,435.45,171.29,9.03;4,45.41,447.41,369.81,9.03">We expect the approach proposed in this work can also boost performance on platforms with such mixed data management options.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2" xml:id="_hct24ET">Software-directed Heterogeneous Memory Management.</head><p xml:id="_m9m88F5"><s xml:id="_BJeZGPv" coords="4,320.69,465.34,119.60,9.03;4,45.78,477.25,396.19,9.08;4,45.78,489.25,394.55,9.03;4,45.78,501.21,63.40,9.03">The alternative strategy of software-based data tiering uses either the OS by itself or the OS in conjunction with the application to assign data into different memory tiers, with facilities to allow migrations of data between tiers as needed.</s><s xml:id="_4rqt7e2" coords="4,112.09,501.21,328.25,9.03;4,45.78,513.16,396.05,9.03">Some heterogeneous memory systems also provide APIs that allow applications to control the placement of their data objects through the use of source code annotations <ref type="bibr" coords="4,409.34,513.16,15.01,9.03" target="#b9">[10,</ref><ref type="bibr" coords="4,426.83,513.16,11.25,9.03" target="#b54">56]</ref>.</s><s xml:id="_6YEC8G8" coords="4,45.48,525.11,394.84,9.03;4,45.78,537.07,253.07,9.03">These finer-grained controls enable developers to coordinate tier assignments with data allocation and usage patterns, potentially exposing powerful efficiencies.</s></p><p xml:id="_fgDbtbx"><s xml:id="_cJsU4gW" coords="4,55.74,549.03,384.57,9.03;4,45.78,560.98,219.90,9.03">Several prior works have integrated software-based data management with program profiling to facilitate the assignment of data to memory tiers.</s><s xml:id="_MaFD4YY" coords="4,269.35,560.98,170.97,9.03;4,45.78,572.94,370.42,9.03">For instance, some prior works integrate coarse-grained architectural profiling with page-level management in the OS <ref type="bibr" coords="4,360.58,572.94,10.35,9.03" target="#b1">[2,</ref><ref type="bibr" coords="4,373.37,572.94,11.46,9.03" target="#b35">36,</ref><ref type="bibr" coords="4,387.28,572.94,11.46,9.03" target="#b44">45,</ref><ref type="bibr" coords="4,401.19,572.94,11.25,9.03" target="#b49">50]</ref>.</s><s xml:id="_axREHq6" coords="4,418.64,572.94,21.65,9.03;4,45.78,584.89,394.51,9.03;4,45.78,596.84,394.53,9.03;4,45.78,608.80,173.31,9.03">Since these works do not attempt to coordinate tier assignments with application data structures and events, they may be vulnerable to inefficiencies that arise from the high-level software working at cross-purposes from the OS and hardware.</s><s xml:id="_qGZ5ntm" coords="4,55.74,620.75,385.67,9.03;4,45.77,632.71,367.27,9.03">Some other projects employ application-level tools to tag and profile certain data structures, and then use heuristic models to assign objects to the appropriate tier <ref type="bibr" coords="4,333.28,632.71,10.39,9.03" target="#b0">[1,</ref><ref type="bibr" coords="4,346.30,632.71,6.83,9.03" target="#b2">3,</ref><ref type="bibr" coords="4,355.76,632.71,11.45,9.03" target="#b15">16,</ref><ref type="bibr" coords="4,369.85,632.71,11.45,9.03" target="#b39">40,</ref><ref type="bibr" coords="4,383.94,632.71,11.46,9.03" target="#b61">63,</ref><ref type="bibr" coords="4,398.03,632.71,11.26,9.03" target="#b64">66]</ref>.</s><s xml:id="_Hjf2dMb" coords="4,415.67,632.71,24.62,9.03;4,45.77,644.67,394.77,9.03;5,45.95,55.82,271.61,8.97;5,424.39,55.82,16.09,8.97;5,45.95,288.15,21.61,8.07">While these efforts demonstrate that application guidance can be useful for certain usage scenarios, they Online Application Guidance for Heterogeneous Memory Systems 45:5 Fig. <ref type="figure" coords="5,61.41,288.15,3.07,8.07">1</ref>.</s><s xml:id="_fGpTWt6" coords="5,72.04,288.15,75.14,8.07">SICM overview <ref type="bibr" coords="5,130.40,288.15,13.43,8.07" target="#b40">[41]</ref>.</s><s xml:id="_tjBdWU6" coords="5,149.35,288.15,291.11,8.07;5,45.95,299.10,153.00,8.07">The high-level provides a portable API, while the low-level implements efficient data management for complex memories.</s></p><p xml:id="_9N98BG3"><s xml:id="_KAuGrAS" coords="5,45.95,320.83,396.24,9.03;5,45.95,332.79,83.73,9.03">require manual source code modifications or expensive online detection to attach recommendations to data objects.</s><s xml:id="_KKvgX62" coords="5,132.14,332.79,310.00,9.03;5,45.95,344.74,394.54,9.03;5,45.95,356.64,153.42,9.08">Several prior works, including our own, have attempted to address this limitation with static and lightweight runtime tools that are able to attach tiering guidance to program data automatically <ref type="bibr" coords="5,124.14,356.69,15.01,9.03" target="#b16">[17,</ref><ref type="bibr" coords="5,141.86,356.69,11.45,9.03" target="#b17">18,</ref><ref type="bibr" coords="5,156.02,356.69,11.45,9.03" target="#b56">58,</ref><ref type="bibr" coords="5,170.19,356.69,11.45,9.03" target="#b58">60,</ref><ref type="bibr" coords="5,184.35,356.69,11.26,9.03" target="#b70">72]</ref>.</s><s xml:id="_Uv9sZ64" coords="5,202.08,356.64,240.08,9.08;5,45.95,368.66,394.80,9.03;5,45.95,380.61,115.69,9.03">However, all of these previous works employ offline profiling and analysis to collect information about how the application uses memory and generate only static tier recommendations.</s><s xml:id="_erh5jTx" coords="5,164.13,380.61,276.35,9.03;5,45.95,392.56,394.76,9.03;5,45.95,404.52,119.47,9.03">In contrast, this project leverages lightweight architectural profiling and novel runtime algorithms to enable automated, feedback-directed data placement with very low execution time overhead.</s><s xml:id="_QRCnDRF" coords="5,167.77,404.52,272.71,9.03;5,45.95,416.47,191.98,9.03">Moreover, it does so without requiring earlier, profiled execution of the same application with representative input.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3" xml:id="_7hhamYH">OFFLINE APPLICATION GUIDANCE FOR HETEROGENEOUS MEMORY SYSTEMS</head><p xml:id="_WXYs2Yp"><s xml:id="_4EbDmrM" coords="5,45.65,455.49,394.82,9.08;5,45.95,467.50,254.13,9.03">The online data tiering approach described in this work builds upon our earlier efforts to improve application performance on heterogeneous memory systems.</s><s xml:id="_37uWR9b" coords="5,303.76,467.50,136.74,9.03;5,45.95,479.40,394.54,9.08;5,45.95,491.42,133.16,9.03">Our previous work extended the SICM runtime and API to implement an offline profile-based approach for guiding data placement on multi-level memory systems.</s><s xml:id="_rNDcb7k" coords="5,182.45,491.42,258.23,9.03;5,45.95,503.37,178.51,9.03">This section provides a brief overview of SICM as well as our offline approach, which is called MemBrain.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1" xml:id="_qkmAh6E">Simplified Interface to Complex Memory</head><p xml:id="_CjT5efk"><s xml:id="_pe8hAUu" coords="5,45.65,542.44,395.09,9.03;5,45.95,554.40,396.06,9.03">The U.S. Department of Energy Exascale Computing Project (ECP) is a large, multi-disciplinary effort with the goal of achieving exaFLOP performance in the supercomputing domain <ref type="bibr" coords="5,408.81,554.40,14.98,9.03" target="#b38">[39,</ref><ref type="bibr" coords="5,426.99,554.40,11.26,9.03" target="#b55">57]</ref>.</s><s xml:id="_vsw4wq4" coords="5,45.65,566.35,167.22,9.03">The SICM is one of the ECP subprojects.</s><s xml:id="_RstVdBu" coords="5,215.76,566.35,224.72,9.03;5,45.95,578.30,282.34,9.03">It seeks to deliver a simple and unified interface to the emerging complex memory hierarchies on exascale nodes <ref type="bibr" coords="5,285.74,578.30,10.39,9.03" target="#b3">[4,</ref><ref type="bibr" coords="5,298.97,578.30,11.46,9.03" target="#b40">41,</ref><ref type="bibr" coords="5,313.27,578.30,11.26,9.03" target="#b59">61]</ref>.</s><s xml:id="_eDCJUcb" coords="5,331.12,578.30,109.35,9.03;5,45.95,590.26,226.60,9.03">To achieve this goal, SICM employs two separate interfaces, as shown in Figure <ref type="figure" coords="5,265.72,590.26,3.41,9.03">1</ref>.</s><s xml:id="_S7n7zqK" coords="5,275.66,590.26,164.81,9.03;5,45.95,602.21,394.54,9.03;5,45.95,614.17,141.51,9.03">The high-level interface delivers an API that allows applications to allocate, migrate, and persist their data without detailed knowledge of the underlying memory hardware.</s><s xml:id="_aKmp7GZ" coords="5,190.39,614.17,250.10,9.03;5,45.95,626.13,396.06,9.03">To implement these operations efficiently, the high-level API invokes the low-level interface, which interacts directly with device-specific services in the OS.</s><s xml:id="_5zPvTAf" coords="5,45.95,638.08,394.54,9.03;5,45.95,650.04,383.98,9.03">Our prior work extends both layers of SICM with profiling tools and analysis, as well as new data management algorithms, to enable guided data placement on complex memory platforms <ref type="bibr" coords="5,411.39,650.04,14.83,9.03" target="#b56">[58]</ref>.</s><s xml:id="_mKXYJSM" coords="6,45.77,221.14,213.78,8.07">Fig. <ref type="figure" coords="6,61.33,221.14,3.07,8.07">2</ref>. Data tiering with offline application guidance <ref type="bibr" coords="6,242.76,221.14,13.44,8.07" target="#b58">[60]</ref>.</s><s xml:id="_GDRMe7V" coords="6,261.80,221.14,179.99,8.07;6,45.77,232.10,396.01,8.07;6,45.77,243.06,395.51,8.07;6,45.77,254.02,311.17,8.07">(a) Compile executable with source code annotations at each allocation site, (b) profile memory usage of each site in a separate program run using architectural sampling, (c) employ bin-packing / sorting heuristics to assign data-tier recommendations to each site, and (d) apply data-tiering recommendations during subsequent program executions.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2" xml:id="_XUYzKty">MemBrain: Automated Application Guidance for Hybrid Memory Systems</head><p xml:id="_frQmRWG"><s xml:id="_hcnV7wJ" coords="6,45.48,290.48,395.10,9.03;6,45.77,302.43,396.05,9.03">To automate the conversion of program profiles to tier recommendations for different memory regions, this work adopts a similar strategy as our previous offline approach called MemBrain <ref type="bibr" coords="6,423.29,302.43,14.82,9.03" target="#b58">[60]</ref>.</s><s xml:id="_PNSbcVC" coords="6,45.77,314.39,394.53,9.03;6,45.77,326.30,228.27,9.08">MemBrain generates data-tier guidance by associating profiles of memory behavior (such as bandwidth and capacity) with program allocation sites.</s><s xml:id="_22TJmUm" coords="6,277.65,326.35,162.65,9.03;6,45.77,338.30,394.50,9.03;6,45.77,350.25,367.65,9.03">Each allocation site corresponds to the source code file name and line number of an instruction that allocates program data (e.g., malloc or new) and may optionally include part or all of the call path leading up to the instruction.</s><s xml:id="_EaabZ76" coords="6,415.80,350.25,26.19,9.03;6,45.77,362.21,394.54,9.03;6,45.77,374.16,41.81,9.03">A separate analysis pass converts the profiles into tier recommendations for each site prior to guided execution.</s><s xml:id="_YFYTf5Z" coords="6,90.06,374.16,192.67,9.03">Figure <ref type="figure" coords="6,118.44,374.16,4.63,9.03">2</ref> presents an overview of this approach.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1" xml:id="_r42yU9S">Converting Site</head><p xml:id="_tkjNwYE"><s xml:id="_ZJ9w677" coords="6,147.73,395.14,137.00,8.98">Profiles to Tier Recommendations.</s><s xml:id="_5r8g8VC" coords="6,288.22,395.09,152.26,9.03;6,45.77,407.04,346.34,9.03">MemBrain includes three options for converting memory usage profiles into tier recommendations for each allocation site.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_mtYZuAP">Knapsack:</head><p xml:id="_9X437EJ"><s xml:id="_emypRuU" coords="6,99.70,427.96,340.58,9.03;6,45.77,439.93,314.43,9.03">The knapsack approach views the task of assigning application data into different device tiers as an instance of the classical 0/1 knapsack optimization problem.</s><s xml:id="_w9pQJBD" coords="6,362.57,439.93,78.82,9.03;6,45.77,451.88,347.51,9.03">In this formulation, each allocation site is an item with a certain value (bandwidth) and weight (capacity).</s><s xml:id="_D5c8a3A" coords="6,395.77,451.88,44.53,9.03;6,45.77,463.84,394.54,9.03;6,45.77,475.79,396.09,9.03">The goal is to fill a knapsack such that the total capacity of the items does not exceed some threshold (chosen as the size of the upper tier), while also maximizing the aggregate bandwidth of the selected items.</s></p><p xml:id="_vdxPbEY"><s xml:id="_DD4jNqe" coords="6,55.74,496.67,384.54,9.08;6,45.77,508.67,336.42,9.03">Hotset: The hotset approach aims to avoid a weakness of knapsack, namely, that it may exclude a site on the basis of its capacity alone, even when that site exhibits high bandwidth.</s><s xml:id="_yASynBN" coords="6,384.20,508.67,56.33,9.03;6,45.77,520.63,394.53,9.03;6,45.77,532.58,83.27,9.03">Hotset simply sorts sites by their bandwidth per unit capacity and selects sites until their aggregate size exceeds a soft capacity limit.</s><s xml:id="_95fJegv" coords="6,131.90,532.58,308.38,9.03;6,45.77,544.53,204.04,9.03">For example, if the capacity of the upper tier is C, then hotset stops adding the sorted sites after the total weight is just past C.</s><s xml:id="_wyDe9gY" coords="6,252.04,544.53,188.25,9.03;6,45.77,556.49,302.02,9.03">By comparison, knapsack will select allocation sites to maximize their aggregate value within a weight upper bound of C.</s></p><p xml:id="_DfHtwKD"><s xml:id="_EY7mdMf" coords="6,55.74,577.37,386.24,9.08;6,45.40,589.37,339.67,9.03">Thermos: Since hotset (intentionally) over-prescribes capacity in the upper tier, cold or lukewarm data could potentially end up crowding out hotter objects during execution.</s><s xml:id="_PdWdFSw" coords="6,388.08,589.37,52.22,9.03;6,45.77,601.32,212.19,9.03">The thermos approach aims to address this occasional drawback.</s><s xml:id="_Eg8ztav" coords="6,260.89,601.32,179.39,9.03;6,45.77,613.28,395.13,9.03;6,45.77,625.24,64.77,9.03">It only assigns a site to the upper tier if the bandwidth (value) the site contributes is greater than the aggregate value of the hottest site(s) it may displace.</s><s xml:id="_fXvrezA" coords="6,113.82,625.24,326.49,9.03;6,45.77,637.19,394.54,9.03;6,45.77,649.15,35.81,9.03">In this way, thermos avoids crowding out performance-critical data, while still allowing large-capacity, high-bandwidth sites to place a portion of their data in the upper-level memory.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4" xml:id="_222V326">ONLINE APPLICATION GUIDANCE FOR HETEROGENEOUS MEMORY SYSTEMS</head><p xml:id="_64Vbnt5"><s xml:id="_jSkD5Ek" coords="7,45.95,97.72,396.20,9.03;7,45.95,109.67,394.71,9.03;7,45.95,121.62,193.15,9.03">Our earlier approaches for guiding data tiering are limited, because they require a separate, profiled execution of each application (with representative input) and only generate static data-tier recommendations for subsequent program runs.</s><s xml:id="_Mrjakj8" coords="7,240.92,121.62,199.56,9.03;7,45.95,133.58,344.71,9.03">This work addresses these limitations by adapting MemBrain for use as an online and fully automated feedback-directed optimization.</s><s xml:id="_S4aWZEA" coords="7,393.59,133.58,47.96,9.03;7,45.95,145.54,396.21,9.03;7,45.95,157.50,394.55,9.03;7,45.95,169.45,245.53,9.03">Specifically, our updated approach monitors application memory behavior, converts this information into datatier recommendations and enforces these recommendations to distribute data efficiently across the memory hierarchy, all within a single run of the application.</s></p><p xml:id="_87M6Xb9"><s xml:id="_XngV3hD" coords="7,55.91,181.40,386.27,9.03;7,45.58,193.36,394.90,9.03;7,45.94,205.31,395.65,9.03;7,45.94,217.27,394.51,9.03;7,45.94,229.23,373.57,9.03">Realizing this vision required two major extensions to our existing SICM+MemBrain framework: (1) updates to the profiling infrastructure, including new arena allocation schemes and OS instrumentation, to increase the efficiency of collecting and organizing memory usage information, and (2) a new online decision engine that analyzes the profiles of all active memory regions and decides when and how to migrate application data across the available memory hardware.</s><s xml:id="_M3nCjWg" coords="7,422.59,229.23,17.90,9.03;7,45.94,241.18,320.02,9.03">This section presents design and implementation details for these new components.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1" xml:id="_TQbjXXg">Effective Memory Usage Monitoring with Low Overhead</head><p xml:id="_8Sr2pjr"><s xml:id="_sgyd4pj" coords="7,45.65,277.04,394.84,9.03;7,45.94,289.00,199.40,9.03">The earlier MemBrain approach attempts to provide memory tier recommendations for the data associated with each program allocation context.</s><s xml:id="_wraPydT" coords="7,247.96,289.00,192.70,9.03;7,45.94,300.96,394.52,9.03;7,45.65,312.91,222.49,9.03">To do so, it requires two bits of information for each allocation context: (1) the cumulative resident set size (RSS) of the data it allocates and (2) the usage rate of its data relative to other contexts.</s><s xml:id="_Q9yvfvc" coords="7,270.95,312.91,169.53,9.03;7,45.94,324.86,394.54,9.03;7,45.94,336.77,211.36,9.08">To collect this information, it employs an offline profile run where each allocation context is associated with a distinct page-aligned region of virtual addresses, known collectively as an arena.</s><s xml:id="_sm85WEH" coords="7,259.85,336.82,180.63,9.03;7,45.94,348.77,270.30,9.03">During the profile run, each new data object is allocated to an arena that is unique to its own allocation context.</s><s xml:id="_SAEKSN4" coords="7,318.53,348.77,121.93,9.03;7,45.94,360.74,393.77,9.03">This approach ensures objects from different allocation contexts do not share the same page, which facilitates profile collection.</s></p><p xml:id="_SqqrUaG"><s xml:id="_3tWP3qX" coords="7,55.91,372.69,386.26,9.03;7,45.94,384.64,232.77,9.03">To estimate the relative access rate of the data in each arena, our profiling tools employ architectural features commonly available in modern processors.</s><s xml:id="_texVeTx" coords="7,281.51,384.64,159.20,9.03;7,45.94,396.60,363.53,9.03">Specifically, the profiler uses the Linux perf facility <ref type="bibr" coords="7,97.21,396.60,16.35,9.03" target="#b62">[64]</ref> to sample the addresses of data accesses that miss the last level cache.</s><s xml:id="_snPSfwB" coords="7,412.74,396.60,27.75,9.03;7,45.94,408.55,394.54,9.03;7,45.94,420.51,125.42,9.03">It then maps each sampled address to its corresponding arena and maintains a count of the number of accesses to data in each arena.</s><s xml:id="_bxPtSU6" coords="7,174.33,420.51,266.13,9.03;7,45.94,432.47,230.83,9.03">In this way, the counts comprise a heatmap of the relative usage of each allocation context at the end of the profile run.</s><s xml:id="_EyEGwbT" coords="7,280.19,432.47,160.29,9.03;7,45.94,444.42,394.55,9.03;7,45.94,456.37,138.83,9.03">Additionally, the profiler estimates the maximum resident set size of each allocation context by keeping track of the number of physical pages associated with each arena.</s><s xml:id="_JxdT8fT" coords="7,187.78,456.37,252.71,9.03;7,45.94,468.33,157.58,9.03">For this work, we have updated this mechanism to reduce its overhead, as described in Section 4.1.2.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1" xml:id="_gUkKhJ8">Hybrid Arena Allocation to Reduce</head><p xml:id="_9H64n6c"><s xml:id="_ySXXkKm" coords="7,223.94,486.31,32.98,8.98">Locking.</s><s xml:id="_ncEpbwz" coords="7,260.41,486.26,181.74,9.03;7,45.94,498.22,394.53,9.03;7,45.94,510.17,396.30,9.03;7,45.94,522.13,71.07,9.03">While our earlier approach is suitable for offline profiling, it can incur significant execution time overheads (more than 2×, in some cases) and often takes too long to build effective guidance for usage in an online feedback-directed optimization (FDO).</s><s xml:id="_CbzCyv3" coords="7,119.51,522.13,320.95,9.03;7,45.94,534.09,266.97,9.03">On further investigation, we found that most of the execution time overhead is due to thread contention during allocation to shared arena spaces.</s><s xml:id="_r83FmnU" coords="7,315.19,534.09,126.97,9.03;7,45.94,546.04,394.52,9.03;7,45.94,557.99,85.22,9.03">In our original profiling configuration, all application threads that allocate from the same program context use the same arena, as shown in Figure <ref type="figure" coords="7,113.96,557.99,13.76,9.03">3(a)</ref>.</s><s xml:id="_6Wp8Ykd" coords="7,133.49,557.99,308.10,9.03;7,45.94,569.95,396.21,9.03;7,45.94,581.90,59.36,9.03">If two or more threads try to allocate from the same context simultaneously, then one thread will acquire a lock and force the other threads to wait while it completes its allocation request.</s><s xml:id="_agwtgac" coords="7,107.43,581.90,333.04,9.03;7,45.94,593.86,396.08,9.03">While such locking can degrade the performance of the profile run, the slowdowns can be avoided during subsequent guided executions by using a different arena allocation strategy.</s><s xml:id="_pAE8X3S" coords="7,45.94,605.82,394.52,9.03;7,45.94,617.77,394.54,9.03;7,45.94,629.72,56.77,9.03">In our original offline approach, the guided run creates a unique set of arenas for every program thread (i.e., one arena for each memory hardware tier) to obviate the need for locking, as shown in Figure <ref type="figure" coords="7,84.91,629.72,14.24,9.03">3(b)</ref>.</s></p><p xml:id="_ybqZkbE"><s xml:id="_qAwXjan" coords="7,55.91,641.68,386.26,9.03;7,45.94,653.63,142.35,9.03">However, this strategy is not feasible for an entirely online approach where profiling is performed alongside guided execution.</s><s xml:id="_7ZegpJU" coords="7,189.97,653.63,250.52,9.03;8,45.77,262.14,308.15,8.08">Moreover, the naïve approach of creating a unique set of arenas Fig. <ref type="figure" coords="8,61.25,262.14,3.07,8.07">3</ref>. Arena allocation strategies for the offline and online data tiering approaches.</s><s xml:id="_EqaBqxV" coords="8,356.10,262.14,84.18,8.07;8,45.77,273.01,334.69,8.17">The dashed and dotted lines show how the nth program thread allocates data from each allocation instruction.</s><s xml:id="_RV5nrm9" coords="8,383.82,273.10,56.47,8.07;8,45.77,284.06,395.52,8.07;8,45.77,295.02,315.49,8.07">In (b), the first and second allocation instructions always use an arena backed by physical memory in the faster DRAM tier, because prior profiling indicates the data created at these sites are accessed frequently.</s><s xml:id="_FFQfE3u" coords="8,363.05,295.02,77.24,8.07;8,45.77,305.98,394.51,8.07;8,45.77,316.94,79.33,8.07">In (c), each allocation instruction will use a thread-exclusive arena until the total bytes allocated by the instruction exceeds a predefined threshold.</s><s xml:id="_qNsfUVV" coords="8,127.75,316.94,312.54,8.07;8,45.77,327.90,345.35,8.07">After this point, the instruction will use a shared arena, which may be remapped to different memory tiers over time, depending on the current profile and tier recommendations.</s></p><p xml:id="_cRY6gB3"><s xml:id="_Zu4p8bZ" coords="8,45.77,347.61,396.20,9.03;8,45.77,359.57,394.73,9.03;8,45.77,371.53,134.43,9.03">for every allocation context for every thread is also not sufficient, because many of the applications that run on complex memory hierarchies employ dozens of threads and reach hundreds or thousands of allocation contexts.</s><s xml:id="_VRtdsMR" coords="8,183.01,371.53,257.28,9.03;8,45.77,383.48,259.16,9.03">Hence, creating thousands of unique arenas for every program thread fragments the address space and reduces spatial locality.</s><s xml:id="_c6quB3S" coords="8,307.64,383.48,132.66,9.03;8,45.77,395.44,394.54,9.03;8,45.77,407.39,57.50,9.03">It also slows down operations to aggregate and analyze memory usage profiles of each arena, which can also reduce the efficacy of this approach.</s></p><p xml:id="_uYNekBm"><s xml:id="_HeZAjYv" coords="8,55.74,419.29,386.25,9.08;8,45.77,431.30,301.14,9.03">To address these issues, we developed a hybrid arena allocation scheme that aims to enable profiling of most application data without the need for locking in most cases.</s><s xml:id="_b97ePcy" coords="8,349.52,431.30,90.77,9.03;8,45.77,443.26,396.20,9.03;8,45.77,455.21,395.62,9.03;8,45.77,467.17,395.66,9.03;8,45.77,479.12,164.73,9.03">Our approach exploits the observations that (1) most of the lock contention during profiling arises due to frequent allocations of very small data objects and (2) even if they are cold or their usage patterns unknown, such allocations can often be assigned to the smaller, faster tier(s) of memory with little penalty, since they do not require much capacity.</s></p><p xml:id="_KsfQPVJ"><s xml:id="_y5FmdRs" coords="8,55.74,491.08,204.00,9.03">Figure <ref type="figure" coords="8,84.31,491.08,3.67,9.03">3</ref>(c) presents our hybrid allocation scheme.</s><s xml:id="_nsQTDwC" coords="8,262.44,491.08,177.86,9.03;8,45.77,503.04,394.51,9.03;8,45.77,514.99,394.53,9.03;8,45.77,526.95,185.23,9.03">The allocator for the hybrid scheme creates two sets of arenas: one set of thread private arenas, each of which may contain data created from any allocation context, and another set of arenas shared among all program threads, each of which corresponds to exactly one allocation context.</s><s xml:id="_2banKCF" coords="8,233.24,526.95,208.76,9.03;8,45.52,538.90,214.60,9.03">By default, all program data are allocated to the private arena corresponding to the thread that created it.</s><s xml:id="_B6PhJdf" coords="8,262.00,538.90,178.33,9.03;8,45.77,550.85,256.48,9.03">However, the runtime also keeps track of the cumulative size of the data allocated at each allocation context.</s><s xml:id="_raW5z7j" coords="8,304.75,550.85,135.55,9.03;8,45.77,562.81,395.62,9.03;8,45.77,574.77,391.51,9.03">When the number of active bytes corresponding to a particular context exceeds a predefined and configurable threshold (say, 4 MB), new data created from that context are allocated to the shared arena designated for that context.</s></p><p xml:id="_xWmz5bs"><s xml:id="_5Rcyd5S" coords="8,55.73,586.72,386.26,9.03;8,45.77,598.68,194.61,9.03">In this way, frequent allocations from contexts with smaller capacity requirements can complete without needing to lock a shared resource.</s><s xml:id="_pYcmcf7" coords="8,242.74,598.68,197.55,9.03;8,45.77,610.63,394.53,9.03;8,45.77,622.58,184.78,9.03">Additionally, by choosing an appropriately small threshold, the private arenas will never require much physical capacity and can always be assigned to the smaller, faster tier(s) with little penalty.</s><s xml:id="_6DhKexY" coords="8,232.86,622.58,207.71,9.03;8,45.77,634.54,384.00,9.03">Hence, the online profiler does not attempt to track the origin of data in the thread private arenas and only profiles the usage of the shared arenas.</s></p><p xml:id="_nzXdYtw"><s xml:id="_aa2ZWRS" coords="9,45.95,55.82,271.61,8.97;9,424.39,55.82,16.09,8.97;9,55.91,82.82,17.65,8.98">Online Application Guidance for Heterogeneous Memory Systems 45:9 4.1.2</s><s xml:id="_s2AR8bH" coords="9,83.53,82.82,242.29,8.98">System-level Integration for More Effective Capacity Profiling.</s><s xml:id="_NZCwSVg" coords="9,329.30,82.77,112.87,9.03;9,45.95,94.73,394.54,9.03;9,45.95,106.68,394.53,9.03;9,45.95,118.63,36.31,9.03">Another challenge in adapting the SICM+MemBrain approach for use as an online FDO is that the approach it uses to measure the capacity requirements of each arena can incur significant overheads and is often too slow to be effective.</s><s xml:id="_3zmesNg" coords="9,85.03,118.63,355.70,9.03;9,45.95,130.60,352.72,9.03">Specifically, our previous approach employed a separate runtime thread to periodically count up the number of resident physical pages using the Linux pagemap facility <ref type="bibr" coords="9,380.13,130.60,14.83,9.03" target="#b60">[62]</ref>.</s><s xml:id="_nN2YtSV" coords="9,401.33,130.60,39.14,9.03;9,45.95,142.55,394.52,9.03;9,45.95,154.50,394.53,9.03;9,45.95,166.46,394.79,9.03;9,45.95,178.41,319.75,9.03">There are two main drawbacks to using this approach in an online framework: (1) to prevent the application from modifying addresses as they are read, the profiling thread has to lock each arena as it walks over the heap, and (2) it can be very slow for large applications, because it requires numerous seek and read system calls to collect information about each and every virtual page.</s></p><p xml:id="_huK6xhp"><s xml:id="_EQsxkH4" coords="9,55.91,190.37,384.56,9.03;9,45.95,202.33,396.22,9.03;9,45.95,214.28,73.69,9.03">For this work, we developed an alternative strategy that leverages existing data structures and deeper integration with the Linux kernel to enable fast and effective capacity profiling for largescale applications.</s><s xml:id="_nJpVTaj" coords="9,122.12,214.28,318.35,9.03;9,45.95,226.24,394.53,9.03;9,45.58,238.19,217.45,9.03">Linux organizes the virtual address space of each process into a set of Virtual Memory Areas (VMAs), where each VMA is comprised of a contiguous range of virtual addresses with similar access permissions and other properties.</s><s xml:id="_QfdxxuY" coords="9,265.73,238.19,174.74,9.03;9,45.95,250.14,394.55,9.03;9,45.95,262.11,353.72,9.03">The metadata for each region are kept in a structure called the vm_area_struct, and information regarding each VMA, such as its address range, backing file, and permissions, can be read by applications via the proc interface.</s></p><p xml:id="_BGqRmp6"><s xml:id="_aBAv7Nq" coords="9,55.91,274.06,384.56,9.03;9,45.94,286.01,394.52,9.03;9,45.94,297.97,149.34,9.03">For this enhancement, we extended Linux's proc interface with facilities for applications to create a new VMA for a given virtual address range, provided that the given range is already part of the process's virtual address space.</s><s xml:id="_7Pq5823" coords="9,197.17,297.97,243.31,9.03;9,45.94,309.92,394.54,9.03;9,45.94,321.87,116.30,9.03;9,162.25,320.05,3.38,6.59;9,168.39,321.87,272.43,9.03;9,45.61,333.83,394.85,9.03;9,45.95,345.79,37.87,9.03">Additionally, we added instrumentation in the page fault and page release paths of the Linux memory manager to track the number of resident physical pages corresponding to each VMA. <ref type="foot" coords="9,162.25,320.05,3.38,6.59" target="#foot_3">2</ref> To track the RSS of each arena, the application runtime creates new VMAs for each contiguous range of addresses within each arena by writing to the custom proc interface.</s><s xml:id="_BK2dTqE" coords="9,85.64,345.79,354.84,9.03;9,45.95,357.74,346.50,9.03">The online profiling thread then reads from this same proc interface to collect up-to-date counts of the number of resident pages for each VMA (and by extension, each arena).</s></p><p xml:id="_5tb3wgd"><s xml:id="_rrTbvFM" coords="9,55.91,369.70,384.57,9.03;9,45.95,381.65,394.55,9.03;9,45.95,393.60,394.54,9.03;9,45.95,405.56,395.63,9.03;9,45.95,417.52,30.53,9.03;9,76.48,415.68,3.38,6.59">While this VMA-based implementation is sufficient to demonstrate the benefits of increased coordination between system-and user-level profiling in this work, it has two limitations that could restrict its adoption by the wider community: (1) It requires modifications to the standard Linux kernel and (2) Linux imposes a hard limit on the total number of VMAs for each process (i.e., 65,536). <ref type="foot" coords="9,76.48,415.68,3.38,6.59" target="#foot_4">3</ref></s><s xml:id="_WGsHJpj" coords="9,82.12,417.52,360.05,9.03;9,45.94,429.48,84.11,9.03">Fortunately, some recent Linux features make it possible to implement this approach without either limitation.</s><s xml:id="_xMGhwXU" coords="9,132.44,429.48,308.04,9.03;9,45.94,441.43,394.50,9.03;9,45.94,453.38,312.20,9.03">Specifically, the extended Berkeley Packet Filter (eBPF), which has been supported in Linux since version 4.1, enables users to write and attach custom instrumentation to a live kernel image, without any risk of crashing or hanging system code <ref type="bibr" coords="9,339.59,453.38,14.84,9.03" target="#b21">[22]</ref>.</s><s xml:id="_guUQNGB" coords="9,360.23,453.38,80.24,9.03;9,45.94,465.34,396.20,9.03;9,45.94,477.29,30.19,9.03">In the next iteration of this work, we plan to develop an eBPF tool that addresses these limitations of our current approach.</s><s xml:id="_K3eCNyR" coords="9,78.10,477.29,362.38,9.03;9,45.94,489.25,379.52,9.03">The tool will create a record of each contiguous range of addresses in each user-level arena and will then use system-level instrumentation to track the resident pages within each range.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2" xml:id="_hQPrKKc">Deciding When and How to Migrate Application Data</head><p xml:id="_gYMb8pd"><s xml:id="_SC95c6H" coords="9,45.59,526.05,394.86,9.03;9,45.94,538.00,145.22,9.03">An important component of any online FDO is how it decides if and when to expend computing resources on program optimization.</s><s xml:id="_GZEFdM9" coords="9,193.66,538.00,246.84,9.03;9,45.94,549.95,217.25,9.03">Optimizing too early can lead to poor optimization decisions due to inaccurate or incomplete profile information.</s><s xml:id="_fsDQGMM" coords="9,266.67,549.95,174.10,9.03;9,45.94,561.91,394.73,9.03;9,45.94,573.86,47.26,9.03">Such premature optimization is especially harmful in the context of this work due to the high cost of migrating data across memory tier boundaries.</s><s xml:id="_CeYqZZt" coords="9,95.31,573.86,345.38,9.03;9,45.94,585.83,281.02,9.03">However, optimizing too late is also harmful, because the program will spend a longer portion of its execution time without the benefit of the optimization.</s><s xml:id="_8TwNTMA" coords="9,329.68,585.83,112.48,9.03;9,45.94,597.78,394.56,9.03;9,45.94,609.73,262.96,9.03">Previous works that use offline profiling or static tier recommendations avoid this dilemma, because the information needed to optimize is readily available at the start of program execution.</s></p><p xml:id="_DwWXrfV"><s xml:id="_VBp6WFF" coords="10,55.74,82.77,384.56,9.03;10,45.77,94.73,261.65,9.03">To construct an online FDO for data tiering, we express the problem of choosing when to migrate application data as an instance of the classical ski rental problem.</s><s xml:id="_RbjkuKv" coords="10,309.52,94.73,130.77,9.03;10,45.77,106.68,395.63,9.03;10,45.77,118.63,395.64,9.03;10,45.77,130.60,92.60,9.03">The ski rental problem describes a class of optimization problems where, at every time step, one must pay a repeating cost (i.e., renting a pair of skis) or pay a larger one-time cost to reduce or eliminate the repeating cost (i.e., buying a pair of skis).</s><s xml:id="_KBP97Xc" coords="10,142.03,130.60,298.28,9.03;10,45.77,142.55,396.05,9.03">This formulation has been used to solve online problems in a range of domains including just-in-time compilation <ref type="bibr" coords="10,226.20,142.55,10.44,9.03" target="#b7">[8]</ref>, cache coherence <ref type="bibr" coords="10,312.60,142.55,14.83,9.03" target="#b33">[34]</ref>, and cloud computing <ref type="bibr" coords="10,423.28,142.55,14.83,9.03" target="#b34">[35]</ref>.</s><s xml:id="_wGvPc8t" coords="10,45.77,154.50,394.50,9.03;10,45.77,166.46,394.78,9.03;10,45.77,178.41,320.23,9.03">For this work, we view the problem of whether to move application data across tiers as a choice between continuing to pay the repeating cost of keeping relatively warm data in a slow memory tier and paying the larger cost of remapping application data to a different tier.</s></p><p xml:id="_kUCGgA4"><s xml:id="_b7cDPVG" coords="10,55.74,190.37,386.26,9.03;10,45.77,202.33,190.05,9.03">Our solution follows the break-even algorithm, which is known to be the best deterministic algorithm for solving the ski rental problem <ref type="bibr" coords="10,217.27,202.33,14.84,9.03" target="#b47">[48]</ref>.</s><s xml:id="_jYmyBmv" coords="10,238.00,202.33,203.84,9.03">Algorithm 1 presents pseudocode of our approach.</s><s xml:id="_gQMx5FQ" coords="10,45.42,214.28,394.87,9.03;10,45.77,226.24,322.40,9.03">As the application executes, a separate runtime thread counts the total number of memory access samples and number of pages resident on each memory tier in each virtual arena.</s><s xml:id="_cAVAb7f" coords="10,369.99,226.24,70.32,9.03;10,45.77,238.19,394.51,9.03;10,45.77,250.14,108.12,9.03">The runtime then examines this information at regular intervals to determine if and how it should move any data to a different memory tier.</s><s xml:id="_sgPMFtv" coords="10,156.63,250.09,283.66,9.08;10,45.77,262.11,396.20,9.03;10,45.77,274.06,299.53,9.03">For this operation, it first estimates the optimal data-tier assignments for every arena and allocation site using one of the three MemBrain strategies (i.e., knapsack, hotset, or thermos) with the current memory profile (Algorithm 1, line <ref type="bibr" coords="10,330.87,274.06,7.21,9.03" target="#b18">19</ref>).</s><s xml:id="_CphkNMJ" coords="10,348.67,274.06,91.61,9.03;10,45.77,286.01,394.73,9.03;10,45.77,297.97,394.51,9.03;10,45.77,309.92,135.83,9.03">Next, it computes and compares two costs: (1) the rental cost, which is the expected cost of keeping the current data-tier assignments, and (2) the purchase cost, which is the cost of migrating application data to match the MemBrain recommendations.</s></p><p xml:id="_9k6dAez"><s xml:id="_jtjDSDg" coords="10,55.73,321.87,386.24,9.03;10,45.77,333.84,394.53,9.03;10,45.77,345.79,394.72,9.03;10,45.77,357.74,396.09,9.03">To compute the rental cost, our approach calculates (a) the number of data reads that are resolved in the slower memory tier, but which would have been resolved on the faster memory if the optimal data-tier assignments were enforced, as well as (b) the number of reads resolved in faster memory that would have been resolved in slower memory with the recommended data placement.</s><s xml:id="_Vmu5ZKE" coords="10,45.47,369.70,394.82,9.03;10,45.77,381.65,117.80,9.03">The runtime can estimate these values online by scaling the relevant sample counts in the current profile by the sample period.</s><s xml:id="_KbHw4Tn" coords="10,166.38,381.65,273.90,9.03;10,45.77,393.61,273.13,9.03">If (a) is greater than (b), then the application is currently paying a repeating cost to keep its data in a suboptimal tiering configuration.</s><s xml:id="_CjRVwWa" coords="10,321.05,393.61,119.26,9.03;10,45.77,405.29,394.53,9.38;10,45.77,417.52,256.63,9.03">To calculate the magnitude of this repeating cost, our approach multiplies (ab) by the average additional execution time cost of each read from the slower memory tier (Algorithm 1, line 12).</s><s xml:id="_gRrtHZG" coords="10,304.49,417.52,135.80,9.03;10,45.77,429.48,394.74,9.03;10,45.77,441.43,107.18,9.03">For example, on our experimental platform, the average read latency of the Optane DC tier is about 300 ns longer than the DDR4 SDRAM memory tier <ref type="bibr" coords="10,134.39,441.43,14.84,9.03" target="#b28">[29]</ref>.</s><s xml:id="_vsHvmrk" coords="10,155.44,441.15,213.93,9.38">Thus, the rental cost is calculated as (ab) * 300 ns.</s></p><p xml:id="_qGrgc5W"><s xml:id="_GQgqQcu" coords="10,55.73,453.38,384.56,9.03;10,45.77,465.35,396.21,9.03;10,45.77,477.30,42.58,9.03">The application can remove this repeating cost for subsequent program intervals by paying the one-time purchase cost of migrating data between tiers to match the (expected) optimal configuration.</s><s xml:id="_Dytbc7b" coords="10,91.38,477.30,348.91,9.03;10,45.40,489.25,395.17,9.03;10,45.77,501.21,394.51,9.03;10,45.55,513.16,14.43,9.03">To estimate this purchase cost, the runtime computes the number of pages of data it would need to move to enforce the optimal tier recommendations and multiplies this value by the average rate with which the platform is able to migrate data between tiers (Algorithm 1, line 13).</s><s xml:id="_XENxP8j" coords="10,62.58,513.16,377.67,9.03;10,45.77,525.07,218.82,9.08">On our Linux-based platform, we found that moving data between tiers (via the move_pages system call) requires about 2 μs for each 4-KB page.</s><s xml:id="_qd5ap8y" coords="10,268.09,525.11,172.19,9.03;10,45.92,537.03,394.35,9.08;10,45.77,549.03,37.62,9.03">Thus, we estimate the purchase cost as 2 μs times the total number of pages that would move if the tier recommendations were to be enforced.</s></p><p xml:id="_yA3fEVK"><s xml:id="_A7tza6h" coords="10,55.73,560.98,386.24,9.03;10,45.77,572.94,69.14,9.03">At each decision interval, the runtime simply compares the rental and purchase costs (Algorithm 1, line 21).</s><s xml:id="_gmQvgmJ" coords="10,118.22,572.94,322.10,9.03;10,45.77,584.89,164.25,9.03">If the cumulative rental cost ever exceeds the purchase cost, then the current data-tier recommendations are enforced.</s><s xml:id="_QHvYmKt" coords="10,212.18,584.89,228.31,9.03;10,45.77,596.85,394.53,9.03;10,45.77,608.81,102.55,9.03">Specifically, any arenas that are mapped to the faster tier and that contain relatively cold program data will first be remapped to the slower tier to make space for the hotter data.</s><s xml:id="_vrRCuMb" coords="10,151.10,608.81,289.38,9.03;10,45.77,620.76,181.63,9.03">Next, arenas with relatively warm program data residing in the slower tier will then be remapped to the faster tier.</s><s xml:id="_nKvh4rW" coords="10,230.45,620.76,209.85,9.03;10,45.77,632.72,326.96,9.03">Additionally, the runtime updates a side table with the current site-tier assignments to ensure accurate bookkeeping going forward.</s></p><p xml:id="_bNdNwNP"><s xml:id="_ERYRK84" coords="11,45.95,55.82,271.61,8.97;11,419.75,55.82,20.72,8.97;11,45.58,82.48,189.73,9.03">Online Application Guidance for Heterogeneous Memory Systems 45:11 ALGORITHM 1: Online Guided Data Tiering.</s><s xml:id="_S32Hu4b" coords="11,237.56,82.48,204.61,9.03;11,45.95,94.43,375.78,9.03">Our approach expresses the problem of data placement in a heterogeneous memory system as an instance of the classical ski rental problem.</s><s xml:id="_tx2rERj" coords="11,424.73,94.43,15.76,9.03;11,45.95,106.39,394.77,9.03;11,45.95,118.35,396.09,9.03">The solution below employs a break-even algorithm to decide when to migrate data between memory tiers to match a set of "optimal" data-tier recommendations based on recent profile information.</s><s xml:id="_7nHma7V" coords="11,45.65,130.25,394.82,9.08;11,45.94,142.25,396.21,9.03;11,45.94,154.21,32.63,9.03">The constant EXT RA_NS_PER_SLOW ER_ACCESS is roughly equal to the extra execution time cost (latency) of each data access to the slower memory tier (in ns), as compared to the faster memory tier.</s><s xml:id="_swvwKwk" coords="11,81.62,154.16,358.85,9.08;11,45.94,166.16,312.05,9.03">NS_PER_PAGE_MOV ED is also a constant, and is equal to the average execution time cost (in ns) of remapping a single page from one tier of memory to the other.</s><s xml:id="_8NWaBTk" coords="11,77.44,403.70,218.38,9.38;11,92.77,513.92,75.72,9.03;11,212.12,513.92,228.34,9.03;11,55.91,554.85,384.59,9.03;11,45.95,566.80,394.77,9.03;11,45.95,578.76,37.37,9.03">rentalCost, purchaseCost ← GetSkiCosts (pro f , recs); ReweightProfile (); optionally reweight to "forget" older profile information Before completing the interval, the runtime may optionally reset or re-weight the profile information to gradually forget older intervals and enable faster adaptation to new memory behavior.</s><s xml:id="_wGP6Nap" coords="11,88.09,578.76,354.08,9.03;11,45.58,590.71,39.77,9.03">However, in our current implementation, profile information is never reset or reweighted.</s><s xml:id="_RxfqcNE" coords="11,89.94,590.71,350.55,9.03;11,45.95,602.66,394.52,9.03;11,45.95,614.63,24.42,9.03">Memory access samples always accumulate over time, and capacity estimates are updated instantaneously as the application maps and unmaps physical memory in its address space.</s><s xml:id="_aw9a5NB" coords="11,73.33,614.63,367.40,9.03;11,45.95,626.58,394.73,9.03;11,45.95,638.53,41.70,9.03">We have found that this configuration tends to work well for applications with relatively stable memory usage patterns, including most of the memory-intensive applications we used for this study.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3" xml:id="_BGvvzZv">Summary of Online Approach</head><p xml:id="_6dCc2P4"><s xml:id="_vY5Gb35" coords="12,45.77,331.76,213.81,9.03">Figure <ref type="figure" coords="12,74.48,331.76,4.63,9.03" target="#fig_2">4</ref> shows an overview of our online approach.</s><s xml:id="_hWfSX2j" coords="12,262.39,331.76,178.10,9.03;12,45.77,343.72,394.53,9.03;12,45.77,355.67,131.68,9.03">The online approach still employs compiler analysis to annotate each allocation instruction, and potentially several layers of function call context, with a unique identifier.</s><s xml:id="_vca8Bb4" coords="12,179.73,355.67,260.57,9.03;12,45.77,367.63,167.18,9.03">Once the annotated executable has been built, the profile-guided data tiering process is entirely automatic.</s><s xml:id="_XvmPmbE" coords="12,215.26,367.63,225.04,9.03;12,45.77,379.58,394.56,9.03;12,45.77,391.53,394.53,9.03;12,45.77,403.49,336.54,9.03">During program execution, the custom runtime collects memory usage information, converts it to tier recommendations for the application's allocation sites and existing program data, and enforces these data-tier recommendations, all within the same program run, and without any need for additional input or direction from the user.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5" xml:id="_hvtTXkH">EXPERIMENTAL SETUP 5.1 Platform Details</head><p xml:id="_GJvQeHH"><s xml:id="_9VgNHcB" coords="12,45.77,461.66,394.54,9.03;12,45.77,473.61,394.54,9.03;12,45.77,485.57,80.17,9.03">Our evaluation platform contains a single Intel Xeon Gold 6246R processor (codenamed "Cascade Lake" or CLX) with 16 compute cores, each running with a clock speed of 3.4 GHz, and a shared 35.75 MB L3 cache.</s><s xml:id="_rqxjffa" coords="12,129.39,485.57,310.90,9.03;12,45.77,497.53,375.47,9.03">The processor includes a memory controller that services requests to both DDR4 SDRAM as well as Optane DC persistent memory through a common memory bus.</s><s xml:id="_9BNqVVt" coords="12,424.56,497.53,15.76,9.03;12,45.77,509.48,395.63,9.03;12,45.77,521.44,256.16,9.03">The bus is divided into six identical channels, each of which is connected to one 32 GB, 2933 MT/s, DDR4 DIMM and one 128 GB, 2666 MT/s, Optane DC module.</s><s xml:id="_UsuFpeX" coords="12,304.76,521.44,135.52,9.03;12,45.77,533.39,308.85,9.03">Thus, the system contains a total of 192 GB of DDR4 SDRAM and 768 GB of Optane DC persistent memory.</s><s xml:id="_2CKd72U" coords="12,357.70,533.39,82.60,9.03;12,45.77,545.34,394.53,9.03;12,45.78,557.31,87.96,9.03">Data reads from the persistent memory require 2× to 3× longer latencies and sustain only 30% to 40% of the bandwidth of the DDR4 memory.</s><s xml:id="_cHPa8Ze" coords="12,135.95,557.31,304.37,9.03;12,45.78,569.26,297.89,9.03">While latency for writes is similar for both types of memory, the DDR4 also supports 5× to 10× more write bandwidth than the Optane memory <ref type="bibr" coords="12,325.12,569.26,14.83,9.03" target="#b28">[29]</ref>.</s></p><p xml:id="_wwfuUcJ"><s xml:id="_b97BCKt" coords="12,55.74,581.21,354.50,9.03">We installed Debian 10 with Linux kernel version 5.7.2 as the base operating system.</s><s xml:id="_rmaF36X" coords="12,413.58,581.21,26.71,9.03;12,45.78,593.12,396.20,9.08;12,45.78,605.12,394.52,9.03;12,45.78,617.07,58.47,9.03">For all software-based tiering configurations (i.e., first touch, offline, and online), we used system configuration tools (e.g., daxctl, ndctl, etc.) to assign the DDR4 and Optane device tiers to separate NUMA nodes.</s><s xml:id="_2matXEw" coords="12,107.57,617.07,332.99,9.03;12,45.78,629.04,291.71,9.03">This configuration allows applications and system software to track and modify allocations to each type of memory using the standard NUMA API <ref type="bibr" coords="12,318.94,629.04,14.84,9.03" target="#b37">[38]</ref>.</s></p><p xml:id="_K64g35F"><s xml:id="_QSJESbd" coords="13,45.95,55.82,271.61,8.97;13,419.75,55.82,20.72,8.97;13,49.76,288.71,390.93,7.22;13,49.93,298.67,315.35,7.22">Online Application Guidance for Heterogeneous Memory Systems 45:13 The columns on the right show the # of allocation sites reached during execution, as well as the execution time and peak resident set size (in GB) of each benchmark with the default (unguided first touch) configuration.</s><s xml:id="_F6gp225" coords="13,367.29,298.67,72.37,7.22;13,49.93,308.63,390.08,7.22;13,49.93,318.60,134.98,7.22">The CORAL table also shows the arguments that were used to construct the different inputs for each workload as well as the absolute figure of merit (FoM) for the default configuration.</s><s xml:id="_tcggxhm" coords="13,186.91,318.56,242.88,7.26;13,49.93,328.56,49.06,7.22">All experiments with the SPEC CPU 2017 benchmarks use the standard ref program input.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2" xml:id="_XbZw2kU">Workloads</head><p xml:id="_whV5CbG"><s xml:id="_KVBe96t" coords="13,45.95,372.25,395.62,9.03;13,45.58,384.20,394.89,9.03;13,45.95,396.15,394.72,9.03;13,45.95,408.11,109.43,9.03">Our evaluation employs applications from two popular sets of benchmark programs: CORAL <ref type="bibr" coords="13,423.04,372.25,14.81,9.03" target="#b45">[46]</ref>, which includes several widely used HPC applications and proxy applications, and SPEC CPU 2017 <ref type="bibr" coords="13,66.52,396.15,14.84,9.03" target="#b67">[69]</ref>, which is comprised of a variety of industry standard applications for stressing processor and memory performance.</s><s xml:id="_PRtt4rs" coords="13,158.55,408.11,283.05,9.03;13,45.95,420.06,394.73,9.03;13,45.95,432.02,273.60,9.03">From CORAL, we selected three proxy applications (LULESH, AMG, and SNAP) and one full-scale scientific computing application (QMCPACK) based on their potential to stress cache and memory performance on our platform.</s><s xml:id="_naVT7H6" coords="13,321.83,432.02,118.65,9.03;13,45.95,443.98,394.53,9.03;13,45.95,455.93,263.01,9.03">To study the impact of online tiering guidance with more varied inputs and capacity requirements, we also constructed and evaluated three separate input sizes for each CORAL application.</s><s xml:id="_ebUZdfD" coords="13,311.28,455.93,129.20,9.03;13,45.95,467.88,394.54,9.03;13,45.95,479.84,24.11,9.03">The top part of Table <ref type="table" coords="13,398.73,455.93,4.63,9.03" target="#tab_1">1</ref> provides descriptions and relevant usage statistics for each CORAL application-input pair included in this study.</s></p><p xml:id="_ZK4av5N"><s xml:id="_DtKRaxg" coords="13,55.91,491.79,386.24,9.03;13,45.95,503.75,394.77,9.03;13,45.95,515.71,80.80,9.03">The benchmarks in SPEC CPU 2017 are designed to test a variety of system behavior and include several single-threaded and CPU-bound applications as well as multi-threaded and memory intensive programs.</s><s xml:id="_CHjFRbK" coords="13,129.52,515.71,310.95,9.03;13,45.64,527.66,395.01,9.03;13,45.95,539.62,244.21,9.03;13,290.14,537.79,3.38,6.59;13,296.00,539.62,146.16,9.03;13,45.95,551.57,394.54,9.03;13,45.95,563.52,282.13,9.03">For this study, we focused our evaluation on multi-threaded floating point (FP) benchmarks that provide the option to distribute their processing over a configurable number of application threads through the use of OpenMP directives. <ref type="foot" coords="13,290.14,537.79,3.38,6.59" target="#foot_6">4</ref> When configured to use larger numbers of software threads, these FP workloads tend to have higher memory bandwidth requirements and thus magnify the importance of data placement on our platform.</s><s xml:id="_dVsWGFJ" coords="13,330.80,563.52,109.90,9.03;13,45.95,575.48,396.06,9.03">The bottom part of Table <ref type="table" coords="13,436.07,563.52,4.63,9.03" target="#tab_1">1</ref> provides descriptions and other relevant usage statistics for our selected SPEC CPU benchmarks.</s><s xml:id="_A4x9Mzf" coords="13,45.60,587.39,337.24,9.08">All of our experiments with these benchmarks use the standard ref program input.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3" xml:id="_ngQdy2s">Common Experimental Configuration</head><p xml:id="_qCQDpkt"><s xml:id="_Zuk6VDz" coords="14,45.42,97.72,294.71,9.03">All applications were compiled using the LLVM compiler toolchain (v.</s><s xml:id="_HGDWtQe" coords="14,343.73,97.72,21.26,9.03">7.1.0)</s><s xml:id="_QTJuxqW" coords="14,368.55,97.72,73.44,9.03;14,45.77,109.67,159.40,9.03">with default optimization settings and -march=x86_64.</s><s xml:id="_ZVJ3W5x" coords="14,208.67,109.67,233.30,9.03;14,45.77,121.62,221.27,9.03">C/C++ codes use the standard clang frontend, and Fortran codes are converted to LLVM IR using Flang <ref type="bibr" coords="14,248.51,121.62,14.83,9.03">[55]</ref>.</s><s xml:id="_FCrjqBw" coords="14,269.60,121.62,170.69,9.03;14,45.78,133.58,220.92,9.03">All guided and non-guided configurations use SICM with the unmodified jemalloc allocator (v.</s><s xml:id="_Pq3jTDa" coords="14,269.81,133.58,171.59,9.03;14,45.78,145.54,295.56,9.03;14,341.33,143.70,3.38,6.59;14,347.46,145.54,92.83,9.03;14,45.78,157.50,396.22,9.03;14,45.78,169.45,108.12,9.03">5.2.0) with oversize_threshold set to 0, background_thread set to true, and max_background_threads set to 1. <ref type="foot" coords="14,341.33,143.70,3.38,6.59" target="#foot_7">5</ref> To prepare executables for guided execution, we configure the compilation pass to clone up to three layers of call path context to each allocation site.</s><s xml:id="_KdnhWwq" coords="14,156.26,169.45,284.04,9.03;14,45.78,181.40,278.05,9.03">Our previous work has shown that this amount of context is sufficient to obtain the benefits of this approach for most applications <ref type="bibr" coords="14,291.31,181.40,15.01,9.03" target="#b18">[19,</ref><ref type="bibr" coords="14,308.83,181.40,11.25,9.03" target="#b58">60]</ref>.</s></p><p xml:id="_JcwrkSD"><s xml:id="_kRMk2Qu" coords="14,55.74,193.36,384.57,9.03;14,45.78,205.31,278.74,9.03">For the default and offline configurations, each benchmark is configured to use 16 software threads to match the number of cores on our experimental platform.</s><s xml:id="_PJj7yDX" coords="14,327.12,205.31,114.88,9.03;14,45.41,217.27,288.98,9.03">The offline configuration always uses the same program input for the profile and evaluation runs.</s><s xml:id="_qH7je5n" coords="14,337.18,217.27,104.22,9.03;14,45.78,229.23,394.53,9.03;14,45.78,241.18,394.53,9.03;14,45.78,253.13,112.86,9.03">The online configuration, as well as the profile run of the offline configuration, creates only 15 software threads for each application, because they require an additional runtime thread to profile and periodically enforce data-tier recommendations.</s><s xml:id="_kWjnRJP" coords="14,161.84,253.13,280.15,9.03;14,45.78,265.09,394.53,9.03;14,45.78,277.05,205.57,9.03">We tested the alternative strategy of over-provisioning compute resources by running 16 application threads alongside this extra thread and leaving it to the system scheduler to resolve conflicts for computing cores.</s><s xml:id="_Ju6tW2b" coords="14,254.04,277.05,187.94,9.03;14,45.78,289.00,396.06,9.03">However, we found that this approach consistently produced much worse performance than the 15-thread configuration with our benchmarks.</s></p><p xml:id="_jfKqncF"><s xml:id="_c8CJuns" coords="14,55.74,300.96,384.57,9.03;14,45.78,312.91,175.03,9.03">To reduce sources of variability between runs, all of our experiments execute each application in isolation on an otherwise idle machine.</s><s xml:id="_c2CqZKd" coords="14,224.08,312.91,216.24,9.03;14,45.78,324.86,390.25,9.03">Prior to each experimental run, an automated script clears out the Linux page cache and disables transparent huge pages for the application process.</s></p><p xml:id="_hHKKM8u"><s xml:id="_mUYvPQQ" coords="14,55.74,336.82,386.24,9.03;14,45.78,348.78,396.07,9.03">To estimate the usage rate of each site, the offline and online profilers use the Linux perf <ref type="bibr" coords="14,412.42,336.82,16.36,9.03" target="#b62">[64]</ref> facility to sample memory reads from the target application that miss the last-level processor caches.</s><s xml:id="_NTVMUak" coords="14,45.78,360.74,394.54,9.03;14,45.53,372.69,50.99,9.03">Specifically, we sample MEM_LOAD_L3_MISS_RETIRED event on our platform with a PEBS reset value of 512.</s><s xml:id="_JSjrKYN" coords="14,98.97,372.69,341.34,9.03;14,45.48,384.64,361.07,9.03">We also compute the resident set size for each site by counting the number of active (4-KB) pages associated with the site's corresponding VMA, as described in Section 4.1.2.</s></p><p xml:id="_A24YXNU"><s xml:id="_Mczx7XN" coords="14,55.74,396.60,386.26,9.03;14,45.78,408.50,395.62,9.08;14,45.78,420.51,254.53,9.03">For the online approach, we experimented with a number of interval lengths for analyzing profile information and migrating program data (i.e., the IntervalTime parameter in Algorithm 1), including: 0.1 seconds, 1 second, 10 seconds, and 100 seconds.</s><s xml:id="_DSMfvqy" coords="14,303.26,420.51,138.72,9.03;14,45.78,432.47,394.72,9.03;14,45.78,444.42,280.34,9.03">We found that relatively short intervals of 1 second or less were more sensitive to shifts in memory usage but also incurred higher overheads due to more frequent interruptions and data migrations.</s><s xml:id="_rVcSWgE" coords="14,329.68,444.42,110.62,9.03;14,45.78,456.37,394.80,9.03;14,45.78,468.33,394.55,9.03;14,45.78,480.29,123.21,9.03">Of the interval lengths we tested, 10 seconds provided the best balance of relatively low migration overheads with relatively quick convergence to a good data-tiering configuration and provided the best overall performance for the applications we tested.</s><s xml:id="_6TXCBhN" coords="14,171.73,480.29,268.58,9.03;14,45.78,492.24,85.04,9.03">Hence, all of our online results in the next section use an interval length of 10 seconds.</s></p><p xml:id="_sAFVTKN"><s xml:id="_aztmKKw" coords="14,55.74,504.20,384.54,9.03;14,45.78,516.15,394.53,9.03;14,45.78,528.10,28.67,9.03">Additionally, we configured the hybrid arena allocator to promote an allocation context to its own thread-shared arena after it allocates more than 4 MB of data (in total) to the thread-private arenas.</s><s xml:id="_uQa6J5A" coords="14,78.08,528.10,362.23,9.03;14,45.78,540.06,114.19,9.03">With this configuration, all of our benchmarks allocate the vast majority of their data objects to the shared arenas.</s><s xml:id="_FQtNGNf" coords="14,162.19,540.06,278.09,9.03;14,45.78,552.02,261.65,9.03">Specifically, the peak capacity of the private arenas is no more than a few MBs in all but two benchmarks (621.wrf_s and 627.cam4_s).</s><s xml:id="_tFjmhW9" coords="14,310.07,552.02,131.32,9.03;14,45.78,563.97,183.25,9.03">In the worst case of 627.cam4_s, the peak RSS of the private arenas is 0.3 GBs.</s></p><p xml:id="_ze64Tc7"><s xml:id="_5jfgM42" coords="14,55.74,575.88,384.57,9.08;14,45.78,587.88,394.54,9.03;15,45.95,55.82,271.61,8.97;15,419.75,55.82,20.72,8.97;15,103.84,239.14,278.72,8.07">Last, in previous works with offline guided data tiering, we have found that the thermos approach is the most effective approach for converting profile information to memory tier recommendations Online Application Guidance for Heterogeneous Memory Systems 45:15 Fig. <ref type="figure" coords="15,119.39,239.14,3.07,8.07">5</ref>. Execution time overhead of memory usage profiling (lower is better).</s><s xml:id="_GnEkdPv" coords="15,45.95,258.08,33.03,9.03"><ref type="bibr" coords="15,45.95,258.08,15.01,9.03" target="#b56">[58,</ref><ref type="bibr" coords="15,63.97,258.08,11.25,9.03" target="#b58">60]</ref>.</s><s xml:id="_bErTuGq" coords="15,81.99,258.08,358.51,9.03;15,45.95,269.98,356.93,9.08">Hence, in this work, all of the offline and online guided data tiering configurations use thermos to partition the allocation sites into sets for the faster and slower memory tiers.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4" xml:id="_XX3hCtb">Reporting Details</head><p xml:id="_T5THhqJ"><s xml:id="_bC2PjMj" coords="15,45.60,306.31,396.56,9.03;15,45.95,318.26,326.90,9.03">Aside from the results showing profile overhead, which use execution time, all performance measurements for each configuration and application are presented as throughput.</s><s xml:id="_4Yh986C" coords="15,376.14,318.26,64.58,9.03;15,45.95,330.23,395.63,9.03;15,45.95,342.18,142.33,9.03">For the CORAL benchmarks, we report the application-specific figure of merit, which, for our selected benchmarks, is always a measure of throughput.</s><s xml:id="_ZHE89mm" coords="15,190.70,342.18,251.45,9.03;15,45.69,354.13,252.68,9.03">For SPEC CPU 2017, we report time per operation (i.e., the inversion of wall clock execution time) for each benchmark run.</s></p><p xml:id="_ckdCe8J"><s xml:id="_NCZu2Gp" coords="15,55.91,366.09,384.57,9.03;15,45.95,378.04,314.76,9.03">Except for the CORAL benchmarks with large and huge input sizes, all results are reported as the mean average of five experimental runs relative to the default configuration.</s><s xml:id="_MmaFwSf" coords="15,363.62,378.04,77.09,9.03;15,45.95,389.99,394.54,9.03;15,45.95,401.96,123.87,9.03">The larger CORAL inputs often require multiple hours for even a single run, and so we only conducted one run of each to limit computing time.</s><s xml:id="_mawFJUV" coords="15,173.29,401.96,267.21,9.03;15,45.95,413.91,394.54,9.03;15,45.95,425.87,396.06,9.03">For experiments with multiple runs, we estimate variability and significance of our results by computing the 95% confidence intervals for the difference between the mean results of the experimental and default configurations, as described in Reference <ref type="bibr" coords="15,423.46,425.87,14.84,9.03" target="#b22">[23]</ref>.</s><s xml:id="_me4thQv" coords="15,45.65,437.82,355.35,9.03">These intervals are plotted as error bars around the sample means in the relevant figures.</s><s xml:id="_K3GPC4H" coords="15,402.96,437.82,38.61,9.03;15,45.95,449.77,370.40,9.03">However, it is important to note that variability is too low for these bars to be visible in some figures.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6" xml:id="_PAbjhnb">EVALUATION 6.1 Online Profile Overhead</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1" xml:id="_Mr2EPAM">Execution Time Overhead.</head><p xml:id="_GD2gjN3"><s xml:id="_Cwwf7k6" coords="15,191.94,501.00,248.55,9.03;15,45.95,512.95,221.93,9.03">Let us first consider the performance overhead of collecting memory usage information during program execution.</s><s xml:id="_aNrFgXd" coords="15,270.29,512.95,171.90,9.03;15,45.95,524.91,394.52,9.03;15,45.58,536.87,278.18,9.03">For this evaluation, we compare the previous offline profiling approach and our online profiling enhancements described in Section 4 (but without any data migration mechanisms) to a default configuration.</s><s xml:id="_2mWnb9h" coords="15,326.52,536.87,115.09,9.03;15,45.95,548.82,394.54,9.03;15,45.95,560.77,396.04,9.03">In the default configuration, each benchmark program is configured to use 16 application threads (one for each hardware thread on our evaluation platform) and the unmodified jemalloc allocator with default execution options.</s><s xml:id="_bWjhC23" coords="15,45.95,572.73,396.21,9.03;15,45.95,584.68,115.56,9.03">In contrast, each profile configuration uses only 15 program threads and an extra thread to conduct all profiling operations.</s><s xml:id="_kcFDc62" coords="15,164.01,584.68,276.46,9.03;15,45.95,596.64,231.07,9.03">For presentation purposes, this section omits detailed results for the CORAL benchmarks with the large and huge input sizes.</s></p><p xml:id="_dn5uenG"><s xml:id="_tRxEtJh" coords="15,55.91,608.60,384.56,9.03;15,45.95,620.55,201.28,9.03">Figure <ref type="figure" coords="15,84.15,608.60,4.63,9.03">5</ref> shows the execution time overhead of the previous offline and current online profiling mechanisms relative to the default configuration.</s><s xml:id="_8jxq3q9" coords="15,250.00,620.55,191.59,9.03;15,45.58,632.51,394.88,9.03;15,45.58,644.46,334.18,9.03">In addition to the two profiling configurations, we also tested two other configurations to isolate the impact of using one less application thread as well as the effect of the hybrid arena allocation strategy described in Section 4.1.1.</s><s xml:id="_rwcKAr6" coords="15,382.27,644.41,58.20,9.08;16,45.77,174.22,315.41,9.03">The default-15 configuration is identical to the baseline but uses only 15 application threads.</s><s xml:id="_jnhwVj6" coords="16,363.81,174.17,76.49,9.08;16,45.77,186.17,394.53,9.03;16,45.77,198.12,211.72,9.03">Hybrid arenas uses our custom allocator to apply the hybrid arena allocation strategy and also uses only 15 application threads but does not collect any profile information.</s></p><p xml:id="_2tcJZv3"><s xml:id="_ZvKJkbr" coords="16,55.74,210.08,384.80,9.03;16,45.77,222.04,273.98,9.03">The results show that using one less application thread to reserve one core for profiling only has a marginal impact for these workloads on our 16-core processor.</s><s xml:id="_M6VJqnR" coords="16,321.83,222.04,118.47,9.03;16,45.77,233.99,394.52,9.03;16,45.55,245.95,56.82,9.03">Overall, both the CORAL and SPEC sets run about 4% slower with one less thread, on average, with a worst-case slow down of 10% for SNAP.</s><s xml:id="_R66zFdR" coords="16,104.05,245.95,180.25,9.03">The hybrid arena allocator has a mixed effect.</s><s xml:id="_x4Tpau3" coords="16,285.97,245.95,156.01,9.03;16,45.77,257.90,316.42,9.03">In some cases, such as SNAP and QMC-PACK, this approach actually improves performance over the default allocator.</s><s xml:id="_eW2fJe8" coords="16,364.34,257.90,75.97,9.03;16,45.77,269.85,379.99,9.03">However, for some SPEC benchmarks, such as 627.cam4_s and 638.imagick_s, it can cause significant slow downs.</s><s xml:id="_p7Nw8eb" coords="16,427.90,269.85,12.39,9.03;16,45.77,281.77,396.21,9.08;16,45.77,293.77,348.59,9.03">On average, and in comparison to the default-15 configuration, the hybrid allocator improves performance by 3% for the CORAL benchmarks, and degrades performance by 6% for SPEC.</s></p><p xml:id="_jYRXmft"><s xml:id="_zqsnFMm" coords="16,55.74,305.72,384.56,9.03;16,45.77,317.68,366.23,9.03">We also find that the new online profiler, which includes the hybrid arena allocator and more efficient RSS accounting, significantly outperforms the previous offline profiling approach.</s><s xml:id="_4m2B29k" coords="16,414.35,317.68,27.63,9.03;16,45.77,329.63,394.72,9.03;16,45.41,341.59,222.61,9.03">On average, execution with the online profiler is 8% faster with the CORAL benchmark set and 26% faster with SPEC compared to the offline profiling approach.</s><s xml:id="_jFMZmRe" coords="16,270.77,341.59,169.53,9.03;16,45.77,353.55,394.53,9.03;16,45.77,365.50,208.66,9.03">Relative to the default configuration with no online profiling and an extra application thread, the online profiler adds 5% and 14% execution time overhead for CORAL and SPEC, respectively.</s><s xml:id="_5g77jgW" coords="16,257.64,365.50,184.34,9.03;16,45.77,377.46,283.39,9.03">If the system includes at least one free computing core to run the profile thread, then the overhead is even lower.</s><s xml:id="_9aPX5bN" coords="16,331.66,377.46,108.63,9.03;16,45.77,388.55,394.51,9.88;16,45.77,401.36,261.36,9.03">Specifically, in comparison to the default-15 configuration, the online profiler adds &lt;1% and &lt;10% execution time cost, on average, for the CORAL and SPEC benchmark sets, respectively.</s></p><p xml:id="_4dDKZWr"><s xml:id="_PgeK8R2" coords="16,55.74,413.32,384.82,9.03;16,45.77,425.28,67.46,9.03">In most cases, the execution time cost is due to the use of the alternative arena allocation strategy during profiling.</s><s xml:id="_tBM3KJa" coords="16,116.26,425.23,324.31,9.08;16,45.77,437.23,396.19,9.03;16,45.77,449.19,184.86,9.03">Indeed, comparing the online profiler and hybrid arenas configurations directly shows that enabling the memory access and RSS tracking adds only about 3.5% overhead, on average, across all of our selected benchmarks.</s><s xml:id="_FkCJ2RA" coords="16,233.82,449.19,206.73,9.03;16,45.77,461.14,394.53,9.03;16,45.77,473.09,394.55,9.03;16,45.77,485.05,167.16,9.03">Hence, while the overhead for profiling is already relatively low compared to previous works, further optimization efforts that enable the runtime to compute and organize the necessary information for each allocation context without affecting data locality could reduce it even further.</s><s xml:id="_gmxGXvn" coords="16,55.74,504.62,97.36,9.08">Time per Profile Interval.</s><s xml:id="_Ffakcrf" coords="16,156.59,504.67,283.71,9.03;16,45.77,516.62,368.55,9.03">In addition to the overhead reductions shown in Figure <ref type="figure" coords="16,380.96,504.67,3.41,9.03">5</ref>, the profiling enhancements developed for this work also reduce the time necessary to collect each profile.</s><s xml:id="_4DJuS68" coords="16,416.20,516.62,24.10,9.03;16,45.77,528.58,396.23,9.03;16,45.77,540.53,27.91,9.03">These reductions enable the runtime to make faster tiering decisions based on more recent memory behavior.</s><s xml:id="_QCCKC9S" coords="16,75.73,540.53,364.58,9.03;16,45.77,552.48,353.36,9.03">Table <ref type="table" coords="16,99.78,540.53,4.63,9.03" target="#tab_2">2</ref> shows the mean and maximum number of seconds necessary to collect and analyze profiles of every allocation context during program execution for each benchmark set.</s><s xml:id="_emnwnHP" coords="16,401.89,552.48,38.40,9.03;16,45.77,564.44,396.07,9.03">Thus, the enhancements reduce the time per profile interval by more than 11× across both benchmark sets.</s><s xml:id="_ptrArT3" coords="16,45.48,576.40,394.83,9.03;16,45.77,588.35,182.05,9.03">The vast majority of this improvement is driven by the system-level instrumentation that tracks the number of pages mapped for each arena.</s><s xml:id="_7V2KECP" coords="16,230.47,588.35,209.84,9.03;16,45.77,600.31,394.52,9.03;16,45.77,612.26,155.69,9.03">As it is no longer necessary to visit every page that is mapped in each arena during the profile interval, the profiler estimates the capacity of each allocation context much more quickly.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2" xml:id="_D73r4dy">Memory Capacity Overhead.</head><p xml:id="_xGR9s8z"><s xml:id="_5HWusTH" coords="16,201.09,631.87,239.40,9.03;16,45.77,643.83,178.91,9.03">Let us next examine the memory capacity overhead of our custom allocator and online profiling tools.</s><s xml:id="_kYs2QAP" coords="16,227.91,643.83,212.39,9.03;17,45.95,55.82,271.61,8.97;17,419.75,55.82,20.72,8.97;17,45.95,238.14,21.10,8.07">There are two primary reasons our approach could Online Application Guidance for Heterogeneous Memory Systems 45:17 Fig. <ref type="figure" coords="17,60.90,238.14,3.07,8.07">6</ref>.</s><s xml:id="_JC3DT2a" coords="17,71.52,238.14,368.94,8.07">Memory capacity overhead (peak RSS) of the hybrid arena scheme and online profiler (lower is better).</s></p><p xml:id="_DGVaab3"><s xml:id="_MXXzBx6" coords="17,45.95,258.84,285.38,9.03">potentially increase memory capacity relative to the default allocator.</s><s xml:id="_7dCc38R" coords="17,334.20,258.84,107.98,9.03;17,45.95,270.80,394.54,9.03;17,45.95,282.75,187.41,9.03">First, dividing the application heap into separate arenas for each allocation context increases fragmentation and could boost the number of unused bytes on resident pages.</s><s xml:id="_F9nna4j" coords="17,235.53,282.75,204.96,9.03;17,45.95,294.70,208.90,9.03">Second, the runtime itself requires some additional space to collect and organize profiling information.</s><s xml:id="_FbCmYAD" coords="17,257.51,294.70,183.37,9.03;17,45.95,306.66,394.69,9.03;17,45.95,318.62,291.84,9.03">In particular, in-memory buffers to store raw data access samples, as well as structures to aggregate and hold memory usage information for each allocation context increase capacity requirements during profiling.</s></p><p xml:id="_7APnvHM"><s xml:id="_GkEGVMN" coords="17,55.91,330.57,384.57,9.03;17,45.95,342.53,394.51,9.03;17,45.95,354.48,394.54,9.03;17,45.95,366.43,39.92,9.03">Figure <ref type="figure" coords="17,83.83,330.57,4.63,9.03">6</ref> shows the peak resident set size of each benchmark with (1) the hybrid arena allocation scheme, but without any profiling enabled, and (2) the online profiler, which uses the hybrid arena scheme but also collects and stores memory usage information that is necessary for our online approach.</s><s xml:id="_DzZu3MG" coords="17,88.34,366.43,344.82,9.03">All results are shown relative to the peak RSS of execution with the default allocator.</s></p><p xml:id="_uCKvdqU"><s xml:id="_RmkD3Ys" coords="17,55.91,378.39,384.56,9.03;17,45.95,390.35,161.06,9.03">Thus, the online profiler does require a modest increase in memory capacity compared to the default allocator for most applications.</s><s xml:id="_rj9wQAg" coords="17,210.52,390.35,230.21,9.03;17,45.95,402.30,396.07,9.03">For the medium CORAL benchmarks, capacity rises by about 10% with online profiling enabled, on average, with a worst-case increase of 18% for LULESH.</s><s xml:id="_zkCEKuG" coords="17,45.95,414.26,394.79,9.03;17,45.95,426.16,299.41,9.08">For SPEC, the average increase is somewhat larger at 29%, but this difference is mostly driven by outsize increases in smaller benchmarks, such as 621.wrf_s and 644.nab_s.</s><s xml:id="_fd56vTD" coords="17,347.97,426.21,92.51,9.03;17,45.95,438.16,394.51,9.03;17,45.95,450.12,263.80,9.03">These benchmarks use only a few hundred MBs of memory with the default configuration, and so, even a relatively small amount of additional capacity appears as a substantial overhead.</s><s xml:id="_TavDJ6c" coords="17,312.44,450.12,128.04,9.03;17,45.95,462.08,342.07,9.03">Additionally, we found that the space overhead of our approach does not necessarily scale with larger workload sizes.</s><s xml:id="_aBMbFRr" coords="17,389.94,462.08,51.63,9.03;17,45.95,474.04,394.53,9.03;17,45.95,485.99,388.32,9.03">In particular, the capacities of the large and huge CORAL workloads with the online profiler (which are not shown in Figure <ref type="figure" coords="17,114.44,485.99,4.63,9.03">6</ref> to avoid clutter) are only about 1% larger, on average, than default execution.</s></p><p xml:id="_GwBpw8Q"><s xml:id="_cMBYyD3" coords="17,55.91,497.94,386.23,9.03;17,45.95,509.90,394.55,9.03;17,45.95,521.86,107.90,9.03">We also note that, while these overheads could potentially increase pressure in the faster memory tier(s), heterogeneous memory platforms typically support much larger capacities through additional memory layers.</s><s xml:id="_RX7Ha2f" coords="17,157.06,521.86,283.43,9.03;17,45.95,533.81,396.21,9.03;17,45.95,545.77,395.35,9.03">Hence, unless the online approach does a poor job of separating the sources of these overheads from hot program data, this additional capacity is less likely to be a burden on real platforms, as it will usually be accommodated by larger (but slower) memory storage.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2" xml:id="_7zEdTSR">Performance of Guided Data Management with Varying Capacity Constraints</head><p xml:id="_E2epffN"><s xml:id="_GXM2DJK" coords="17,45.95,583.80,394.55,9.03;17,45.95,595.76,298.89,9.03">Our next set of experiments aim to evaluate the performance of offline and online guided data management with varying capacity constraints in the faster memory tier.</s><s xml:id="_gnvpRvq" coords="17,347.42,595.76,93.07,9.03;17,45.95,607.72,394.76,9.03;17,45.95,619.67,51.81,9.03">For this evaluation, we again use the CORAL benchmarks with medium input sizes as well as the selected SPEC CPU 2017 benchmarks.</s><s xml:id="_MbbdF4t" coords="17,100.03,619.67,340.45,9.03;17,45.95,631.63,394.53,9.03;17,45.95,643.58,32.58,9.03">To evaluate each workload and configuration with different capacity constraints, we extended our Linux kernel with new facilities to control the amount of DRAM available for a given process.</s><s xml:id="_TCW3s6k" coords="17,80.61,643.58,359.85,9.03;18,45.77,462.14,396.02,8.07;18,45.77,473.10,395.92,8.07">Specifically, we added an option to the memory control group (cgroup) interface <ref type="bibr" coords="17,413.84,643.58,16.36,9.03" target="#b48">[49]</ref> to Fig. <ref type="figure" coords="18,62.35,462.14,3.07,8.07">7</ref>. Performance (throughput) of offline and online guidance-based approaches, compared to the unguided first touch configuration, with varying amounts of capacity available in the faster DDR4 memory tier.</s><s xml:id="_3WYfmuq" coords="18,45.50,484.06,394.81,8.07;18,45.77,495.02,25.82,8.07">All results are shown relative to a configuration with all program data allocated to the DDR4 tier (higher is better).</s><s xml:id="_qEWYW3X" coords="18,73.53,494.93,366.99,8.17;18,45.77,505.98,238.60,8.07">Note that the DDR4 tier capacities shown along the x-axis are calculated as a percentage of the peak resident set size during execution with the default configuration.</s></p><p xml:id="_qvfeaF5"><s xml:id="_aFzmfaa" coords="18,45.77,534.88,394.53,9.03;18,45.77,546.83,355.24,9.03">allow individual processes or process groups to limit the amount of physical memory that they are able to allocate and keep resident on a particular NUMA node at any given point in time.</s><s xml:id="_ME6Tee2" coords="18,403.11,546.83,37.19,9.03;18,45.77,558.78,394.52,9.03;18,45.77,570.74,394.88,9.03;18,45.77,582.69,338.23,9.03">Thus, if a process attempts to map a virtual page to a new physical page on a node whose specified limit has already been reached, then the kernel will force the process to use a page from a different NUMA node to satisfy the fault or fallback to page reclaim if no other memory is available.</s></p><p xml:id="_qX86wRz"><s xml:id="_aN8hpse" coords="18,55.74,594.65,386.26,9.03;18,45.77,606.61,207.67,9.03">To prepare these experiments, we first measured the peak resident set size of a run of the default configuration of each benchmark application.</s><s xml:id="_AmM9nGJ" coords="18,256.21,606.61,184.09,9.03;18,45.77,618.56,394.79,9.03;18,45.77,630.51,130.03,9.03">Subsequent experiments then use the cgroup controls to limit the capacity available in the DRAM tier to be a percentage of the measured peak RSS of the running application.</s><s xml:id="_3H97mN4" coords="18,179.07,630.51,262.90,9.03;18,45.77,642.47,288.55,9.03">Specifically, we tested configurations with DRAM capacity limited to 10%, 20%, 30%, 40%, and 50% of the peak RSS of the application.</s><s xml:id="_nvdFCnC" coords="18,337.07,642.47,103.21,9.03;19,45.95,55.82,271.61,8.97;19,419.75,55.82,20.72,8.97;19,45.95,82.77,394.78,9.03;19,45.58,94.68,274.57,9.08">For comparison against a Online Application Guidance for Heterogeneous Memory Systems 45:19 standard data tiering approach that does not use any profile guidance, we also ran each benchmark with each capacity limit with an unguided first touch configuration.</s><s xml:id="_qVzdGBq" coords="19,322.67,94.73,117.80,9.03;19,45.95,106.68,394.55,9.03;19,45.95,118.63,233.73,9.03">The first touch configuration simply satisfies all memory demands from the application with allocations from the DRAM tier if capacity is available and, otherwise, from the Optane tier.</s></p><p xml:id="_tAeYEx9"><s xml:id="_rx9P97q" coords="19,55.91,130.60,384.55,9.03;19,45.94,142.55,178.83,9.03">Figure <ref type="figure" coords="19,85.27,130.60,4.63,9.03">7</ref> presents the performance of each benchmark with first touch as well as the offline and online guided data tiering approaches.</s><s xml:id="_f9Scjfe" coords="19,228.36,142.55,212.11,9.03;19,45.94,154.50,394.72,9.03;19,45.94,166.46,177.16,9.03">All results show throughput relative to the default configuration with no capacity limitations in the DRAM tier (i.e., all memory objects use the faster memory devices), and, thus, higher is better.</s><s xml:id="_PJYnNwg" coords="19,225.15,166.46,215.32,9.03;19,45.94,178.41,29.12,9.03">We can make several key observations based on these results.</s><s xml:id="_RrZhAXD" coords="19,76.97,178.41,363.50,9.03;19,45.94,190.37,104.28,9.03">First, profile guided data tiering enables significant speedups compared to first touch for all four CORAL benchmarks.</s><s xml:id="_TvMJR4C" coords="19,152.17,190.37,288.32,9.03;19,45.65,202.33,396.36,9.03">In the best cases, the offline approach is up to 7.3× faster than first touch (LULESH, 20% DRAM), while the online approach is up to 7.1× faster (QMCPACK, 50% DRAM).</s><s xml:id="_VD79UhP" coords="19,45.59,214.28,394.89,9.03;19,45.94,226.24,381.27,9.03">Average (geometric mean) speedups with the CORAL benchmarks range from 2.1× to 3.3× for the offline approach, and 1.8× to 2.5× for the online approach, across the different capacity limits.</s></p><p xml:id="_UwhJRb4"><s xml:id="_zr9fnQ5" coords="19,55.90,238.19,384.56,9.03;19,45.94,250.14,137.34,9.03">The performance impact of profile guided data tiering with the SPEC benchmark set is more modest but still significant.</s><s xml:id="_JMv3Fgb" coords="19,186.72,250.14,254.86,9.03;19,45.94,262.11,396.08,9.03">Several benchmarks (specifically, 607.cactuBSSN_s, 621.wrf_s, 638.imagick_s, and 644.nab_s) exhibit little or no improvement with guided data management.</s><s xml:id="_4Br7XPT" coords="19,45.94,274.06,394.79,9.03;19,45.94,286.01,396.07,9.03">In some cases (specifically, 638.imagick_s, and 644.nab_s), the online approach actually slightly degrades performance, because the overhead of profiling is not offset by any gains in efficiency.</s><s xml:id="_kMSrsyF" coords="19,45.94,297.97,396.07,9.03">However, guided data tiering does enable significant speedups for the other SPEC benchmarks.</s><s xml:id="_MjZ9aga" coords="19,45.94,309.92,394.52,9.03;19,45.94,321.87,395.61,9.03;19,45.94,333.84,151.72,9.03">For instance, the offline approach speeds up some configurations of 603.bwaves_s and 654.roms_s by more than 50%, while the online approach speeds up these applications by up to 18% and 35%, respectively, compared to first touch.</s><s xml:id="_jDCYs4Q" coords="19,200.48,333.84,240.01,9.03;19,45.94,345.79,396.23,9.03;19,45.94,357.74,30.19,9.03">The best case for both the offline and online approaches is 628.pop2_s with 20% DRAM capacity, which speeds up by more than 84% with either guided approach.</s><s xml:id="_HgTt26C" coords="19,79.40,357.74,361.07,9.03;19,45.94,369.70,394.53,9.03;19,45.94,381.65,68.03,9.03">Overall, and across the different capacity limits, average speedups for the full group of SPEC benchmarks ranges from 5.7% to 14.6% for the offline approach and 1.8% to 8.6% for the online approach.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head xml:id="_TTZEff5">6.2.1</head><p xml:id="_2p7whb8"><s xml:id="_urPhTqz" coords="19,83.52,403.62,191.33,8.98">Comparing the Offline and Online Approaches.</s><s xml:id="_EhQvz4S" coords="19,278.36,403.57,162.12,9.03;19,45.94,415.53,394.71,9.03;19,45.94,427.48,170.54,9.03">While there are a few cases where the online approach outperforms the offline approach, in general, the offline approach enables faster execution times than the online approach.</s><s xml:id="_sy8Kb72" coords="19,218.84,427.48,223.32,9.03;19,45.94,439.43,170.74,9.03">To explain why, we analyzed the individual online profiles of several of our selected benchmarks.</s><s xml:id="_rhvMhKV" coords="19,218.54,439.43,221.93,9.03;19,45.57,451.39,394.91,9.03;19,45.94,463.35,394.52,9.03;19,45.94,475.31,167.80,9.03">We found that the capacity and usage of data associated with each allocation site may shift substantially during early portions of the run, but after a short initial period, the sorted rank of each site and selection of sites assigned to the upper tier remains relatively stable from interval to interval.</s><s xml:id="_MADaWKu" coords="19,216.26,475.31,224.24,9.03;19,45.94,487.26,354.36,9.03">Indeed, in every case we analyzed, the online approach generates a solution that is similar to the offline approach after this short initial period.</s></p><p xml:id="_cecHhSs"><s xml:id="_bctZ2H5" coords="19,55.90,499.21,384.79,9.03;19,45.94,511.17,394.53,9.03;19,45.94,523.13,394.80,9.03;19,45.94,535.08,180.22,9.03">To understand how this behavior impacts the operation of the online approach, consider Figure <ref type="figure" coords="19,75.02,511.17,3.41,9.03">8</ref>, which plots the total memory (DRAM + Optane) bandwidth as well as the number of GBs migrated between memory tiers over time with the online approach for the four CORAL benchmarks with their medium input sizes.</s><s xml:id="_BhaWQQb" coords="19,229.42,535.08,211.04,9.03;19,45.94,547.04,254.82,9.03">These results correspond to the configuration with DRAM capacity limited to 50% of the application's peak RSS.</s><s xml:id="_b2fpkDr" coords="19,304.15,547.04,138.00,9.03;19,45.57,558.99,394.90,9.03;19,45.94,570.94,288.08,9.03">Thus, we see that memory bandwidth for the online run is relatively low during early portions of the run, that is, until the runtime has enough profile information to make good data placement decisions.</s><s xml:id="_TpGumWM" coords="19,336.18,570.94,104.53,9.03;19,45.94,582.90,333.32,9.03">Additionally, the majority of data migration occurs during this early period of relatively poor performance.</s><s xml:id="_WT7Bwys" coords="19,382.27,582.90,58.21,9.03;19,45.94,594.86,394.56,9.03;19,45.94,606.81,394.54,9.03;19,45.94,618.77,45.80,9.03">Even after the runtime identifies a good data placement strategy, later intervals may still sometimes interrupt the application to change data placement, but these migrations have relatively little impact on system bandwidth.</s></p><p xml:id="_TakgMtE"><s xml:id="_WvqZY3t" coords="19,55.90,630.72,386.26,9.03;19,45.94,642.67,155.80,9.03">However, frequent data movement, if not accompanied with a rise in efficiency, can be a significant overhead of the online approach.</s><s xml:id="_Eva2VDj" coords="19,204.07,642.67,236.41,9.03;20,45.77,358.15,394.53,8.07;20,45.77,369.10,68.60,8.07">The rightmost column of Table <ref type="table" coords="19,332.00,642.67,4.63,9.03" target="#tab_3">3</ref> shows the migration rate Fig. <ref type="figure" coords="20,61.40,358.15,3.07,8.07">8</ref>. Data bandwidth (GB per second) and migrations (GBs) over time for four CORAL benchmarks with the medium input.</s><s xml:id="_W6yb3v3" coords="20,116.57,369.10,323.73,8.07;20,45.77,380.06,191.93,8.07">The results are collected over 10-second intervals during a run with the available DRAM limited to 50% of the peak RSS of the application.</s><s xml:id="_3FTjnfh" coords="20,241.06,379.97,199.24,8.17;20,45.77,391.02,395.91,8.07">Bandwidth is plotted on the left y-axis, which has a maximum value of 100 GB/second, matching the maximum sustainable DRAM bandwidth on our platform.</s><s xml:id="_JbXYwfH" coords="20,45.77,401.89,381.23,8.17">Data migrations are plotted on the right y-axis, which ranges from 0 to the peak RSS of the application.</s><s xml:id="_ugyatXJ" coords="21,45.65,82.77,394.83,9.03;21,45.95,94.73,396.24,9.03;21,45.95,106.68,23.43,9.03">(in MB per seconds) during execution with the online approach alongside the execution times of the default (unguided), offline, and online approaches for each CORAL-medium and SPEC benchmark.</s><s xml:id="_RGXj2y8" coords="21,73.00,106.68,367.46,9.03;21,45.95,118.63,63.13,9.03">The results use the configuration that limits DRAM capacity to 50% of the peak RSS of the application.</s><s xml:id="_uBuBEPZ" coords="21,111.73,118.63,328.75,9.03;21,45.65,130.55,213.00,9.08">Thus, migration rates for the online approach range from less than 1 MB/second (for 621.wrf_s) to almost 339 MB/second (for AMG).</s><s xml:id="_XbH7jBY" coords="21,261.69,130.60,178.80,9.03;21,45.95,142.55,354.29,9.03">Overall, applications with higher migration rates do tend to exhibit more substantial slowdowns compared to the offline approach.</s><s xml:id="_7ynpNZ3" coords="21,402.95,142.55,38.61,9.03;21,45.95,154.45,394.54,9.08;21,45.95,166.46,174.33,9.03">However, for some applications, such as QMCPACK and 649.fotonik3d_s, the benefits of dynamic adaptation ultimately outweigh these additional costs.</s></p><p xml:id="_ME2wRAS"><s xml:id="_xdBVKtG" coords="21,55.91,178.41,384.56,9.03;21,45.95,190.37,194.16,9.03">In sum, there are three main reasons that the performance of the online approach sometimes lags the offline approach for these benchmarks.</s><s xml:id="_MEjv2Ng" coords="21,242.90,190.37,197.81,9.03;21,45.95,202.33,378.97,9.03">For one, the offline approach does not incur any overhead for profiling and is able to use an additional computing core for program execution.</s><s xml:id="_xWGEQWY" coords="21,427.35,202.33,13.13,9.03;21,45.95,214.28,394.54,9.03;21,45.95,226.24,370.34,9.03">We expect that future efforts could reduce online profile overhead by disabling some or most of the profiler after it has collected sufficient information to characterize the current usage pattern.</s><s xml:id="_f8z7P2w" coords="21,418.29,226.24,22.18,9.03;21,45.95,238.19,396.24,9.03;21,45.95,250.14,22.40,9.03">Some relatively lightweight profiling might still be necessary to detect if program usage shifts substantially.</s><s xml:id="_NAsDgAn" coords="21,70.89,250.14,369.59,9.03;21,45.95,262.11,394.51,9.03;21,45.95,274.06,66.07,9.03">Next, even if the online approach eventually generates a similar or better solution than the offline approach, the application still executes with suboptimal data placement during the initial profiling period.</s><s xml:id="_eSGCqNr" coords="21,114.52,274.06,327.06,9.03;21,45.95,286.01,394.54,9.03;21,45.65,297.97,109.04,9.03">We found that this effect can have a considerable negative impact in some cases, as this initial period can be a significant portion of the total execution time for some benchmarks (e.g., AMG in Figure <ref type="figure" coords="21,133.92,297.97,13.85,9.03">8(b)</ref>).</s><s xml:id="_yu2TT7S" coords="21,158.06,297.97,282.40,9.03;21,45.95,309.92,396.05,9.03">And third, certain usage patterns may cause the online approach to move data between tiers more frequently but with relatively little benefit to overall performance.</s><s xml:id="_FhfRPR9" coords="21,45.95,321.87,394.57,9.03;21,45.58,333.84,395.17,9.03;21,45.95,345.79,390.97,9.03">Even with these limitations, the online approach is still preferable to the offline approach in cases where (1) it is not feasible to collect or maintain offline profile data or (2) it is not possible or very difficult to construct and profile program inputs that are representative of production execution.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3" xml:id="_esy6xKq">Performance Analysis with Large Memory Workloads</head><p xml:id="_nAAuYny"><s xml:id="_a2fPEAV" coords="21,45.47,385.64,396.71,9.03;21,45.95,397.59,180.51,9.03">We conclude our evaluation by examining the impact of guided data tiering on the CORAL benchmarks with the large and huge input sizes.</s><s xml:id="_HTVjXsS" coords="21,230.07,397.59,210.40,9.03;21,45.95,409.55,127.56,9.03">There are several benefits of evaluation with such large-scale memory workloads.</s><s xml:id="_GBhcKSN" coords="21,176.17,409.55,264.33,9.03;21,45.95,421.50,394.55,9.03;21,45.95,433.46,93.98,9.03">Since these workloads require more memory capacity than there is available DRAM on our platform, there is no need to artificially limit the available capacity of the faster memory tier.</s><s xml:id="_Pttj8Qg" coords="21,142.70,433.46,299.46,9.03;21,45.95,445.42,240.18,9.03">As a result, data movement costs are also more realistic, because the system may migrate data into and out of the entire DRAM tier.</s><s xml:id="_B4JCAgk" coords="21,288.41,445.42,152.27,9.03;21,45.95,457.37,394.53,9.03;21,45.95,469.32,251.27,9.03;21,297.23,467.50,3.38,6.59">Additionally, this approach allows for direct comparison between guided (and unguided) software-based data tiering approaches and the hardware-managed DRAM caching available on our platform. <ref type="foot" coords="21,297.23,467.50,3.38,6.59" target="#foot_8">6</ref></s><s xml:id="_Wfac3kS" coords="21,55.91,481.28,384.57,9.03;21,45.95,493.23,355.05,9.03">igure <ref type="figure" coords="21,85.12,481.28,4.63,9.03" target="#fig_3">9</ref> shows the performance of the four CORAL benchmarks with the offline and online approaches alongside the hardware-managed DRAM caching option on our platform.</s><s xml:id="_XaCUDmN" coords="21,404.39,493.23,36.29,9.03;21,45.95,505.19,396.06,9.03">Each bar shows throughput relative to the unguided first-touch configuration, and, thus, higher is better.</s><s xml:id="_5ccgExa" coords="21,45.47,517.15,395.01,9.03;21,45.95,529.10,74.12,9.03">We find that the offline and online approaches significantly outperform unguided first touch in almost every case.</s><s xml:id="_Ttp4Jum" coords="21,122.59,529.10,317.88,9.03;21,45.72,541.05,262.86,9.03">In the best case, LULESH with the large input achieves speedups of more than 7.7× and 6.1× for the offline and online approaches, respectively.</s></p><p xml:id="_2QWTHeH"><s xml:id="_7jZRkNc" coords="21,55.91,553.01,386.26,9.03;21,45.95,564.96,286.81,9.03">For LULESH, AMG, and SNAP, the guided approaches achieve similar, or somewhat better, performance than the hardware-managed caching mode on our platform.</s><s xml:id="_DJKWjRD" coords="21,335.54,564.96,104.91,9.03;21,45.95,576.92,394.86,9.03;21,45.95,588.88,234.52,9.03">The biggest improvement comes with the large input of AMG, which is almost 70% faster with the offline approach and 45% faster with the online approach compared to HW caching.</s><s xml:id="_h6Jqa9D" coords="21,282.69,588.88,157.77,9.03;21,45.95,600.83,394.55,9.03;21,45.95,612.79,383.85,9.03">For these cases, there is also significant potential for memory energy savings with the guided approaches, as hardware-managed caching typically generates much more data movement between tiers than software-based approaches.</s><s xml:id="_esbV8Pu" coords="22,55.74,279.80,384.57,9.03;22,45.77,291.75,396.22,9.03;22,45.77,303.71,38.53,9.03">For QMCPACK, however, hardware-managed caching is much more efficient than either guided approach and achieves speedups ranging from 2.8× to 7× faster compared to the guided approaches.</s><s xml:id="_mkDVd8a" coords="22,87.02,303.71,354.83,9.03">This case highlights one of the remaining limitations of our profile guided approaches.</s><s xml:id="_QEsebcK" coords="22,45.77,315.66,394.53,9.03;22,45.52,327.62,158.37,9.03">On further analysis, we found that the larger QMCPACK inputs use a single allocation site for the vast majority of their data allocations.</s><s xml:id="_6ZeUvCk" coords="22,207.12,327.62,233.18,9.03;22,45.77,339.58,270.72,9.03">Specifically, this site creates between 60% and 63% of all resident program data during runs with the large and huge inputs.</s><s xml:id="_3E5SNRb" coords="22,319.04,339.58,121.26,9.03;22,45.77,351.53,394.52,9.03;22,45.77,363.48,47.51,9.03">Despite its large size, the data created at this site also exhibits the most frequent usage per byte relative to the other data in the application.</s><s xml:id="_mKrtG4r" coords="22,96.41,363.48,343.89,9.03;22,45.77,375.44,323.00,9.03">As a result, these data are almost always assigned to the DRAM tier during guided execution, even if a significant portion of them is relatively cold for some time.</s><s xml:id="_tRVjK7B" coords="22,371.46,375.44,68.86,9.03;22,45.77,387.39,394.57,9.03;22,45.77,399.35,396.19,9.03;22,45.77,411.31,23.48,9.03">Hence, while the guided approaches still outperform first touch, they lag the performance of the hardware-based approach, which is able to evict and replace cold data in the DRAM cache at a much finer granularity.</s><s xml:id="_vynEt9A" coords="22,71.71,411.31,368.60,9.03;22,45.77,423.26,394.51,9.03;22,45.77,435.21,96.87,9.03">One approach that could potentially address this limitation is to break up large sets of data created from the same site based on different data features, such as the age of the data, or the PID of the allocating thread.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7" xml:id="_jx7sc3Z">FUTURE WORK</head><p xml:id="_gerctaU"><s xml:id="_bqChjnc" coords="22,45.48,475.23,395.05,9.03;22,45.77,487.18,158.73,9.03">This study targeted a state-of-the-art heterogeneous memory platform with conventional DDR4 SDRAM and non-volatile Optane RAM.</s><s xml:id="_v8SXep4" coords="22,206.65,487.18,235.36,9.03;22,45.77,499.13,396.19,9.03;22,45.77,511.09,395.60,9.03;22,45.77,523.04,366.93,9.03">In the next few years, memory systems are expected to become even more complex, with more diverse technologies and capabilities within a single heterogeneous architecture, including high bandwidth memories, mixed HW/SW data management modes, processing-in-memory, non-uniform cache access, as well as GPUs and other accelerators.</s><s xml:id="_DJzwZnD" coords="22,415.24,523.04,25.05,9.03;22,45.77,535.00,394.54,9.03;22,45.77,546.96,394.55,9.03;22,45.77,558.91,151.93,9.03">As we take this work forward, we plan to modify our tools and framework to support these technologies and will explore the challenges and opportunities that arise from guiding data management on more complex memory architectures.</s></p><p xml:id="_X7k9FwW"><s xml:id="_czWDny6" coords="22,55.74,570.87,384.60,9.03;22,45.77,582.82,342.78,9.03">To prepare for these future platforms, we are currently refactoring our toolset to separate the profiling and data management components of our approach into separate modules.</s><s xml:id="_kwG9V6M" coords="22,391.20,582.82,49.09,9.03;22,45.77,594.77,394.51,9.03;22,45.77,606.73,93.31,9.03">At the same time, we are building new versions of this approach that do not require access to program source code or recompilation.</s><s xml:id="_HUX8N4n" coords="22,142.52,606.73,297.80,9.03;22,45.77,618.69,394.50,9.03;22,45.77,630.64,396.24,9.03;22,45.77,642.60,27.02,9.03">The updated toolset will allow the runtime allocator, or the application itself, to specify address ranges with similar expected usage and then send this information to a system-level daemon that tracks and manages data allocations and placement for one or more processes.</s><s xml:id="_97GGH2t" coords="22,75.38,642.60,364.91,9.03;23,45.95,82.77,394.73,9.03;23,45.95,94.73,155.47,9.03">The goal is to create a more flexible toolset, which will enable users to rapidly design and implement alternative strategies for profiling application data usage, along with new policies for managing application data placement.</s><s xml:id="_7dXx3NQ" coords="23,204.17,94.73,236.32,9.03;23,45.95,106.68,394.73,9.03;23,45.95,118.63,396.19,9.03;23,45.95,130.60,56.88,9.03">Eventually, we will use these tools to investigate different design choices and options for guiding data placement, and will evaluate their benefits with a wider range of workloads on computing platforms with more varied and complex heterogeneous memory hardware.</s><s xml:id="_4RA3GMg" coords="23,104.93,130.60,335.53,9.03;23,45.95,142.55,394.53,9.03;23,45.95,154.50,323.97,9.03">Some planned experiments include evaluating different ways of clustering program data into groups with similar expected usage (e.g., as described at the end of Section 6.3), with the goal of enabling more effective prediction and management of memory usage.</s><s xml:id="_2Wt2geV" coords="23,373.08,154.50,67.38,9.03;23,45.95,166.46,395.64,9.03;23,45.95,178.41,211.60,9.03">Additionally, we plan to explore challenges presented by workloads with more varied and irregular access patterns, including web servers and multi-process workloads.</s></p><p xml:id="_9Zwc98Q"><s xml:id="_tBPcjpH" coords="23,55.91,190.37,385.68,9.03;23,45.95,202.33,181.36,9.03">Last, we plan to develop techniques that exploit features of managed languages, such as Java, to further enhance guided data management.</s><s xml:id="_QfMVe7X" coords="23,229.48,202.33,211.00,9.03;23,45.95,214.28,394.53,9.03;23,45.95,226.24,42.49,9.03">Managed language virtual machines (VMs) offer a number of features that can simplify (and often boost the efficiency of) classifying and migrating heap data.</s><s xml:id="_YKcWBZt" coords="23,91.90,226.24,348.56,9.03;23,45.95,238.19,394.52,9.03;23,45.95,250.14,175.57,9.03">In particular, these systems typically shield applications from directly accessing the locations of objects on the heap, thereby freeing them from the need to update references with relocated addresses when objects migrate.</s><s xml:id="_JDk4j8s" coords="23,225.22,250.14,215.26,9.03;23,45.95,262.11,394.54,9.03;23,45.95,274.06,288.90,9.03">Other VM features, such as garbage collection, and emulation engines that are designed for FDOs, can also make guidance-based data management easier to deploy and more effective for managed language applications.</s></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8" xml:id="_HC8UcTT">CONCLUSIONS</head><p xml:id="_fkXhkWt"><s xml:id="_MdAp4fJ" coords="23,45.65,313.91,394.84,9.03;23,45.95,325.86,147.05,9.03">This work develops the first-ever fully automatic and online profile guided data tiering solution for heterogeneous memory systems.</s><s xml:id="_AvQHgXu" coords="23,195.31,325.81,245.17,9.08;23,45.95,337.81,394.51,9.03;23,45.95,349.77,39.58,9.03">It extends our previous offline profiling-based approach with new techniques to collect data tiering guidance with very low, and often negligible, performance overhead.</s><s xml:id="_wFRHgk9" coords="23,87.70,349.77,354.48,9.03;23,45.95,361.73,396.25,9.03;23,45.95,373.69,18.62,9.03">It also develops a novel online algorithm that periodically analyzes this high-level information and uses it to steer data allocation and placement across a heterogeneous memory architecture.</s><s xml:id="_xb3QdHZ" coords="23,66.80,373.69,373.69,9.03;23,45.95,385.64,396.21,9.03;23,45.95,397.59,237.58,9.03">The evaluation shows that this approach significantly outperforms unguided data placement on a state-of-the-art Intel platform with DDR4 SDRAM and Optane NVRAM, with speedups ranging from 1.4× to 7× for a standard set of HPC workloads.</s><s xml:id="_J8seGkY" coords="23,286.54,397.59,153.94,9.03;23,45.95,409.55,394.72,9.03;23,45.95,421.50,160.95,9.03">Additionally, we find that, aside from a short startup period needed for convergence, the online approach achieves performance similar to that of a well-tuned offline approach.</s><s xml:id="_SzAJErk" coords="23,209.28,421.50,231.19,9.03;23,45.95,433.46,394.54,9.03;23,45.95,445.42,307.86,9.03">However, because it adapts automatically to the program as it runs, it does not need to collect or store profile information from a separate execution, which can be unwieldy and may lead to stale or unrepresentative profile guidance.</s></p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0" coords="11,52.32,182.03,161.38,9.08;11,52.32,195.35,5.59,7.22;11,77.44,193.75,194.83,9.38;11,52.32,207.31,5.59,7.22;11,77.83,205.94,169.30,9.08;11,52.32,219.26,5.59,7.22;11,92.38,217.66,135.87,9.38;11,52.32,231.22,5.59,7.22;11,92.77,229.04,259.12,9.96;11,52.32,243.18,5.59,7.22;11,107.38,241.00,55.26,9.96;11,52.32,255.13,5.59,7.22;11,107.26,252.95,59.58,9.96;11,52.32,267.08,5.59,7.22;11,92.77,264.90,278.85,9.96;11,52.32,279.04,5.59,7.22;11,107.19,276.86,55.41,9.96;11,48.61,290.99,9.30,7.22;11,107.26,288.81,59.58,9.96;11,48.61,305.30,9.30,7.22;11,77.83,303.12,55.39,9.88;11,48.61,317.26,9.30,7.22;11,92.38,315.66,258.63,9.38"><head>1 : 5 :</head><label>15</label><figDesc><div><p xml:id="_DhgkaFH"><s xml:id="_Veqv2zD" coords="11,62.88,182.03,150.81,9.08;11,52.32,195.35,5.59,7.22;11,77.44,193.75,194.83,9.38;11,52.32,207.31,5.59,7.22;11,77.83,205.94,169.30,9.08;11,52.32,219.26,5.59,7.22;11,92.38,217.66,135.87,9.38;11,92.77,229.04,259.12,9.96;11,52.32,243.18,5.59,7.22;11,107.38,241.00,55.26,9.96;11,52.32,255.13,5.59,7.22;11,107.26,252.95,59.58,9.96;11,52.32,267.08,5.59,7.22;11,92.77,264.90,278.85,9.96;11,52.32,279.04,5.59,7.22;11,107.19,276.86,55.41,9.96;11,48.61,290.99,9.30,7.22;11,107.26,288.81,59.58,9.96;11,48.61,305.30,9.30,7.22;11,77.83,303.12,55.39,9.88;11,48.61,317.26,9.30,7.22;11,92.38,315.66,258.63,9.38">procedure GetSkiCosts(pro f , recs)2: rentalCost ← purchaseCost ← a ← b ← c ← 0; 3:for (site, curTier , accs, paдes) in pro f do 4: recTier ← GetRecTier(site, recs); if curTier = OPT AN E_T IER and recTier = DRAM_T IER then 6: a ← a + accs; 7: c ← c + paдes; 8: else if curTier = DRAM_T IER and recTier = OPT AN E_T IER then 9: b ← b + accs; 10: c ← c + paдes; 11: if a &gt; b then 12: rentalCost ← ((ab) * EXT RA_NS_PER_SLOW ER_ACCESS );</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1" coords="11,48.61,331.21,9.30,7.22;11,77.34,329.61,197.80,9.38;11,48.61,343.17,9.30,7.22;11,77.83,341.79,143.23,9.08;11,48.61,357.48,9.30,7.22;11,48.61,368.11,127.47,9.03;11,48.61,381.39,9.30,7.22;11,77.34,379.79,133.25,9.38;11,48.61,393.34,9.30,7.22;11,77.44,391.74,112.03,9.38;11,227.11,392.02,213.97,9.03;11,48.61,405.30,9.30,7.22"><head></head><label></label><figDesc><div><p xml:id="_5R35ykg"><s xml:id="_zX8HQzP" coords="11,77.44,391.74,112.03,9.38;11,227.11,392.02,213.97,9.03;11,48.61,405.30,9.30,7.22">recs ← GetTierRecs (pro f ); uses one of the MemBrain approaches (Section 3.2.1)19:</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2" coords="12,45.77,236.14,394.54,8.07;12,45.77,247.11,396.00,8.07;12,45.77,258.06,394.66,8.07;12,45.77,269.02,394.53,8.07;12,45.77,279.98,394.52,8.07;12,45.77,290.94,273.51,8.07"><head>Fig. 4 .</head><label>4</label><figDesc><div><p xml:id="_mdFeWmX"><s xml:id="_BJNXWSw" coords="12,45.77,236.14,197.08,8.07">Fig. 4. Data tiering with online application guidance.</s><s xml:id="_h7b5GMh" coords="12,245.33,236.14,194.98,8.07;12,45.77,247.11,196.96,8.07">(a) Users first compile the application with a custom pass to insert annotations at each allocation call site.</s><s xml:id="_uzB5afR" coords="12,245.22,247.11,196.55,8.07;12,45.77,258.06,394.66,8.07;12,45.77,269.02,321.46,8.07">(b) Program execution proceeds inside a custom runtime layer, which automatically profiles memory usage behavior, converts it into tier recommendations for each allocation site, and enforces these recommendations during program execution.</s><s xml:id="_EtQeKjE" coords="12,370.50,269.02,69.81,8.07;12,45.77,279.98,394.52,8.07;12,45.77,290.94,273.51,8.07">In (b), interactions and operations drawn with dashed lines only occur at regular, timer-based intervals, while the solid lines correspond to activities that can occur throughout the program execution.</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3" coords="22,46.37,254.14,393.34,8.07"><head>Fig. 9 .</head><label>9</label><figDesc><div><p xml:id="_P557kQu"><s xml:id="_5kMGEYR" coords="22,46.37,254.14,21.68,8.07">Fig.9.</s><s xml:id="_BzrpKDd" coords="22,72.54,254.14,367.17,8.07">Performance (throughput) of CORAL benchmarks with large and huge input sizes (higher is better).</s></p></div></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" coords="13,56.01,81.19,376.90,203.28"><head>Table 1 .</head><label>1</label><figDesc><div><p xml:id="_6EUQYmF"><s xml:id="_bvAfjBr" coords="13,191.15,81.19,136.30,8.07">Workload Descriptions and Statistics</s></p></div></figDesc><table coords="13,56.01,98.17,376.90,186.30"><row><cell></cell><cell></cell><cell></cell><cell>CORAL</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Application</cell><cell>Description</cell><cell>Input</cell><cell>Input Arguments</cell><cell>FoM</cell><cell>Time</cell><cell>GB</cell><cell>Sites</cell></row><row><cell></cell><cell>Hydrodynamics stencil calculation, very</cell><cell>Medium</cell><cell>-s 400 -i 6 -r 11 -b 0 -c 64 -p</cell><cell>1,066.93</cell><cell>6.2 m</cell><cell>66.2</cell><cell>87</cell></row><row><cell>LULESH</cell><cell>little communication between computa-</cell><cell>Large</cell><cell>-s 800 -i 3 -r 11 -b 0 -c 64 -p</cell><cell>103.13</cell><cell>4.2 h</cell><cell>522.9</cell><cell>87</cell></row><row><cell></cell><cell>tional units. FoM: zones per second</cell><cell>Huge</cell><cell>-s 850 -i 3 -r 11 -b 0 -c 64 -p</cell><cell>120.1</cell><cell>4.3 h</cell><cell>627.3</cell><cell>87</cell></row><row><cell></cell><cell>Parallel algebraic multigrid solver for lin-</cell><cell>Medium</cell><cell>-problem 2 -n 340 340 340</cell><cell>5.66E8</cell><cell>7.7 m</cell><cell>72.2</cell><cell>209</cell></row><row><cell>AMG</cell><cell>ear systems on unstructured grids. FoM:</cell><cell>Large</cell><cell>-problem 2 -n 520 520 520</cell><cell>4.36E8</cell><cell>35.7 m</cell><cell>260.4</cell><cell>209</cell></row><row><cell></cell><cell>(nnz  *  (iter s + steps ))/seconds</cell><cell>Huge</cell><cell>-problem 2 -n 600 600 600</cell><cell>3.06E8</cell><cell>1.3 h</cell><cell>392.4</cell><cell>209</cell></row><row><cell></cell><cell>Mimics the computational needs of PAR-</cell><cell>Medium</cell><cell>nx=272, ny=102, nz=68</cell><cell>6.0E-2</cell><cell>12.9 m</cell><cell>61.4</cell><cell>87</cell></row><row><cell>SNAP</cell><cell>TISN, a Boltzmann transport equation</cell><cell>Large</cell><cell>nx=272, ny=272, nz=120</cell><cell>2.6E-2</cell><cell>2.3 h</cell><cell>288.8</cell><cell>90</cell></row><row><cell></cell><cell>solver. FoM: inverse of grind time (ns)</cell><cell>Huge</cell><cell>nx=272, ny=272, nz=192</cell><cell>2.4E-2</cell><cell>3.9 h</cell><cell>462.1</cell><cell>90</cell></row><row><cell></cell><cell>Quantum Monte Carlo simulation of</cell><cell>Medium</cell><cell>NiO S64, 40 walkers, VMC with drift</cell><cell>6.0E-2</cell><cell>10.2 m</cell><cell>16.5</cell><cell>1408</cell></row><row><cell>QMCPACK</cell><cell>the electronic structure of atoms. FoM:</cell><cell>Large</cell><cell>NiO S128, 40 walkers, VMC with drift</cell><cell>1.3E-3</cell><cell>10.4 h</cell><cell>357.0</cell><cell>1402</cell></row><row><cell></cell><cell>(blocks  *  steps  *  Nw )/seconds</cell><cell>Huge</cell><cell>NiO S256, 48 walkers, VMC with drift</cell><cell>3.3E-4</cell><cell>40.0 h</cell><cell>375.9</cell><cell>1408</cell></row><row><cell></cell><cell></cell><cell cols="2">SPEC ® CPU 2017</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Application</cell><cell></cell><cell cols="2">Description</cell><cell></cell><cell cols="3">Time GB Sites</cell></row><row><cell>603.bwaves_s</cell><cell cols="3">Numerically simulates blast waves in three dimensional transonic transient laminar viscous flow.</cell><cell></cell><cell cols="2">1.9 m 11.4</cell><cell>34</cell></row><row><cell cols="5">607.cactuBSSN_s Based on Cactus Computational Framework, uses EinsteinToolkit to solve Einstein's equations in a vacuum.</cell><cell>2.7 m</cell><cell>6.6</cell><cell>809</cell></row><row><cell>621.wrf_s</cell><cell cols="4">Weather Research and Forecasting (WRF) Model, simulates one day of the Jan. 2000 North American Blizzard.</cell><cell>3.1 m</cell><cell>0.2</cell><cell>4869</cell></row><row><cell>627.cam4_s</cell><cell cols="4">Community Atmosphere Model (CAM), atmospheric component for Community Earth System Model (CESM).</cell><cell>7.6 m</cell><cell>1.2</cell><cell>1691</cell></row><row><cell>628.pop2_s</cell><cell cols="4">Parallel Ocean Program (POP), simultaneously simulates earth's atmosphere, ocean, land surface and sea-ice.</cell><cell>3.6 m</cell><cell>1.5</cell><cell>1107</cell></row><row><cell>638.imagick_s</cell><cell cols="4">Performs various operations to transform an input image and compares the result to a reference image.</cell><cell>5.4 m</cell><cell>6.9</cell><cell>4</cell></row><row><cell>644.nab_s</cell><cell cols="4">Nucleic Acid Builder (NAB), performs FP calculations that occur commonly in life science computation.</cell><cell>3.2 m</cell><cell>0.6</cell><cell>88</cell></row><row><cell>649.fotonik3d_s</cell><cell cols="4">Computes transmission coefficient of a photonic waveguide using the FDTD method for Maxwell's equations.</cell><cell>3.2 m</cell><cell>9.5</cell><cell>127</cell></row><row><cell>654.roms_s</cell><cell cols="4">Regional Ocean Modeling System, forecasts water temperature, ocean currents, salinity, and sea surface height.</cell><cell cols="2">4.9 m 10.2</cell><cell>395</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" coords="16,106.70,81.19,272.40,74.47"><head>Table 2 .</head><label>2</label><figDesc><div><p><s xml:id="_ZzADceN" coords="16,139.03,81.19,240.07,8.07;16,145.15,92.14,122.16,8.07">[58] and Maximum Time (in Seconds) to Collect a Single Profile Using the Offline Profiler[58]vs.</s><s xml:id="_d7qJUBX" coords="16,269.56,92.14,71.35,8.07">Our Online Profiler</s></p></div></figDesc><table coords="16,114.42,109.97,259.71,45.69"><row><cell>Benchmark Set</cell><cell cols="4">Mean Profile Time (s) Max Profile Time (s) Offline [58] Online Offline [58] Online</cell></row><row><cell>CORAL-medium</cell><cell>6.058</cell><cell>0.521</cell><cell>16.897</cell><cell>1.633</cell></row><row><cell>SPEC ® CPU</cell><cell>2.260</cell><cell>0.205</cell><cell>6.463</cell><cell>0.542</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" coords="20,126.13,423.87,234.95,205.98"><head>Table 3 .</head><label>3</label><figDesc><div><p xml:id="_dPKkVqk"><s xml:id="_PyJtZUq" coords="20,168.92,423.87,180.39,8.07;20,126.13,434.83,231.29,8.07">Execution Time of Unguided, Offline, and Online Approaches alongside Migration Rate during Online Execution</s></p></div></figDesc><table coords="20,127.49,452.65,233.59,177.19"><row><cell>Application</cell><cell cols="3">Execution Time (s) Unguided Offline Online</cell><cell>MB/sec</cell></row><row><cell>LULESH</cell><cell>2,097</cell><cell>467</cell><cell>611</cell><cell>197.5</cell></row><row><cell>AMG</cell><cell>974</cell><cell>669</cell><cell>817</cell><cell>338.9</cell></row><row><cell>SNAP</cell><cell>1,659</cell><cell>901</cell><cell>1,206</cell><cell>136.3</cell></row><row><cell>QMCPACK</cell><cell>5,895</cell><cell>1,181</cell><cell>882</cell><cell>26.8</cell></row><row><cell>603.bwaves_s</cell><cell>413</cell><cell>270</cell><cell>348</cell><cell>99.5</cell></row><row><cell>607.cactuBSSN_s</cell><cell>435</cell><cell>435</cell><cell>475</cell><cell>14.1</cell></row><row><cell>621.wrf_s</cell><cell>379</cell><cell>370</cell><cell>392</cell><cell>0.4</cell></row><row><cell>627.cam4_s</cell><cell>1,799</cell><cell>1,466</cell><cell>1,672</cell><cell>3.7</cell></row><row><cell>628.pop2_s</cell><cell>701</cell><cell>628</cell><cell>610</cell><cell>21.9</cell></row><row><cell>638.imagick_s</cell><cell>400</cell><cell>400</cell><cell>429</cell><cell>16.5</cell></row><row><cell>644.nab_s</cell><cell>193</cell><cell>193</cell><cell>205</cell><cell>3.3</cell></row><row><cell>649.fotonik3d_s</cell><cell>1,473</cell><cell>1,343</cell><cell>1,282</cell><cell>146.7</cell></row><row><cell>654.roms_s</cell><cell>1,002</cell><cell>659</cell><cell>788</cell><cell>225.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0" coords="2,45.50,669.90,365.42,7.22"><p xml:id="_DZMnfbk"><s xml:id="_5rgM8Kz" coords="2,45.50,669.90,275.44,7.22">ACM Transactions on Architecture and Code Optimization, Vol. 19, No. 3, Article 45.</s><s xml:id="_2gG4nuS" coords="2,322.91,669.90,88.00,7.22">Publication date: July 2022.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1" coords="3,49.18,624.61,392.16,7.22;3,45.94,634.56,395.72,7.22;3,45.94,644.52,228.98,7.22;3,75.03,669.90,346.85,7.22"><p xml:id="_gHX9kbe"><s xml:id="_czsGwGa" coords="3,49.18,624.61,392.16,7.22;3,45.94,634.56,395.72,7.22">Described in Section 3.1, the SICM project, which is part of the U.S. Department of Energy Exascale Computing Project<ref type="bibr" coords="3,426.50,624.61,11.88,7.22" target="#b38">[39]</ref>, is a memory allocator and runtime system designed to facilitate usage of HPC applications on complex memory machines.</s><s xml:id="_qWakFx7" coords="3,45.94,644.52,228.98,7.22;3,75.03,669.90,275.44,7.22">Its source code adopts and extends the popular jemalloc allocator<ref type="bibr" coords="3,260.09,644.52,11.86,7.22" target="#b20">[21]</ref>.ACM Transactions onArchitecture and Code Optimization, Vol. 19, No. 3, Article 45.</s><s xml:id="_MtysgQj" coords="3,352.45,669.90,69.43,7.22">Publication date: July</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2022" xml:id="foot_2" coords=""><p xml:id="_nUyHJ8x"></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3" coords="9,49.32,634.56,391.38,7.22;9,45.94,644.52,175.33,7.22"><p xml:id="_vmGm9hr"><s xml:id="_j4g4Cg6" coords="9,49.32,634.56,391.38,7.22;9,45.94,644.52,175.33,7.22">This instrumentation is actually straightforward to implement in recent Linux kernels as it follows existing code to track of the resident set size of each memory control group.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4" coords="9,49.32,654.49,382.21,7.22;9,75.03,669.90,264.27,7.22"><p xml:id="_cRbGXvN"><s xml:id="_fEU8dqw" coords="9,49.32,654.49,382.21,7.22;9,75.03,669.90,264.27,7.22">It is important to note that the applications we selected for our evaluation did not reach this limit in our experiments.ACM Transactions on Architecture and Code Optimization, Vol. 19, No. 3, Article</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="45" xml:id="foot_5" coords="9,352.45,669.90,88.00,7.22"><p xml:id="_eTFH9uF"><s xml:id="_7zWpUw8" coords="9,352.45,669.90,88.00,7.22">Publication date: July 2022.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_6" coords="13,49.18,622.93,391.28,7.22;13,45.65,632.85,394.81,7.26;13,45.94,642.86,259.73,7.22;13,75.03,669.90,365.42,7.22"><p xml:id="_Kvc4NFR"><s xml:id="_R5uzFM8" coords="13,49.18,622.93,391.28,7.22;13,45.65,632.85,394.81,7.26;13,45.94,642.86,259.73,7.22;13,75.03,669.90,275.44,7.22">Specifically, our study includes all of the multi-threaded (6xx ) FP benchmarks in SPEC CPU 2017 with OpenMP directives with the exception of 619.lbm_s, which is omitted, because it only allocates a single, large heap object throughout its entire run, and is therefore not likely to exhibit benefits with guided object placement.ACM Transactions onArchitecture and Code Optimization, Vol. 19, No. 3, Article 45.</s><s xml:id="_5gCVrXx" coords="13,352.45,669.90,88.00,7.22">Publication date: July 2022.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_7" coords="14,49.14,623.02,391.16,7.22;14,45.77,632.98,394.51,7.22;14,45.77,642.95,185.28,7.22;14,45.50,669.90,365.42,7.22"><p xml:id="_ryQm7Wy"><s xml:id="_4qds8PB" coords="14,49.14,623.02,391.16,7.22;14,45.77,632.98,139.79,7.22">Setting oversize_threshold to 0 disables a feature of jemalloc that allocates objects larger than a specific size to a dedicated arena (to reduce fragmentation).</s><s xml:id="_a87mRsc" coords="14,187.91,632.98,252.38,7.22;14,45.77,642.95,185.28,7.22;14,45.50,669.90,275.44,7.22">The other two parameters control the number of background threads, which enable jemalloc to purge unused pages asynchronously.ACM Transactions onArchitecture and Code Optimization, Vol. 19, No. 3, Article 45.</s><s xml:id="_rBW2nmW" coords="14,322.91,669.90,88.00,7.22">Publication date: July 2022.</s></p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_8" coords="21,49.32,643.95,347.24,7.26"><p xml:id="_p2pHs3t"><s xml:id="_7AsEdW9" coords="21,49.32,643.95,347.24,7.26">In Intel's literature, the hardware-managed caching option for our platform is referred to as memory mode.</s></p></note>
		</body>
		<back>

			<div type="funding">
<div xml:id="_hvFt9Jv"><p xml:id="_hf9sEsz">This work was supported by the <rs type="funder">Exascale Computing Project</rs> (<rs type="grantNumber">17-SC-20-SC</rs>), a collaborative effort of the <rs type="funder">U.S. Department of Energy Office of Science</rs> and the <rs type="funder">National Nuclear Security Administration</rs>, we well as the <rs type="funder">National Science Foundation</rs> under <rs type="grantNumber">CNS-1943305</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8zFyFF5">
					<idno type="grant-number">17-SC-20-SC</idno>
				</org>
				<org type="funding" xml:id="_2CcysVT">
					<idno type="grant-number">CNS-1943305</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" xml:id="_cxDtcRZ"><p xml:id="_WufPRn2"><s xml:id="_KkszSac" coords="27,45.95,55.82,271.61,8.97;27,419.75,55.82,20.72,8.97">Online Application Guidance for Heterogeneous Memory Systems 45:27</s></p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct coords="23,63.02,484.59,377.44,7.22;23,63.02,494.52,335.48,7.26" xml:id="b0">
	<analytic>
		<title level="a" type="main" coord="23,388.57,484.59,51.88,7.22;23,63.02,494.56,191.64,7.22" xml:id="_rFxQaZ8">Page placement strategies for GPUs within heterogeneous memory systems</title>
		<author>
			<persName coords=""><forename type="first">Neha</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Nellans</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mike O'</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">W</forename><surname>Keckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_n7Gh9Mm" coord="23,260.10,494.52,44.95,7.26">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="607" to="618" />
			<date type="published" when="2015-03">2015. March 2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Neha Agarwal, David Nellans, Mark Stephenson, Mike O&apos;Connor, and Stephen W. Keckler. 2015. Page placement strategies for GPUs within heterogeneous memory systems. SIGPLAN Not. 50, 4 (March 2015), 607-618.</note>
</biblStruct>

<biblStruct coords="23,63.02,504.52,377.47,7.22;23,63.02,514.44,377.44,7.26;23,63.02,524.40,217.66,7.26" xml:id="b1">
	<analytic>
		<title level="a" type="main" coord="23,211.61,504.52,228.88,7.22;23,63.02,514.48,42.57,7.22" xml:id="_mdDtjdC">Thermostat: Application-transparent page management for two-tiered main memory</title>
		<author>
			<persName coords=""><forename type="first">Neha</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Thomas</forename><forename type="middle">F</forename><surname>Wenisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_G8tsX4W" coord="23,119.57,514.44,320.89,7.26;23,63.02,524.40,113.37,7.26">Proceedings of the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;17)</title>
		<meeting>the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;17)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="631" to="644" />
		</imprint>
	</monogr>
	<note type="raw_reference">Neha Agarwal and Thomas F. Wenisch. 2017. Thermostat: Application-transparent page management for two-tiered main memory. In Proceedings of the 22nd International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;17). ACM, New York, NY, 631-644.</note>
</biblStruct>

<biblStruct coords="23,63.02,534.37,378.32,7.26;23,63.02,544.33,269.17,7.26" xml:id="b2">
	<analytic>
		<title level="a" type="main" coord="23,133.78,534.41,238.70,7.22" xml:id="_D565zMX">Performance evaluation of intel optane memory for managed workloads</title>
		<author>
			<persName coords=""><forename type="first">Shoaib</forename><surname>Akram</surname></persName>
		</author>
		<idno type="DOI">10.1145/3451342</idno>
		<ptr target="https://doi.org/10.1145/3451342" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_KT8Mz55" coord="23,378.73,534.37,62.61,7.26;23,63.02,544.33,35.82,7.26;23,123.09,544.37,22.26,7.22">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">26</biblScope>
			<date type="published" when="2021-04">2021. April 2021</date>
		</imprint>
	</monogr>
	<note>Article</note>
	<note type="raw_reference">Shoaib Akram. 2021. Performance evaluation of intel optane memory for managed workloads. ACM Trans. Archit. Code Optim. 18, 3, Article 29 (April 2021), 26 pages. https://doi.org/10.1145/3451342</note>
</biblStruct>

<biblStruct coords="23,63.02,554.29,378.38,7.26;23,62.77,564.29,325.85,7.22" xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName coords=""><surname>Anl</surname></persName>
		</author>
		<ptr target="https://www.anl.gov/article/us-department-of-energy-and-intel-to-deliver-first-exascale-supercomputer" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>U.S. Department of Energy and Intel to Deliver First Exascale Supercomputer. ANL</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">ANL. 2019. U.S. Department of Energy and Intel to Deliver First Exascale Supercomputer. ANL. Retrieved from https: //www.anl.gov/article/us-department-of-energy-and-intel-to-deliver-first-exascale-supercomputer.</note>
</biblStruct>

<biblStruct coords="23,63.02,574.26,378.80,7.22;23,63.02,584.18,377.43,7.26;23,62.53,594.14,172.08,7.26" xml:id="b4">
	<analytic>
		<title level="a" type="main" coord="23,261.17,574.26,180.65,7.22;23,63.02,584.22,86.88,7.22" xml:id="_mbCgP9y">Resource containers: A new facility for resource management in server systems</title>
		<author>
			<persName coords=""><forename type="first">Gaurav</forename><surname>Banga</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Peter</forename><surname>Druschel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeffrey</forename><forename type="middle">C</forename><surname>Mogul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_prcttd6" coord="23,165.52,584.18,274.93,7.26;23,62.53,594.14,29.15,7.26">Proceedings of the 3rd Symposium on Operating Systems Design and Implementation (OSDI&apos;99)</title>
		<meeting>the 3rd Symposium on Operating Systems Design and Implementation (OSDI&apos;99)<address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="45" to="58" />
		</imprint>
	</monogr>
	<note type="raw_reference">Gaurav Banga, Peter Druschel, and Jeffrey C. Mogul. 1999. Resource containers: A new facility for resource man- agement in server systems. In Proceedings of the 3rd Symposium on Operating Systems Design and Implementation (OSDI&apos;99). USENIX Association, Berkeley, CA, 45-58.</note>
</biblStruct>

<biblStruct coords="23,63.02,604.11,377.45,7.26;23,63.02,614.07,353.38,7.26" xml:id="b5">
	<analytic>
		<title level="a" type="main" coord="23,216.19,604.15,132.64,7.22" xml:id="_QPdWaQD">Jigsaw: Scalable software-defined caches</title>
		<author>
			<persName coords=""><forename type="first">Nathan</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_MBD87Zh" coord="23,364.37,604.11,76.10,7.26;23,63.02,614.07,277.88,7.26">Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques (PACT&apos;13</title>
		<meeting>the 22nd International Conference on Parallel Architectures and Compilation Techniques (PACT&apos;13</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="213" to="224" />
		</imprint>
	</monogr>
	<note type="raw_reference">Nathan Beckmann and Daniel Sanchez. 2013. Jigsaw: Scalable software-defined caches. In Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques (PACT&apos;13). IEEE Press, 213-224.</note>
</biblStruct>

<biblStruct coords="23,63.02,624.07,377.45,7.22;23,63.02,633.99,377.45,7.26;23,63.02,643.95,245.31,7.26" xml:id="b6">
	<analytic>
		<title level="a" type="main" coord="23,405.40,624.07,35.07,7.22;23,63.02,634.03,139.12,7.22" xml:id="_TJ8EzCr">Dune: Safe user-level access to privileged CPU features</title>
		<author>
			<persName coords=""><forename type="first">Adam</forename><surname>Belay</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrea</forename><surname>Bittau</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Mashtizadeh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Terei</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">David</forename><surname>Mazières</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Christos</forename><surname>Kozyrakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_pEbhjq5" coord="23,215.15,633.99,225.31,7.26;23,63.02,643.95,94.97,7.26">Proceedings of the 10th USENIX conference on Operating Systems Design and Implementation (OSDI&apos;12)</title>
		<meeting>the 10th USENIX conference on Operating Systems Design and Implementation (OSDI&apos;12)<address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="335" to="348" />
		</imprint>
	</monogr>
	<note type="raw_reference">Adam Belay, Andrea Bittau, Ali Mashtizadeh, David Terei, David Mazières, and Christos Kozyrakis. 2012. Dune: Safe user-level access to privileged CPU features. In Proceedings of the 10th USENIX conference on Operating Systems Design and Implementation (OSDI&apos;12). USENIX Association, Berkeley, CA, 335-348.</note>
</biblStruct>

<biblStruct coords="24,62.85,84.09,377.43,7.22;24,62.85,94.02,377.60,7.26;24,62.85,104.02,279.20,7.22" xml:id="b7">
	<analytic>
		<title level="a" type="main" coord="24,259.12,84.09,181.16,7.22;24,62.85,94.06,49.83,7.22" xml:id="_nx7Jsc3">PAYJIT: Space-optimal JIT compilation and its practical implementation</title>
		<author>
			<persName coords=""><forename type="first">Jacob</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaoran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3178372.3179523</idno>
		<ptr target="https://doi.org/10.1145/3178372.3179523" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_3xM6G37" coord="24,126.77,94.02,259.16,7.26">Proceedings of the 27th International Conference on Compiler Construction (CC&apos;18)</title>
		<meeting>the 27th International Conference on Compiler Construction (CC&apos;18)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="71" to="81" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jacob Brock, Chen Ding, Xiaoran Xu, and Yan Zhang. 2018. PAYJIT: Space-optimal JIT compilation and its practical implementation. In Proceedings of the 27th International Conference on Compiler Construction (CC&apos;18). Association for Computing Machinery, New York, NY, 71-81. https://doi.org/10.1145/3178372.3179523</note>
</biblStruct>

<biblStruct coords="24,62.85,113.94,378.32,7.26;24,62.66,123.94,213.07,7.22" xml:id="b8">
	<analytic>
		<title level="a" type="main" coord="24,275.98,113.98,103.92,7.22" xml:id="_qAXhxGY">Cache-conscious data placement</title>
		<author>
			<persName coords=""><forename type="first">Brad</forename><surname>Calder</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chandra</forename><surname>Krintz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simmi</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Todd</forename><surname>Austin</surname></persName>
		</author>
		<idno type="DOI">10.1145/291006.291036</idno>
		<ptr target="https://doi.org/10.1145/291006.291036" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_DB8Yw4T" coord="24,385.30,113.94,44.83,7.26">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="139" to="149" />
			<date type="published" when="1998-10">1998. October 1998</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Brad Calder, Chandra Krintz, Simmi John, and Todd Austin. 1998. Cache-conscious data placement. SIGPLAN Not. 33, 11 (October 1998), 139-149. https://doi.org/10.1145/291006.291036</note>
</biblStruct>

<biblStruct coords="24,62.85,133.91,378.69,7.22;24,62.85,143.87,377.65,7.22;24,62.85,153.83,198.81,7.22" xml:id="b9">
	<monogr>
		<title level="m" type="main" coord="24,81.97,143.87,358.53,7.22;24,62.85,153.83,23.63,7.22" xml:id="_TyvqrXq">memkind: An Extensible Heap Memory Manager for Heterogeneous Memory Platforms and Mixed Memory Policies</title>
		<author>
			<persName coords=""><forename type="first">Christopher</forename><surname>Cantalupo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vishwanath</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krzysztof</forename><surname>Czurlyo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Simon</forename><forename type="middle">David</forename><surname>Hammond</surname></persName>
		</author>
		<ptr target="https://www.osti.gov/biblio/1245908" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Christopher Cantalupo, Vishwanath Venkatesan, Jeff Hammond, Krzysztof Czurlyo, and Simon David Hammond. 2015. memkind: An Extensible Heap Memory Manager for Heterogeneous Memory Platforms and Mixed Memory Policies. Retrieved from https://www.osti.gov/biblio/1245908.</note>
</biblStruct>

<biblStruct coords="24,62.85,163.75,377.45,7.26;24,62.85,173.72,257.93,7.26" xml:id="b10">
	<analytic>
		<title level="a" type="main" coord="24,197.90,163.79,172.38,7.22" xml:id="_UjuK8fu">Region analysis and transformation for java programs</title>
		<author>
			<persName coords=""><forename type="first">Sigmund</forename><surname>Cherem</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Radu</forename><surname>Rugina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_nBGZ39a" coord="24,384.13,163.75,56.17,7.26;24,62.85,173.72,161.10,7.26">Proceedings of the 4th Intl. Symp. on Memory Management (ISMM&apos;04)</title>
		<meeting>the 4th Intl. Symp. on Memory Management (ISMM&apos;04)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="85" to="96" />
		</imprint>
	</monogr>
	<note type="raw_reference">Sigmund Cherem and Radu Rugina. 2004. Region analysis and transformation for java programs. In Proceedings of the 4th Intl. Symp. on Memory Management (ISMM&apos;04). ACM, New York, NY, 85-96.</note>
</biblStruct>

<biblStruct coords="24,62.85,183.68,377.45,7.26;24,62.85,193.64,378.31,7.26;24,62.85,203.65,159.68,7.22" xml:id="b11">
	<analytic>
		<title level="a" type="main" coord="24,208.14,183.72,160.20,7.22" xml:id="_ZUAunK5">Cache-conscious coallocation of hot data streams</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Trishul</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ran</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Shaham</surname></persName>
		</author>
		<idno type="DOI">10.1145/1133981.1134011</idno>
		<ptr target="https://doi.org/10.1145/1133981.1134011" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_KwsN9XG" coord="24,383.00,183.68,57.30,7.26;24,62.85,193.64,301.01,7.26">Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI&apos;06)</title>
		<meeting>the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI&apos;06)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
	<note type="raw_reference">Trishul M. Chilimbi and Ran Shaham. 2006. Cache-conscious coallocation of hot data streams. In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI&apos;06). ACM, New York, NY, 252-262. https://doi.org/10.1145/1133981.1134011</note>
</biblStruct>

<biblStruct coords="24,62.84,213.61,377.45,7.22;24,62.85,223.53,377.46,7.26;24,62.85,233.49,162.42,7.26" xml:id="b12">
	<analytic>
		<title level="a" type="main" coord="24,278.04,213.61,162.26,7.22;24,62.85,223.57,226.62,7.22" xml:id="_GaPDEPT">CAMEO: A two-level memory organization with capacity of main memory and flexibility of hardware-managed cache</title>
		<author>
			<persName coords=""><forename type="first">Chiachen</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Aamer</forename><surname>Jaleel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Moinuddin</forename><forename type="middle">K</forename><surname>Qureshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_neeQWFa" coord="24,304.53,223.53,135.77,7.26;24,62.85,233.49,101.26,7.26">Proceedings of the 47th Annual IEEE/ACM Intl. Symp. on Microarchitecture</title>
		<meeting>the 47th Annual IEEE/ACM Intl. Symp. on Microarchitecture</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chiachen Chou, Aamer Jaleel, and Moinuddin K. Qureshi. 2014. CAMEO: A two-level memory organization with capacity of main memory and flexibility of hardware-managed cache. In Proceedings of the 47th Annual IEEE/ACM Intl. Symp. on Microarchitecture (MICRO-47). 1-12.</note>
</biblStruct>

<biblStruct coords="24,62.85,243.50,377.45,7.22;24,62.55,253.46,374.08,7.22" xml:id="b13">
	<monogr>
		<title level="m" type="main" coord="24,123.48,243.50,261.12,7.22" xml:id="_NQkSdKN">Intel to Launch Next-Gen Sapphire Rapids Xeon with High-Bandwidth Memory</title>
		<ptr target="www.anandtech.com/show/16795/intel-to-launch-next-gen-sapphire-rapids-xeon-with-high-bandwidth-memory" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Ian Cutress</publisher>
		</imprint>
	</monogr>
	<note type="raw_reference">Ian Cutress. 2021. Intel to Launch Next-Gen Sapphire Rapids Xeon with High-Bandwidth Memory. Retrieved from www.anandtech.com/show/16795/intel-to-launch-next-gen-sapphire-rapids-xeon-with-high-bandwidth-memory.</note>
</biblStruct>

<biblStruct coords="24,62.85,263.42,377.45,7.22;24,62.85,273.38,378.66,7.22;24,62.85,283.31,338.40,7.26" xml:id="b14">
	<analytic>
		<title level="a" type="main" coord="24,164.14,273.38,273.92,7.22" xml:id="_B4GAZt2">Traffic management: A holistic approach to memory placement on NUMA systems</title>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Dashti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alexandra</forename><surname>Fedorova</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Funston</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Fabien</forename><surname>Gaud</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Renaud</forename><surname>Lachaize</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Baptiste</forename><surname>Lepers</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vivien</forename><surname>Quema</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_ZdgEHxu" coord="24,62.85,283.31,80.24,7.26">In ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="381" to="394" />
			<date type="published" when="2013">2013</date>
			<publisher>Association for Computing Machinery</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Mohammad Dashti, Alexandra Fedorova, Justin Funston, Fabien Gaud, Renaud Lachaize, Baptiste Lepers, Vivien Quema, and Mark Roth. 2013. Traffic management: A holistic approach to memory placement on NUMA systems. In ACM SIGPLAN Notices, Vol. 48. ACM, Association for Computing Machinery, New York, NY, 381-394.</note>
</biblStruct>

<biblStruct coords="24,62.85,293.30,378.30,7.22;24,62.71,303.22,377.57,7.26;24,62.85,313.19,296.39,7.26" xml:id="b15">
	<analytic>
		<title level="a" type="main" coord="24,196.21,303.26,155.15,7.22" xml:id="_rYuzjfr">Data tiering in heterogeneous memory systems</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amitabha</forename><surname>Dulloor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zheguang</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Narayanan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nadathur</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Rajesh</forename><surname>Satish</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeff</forename><surname>Sankaran</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karsten</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Schwan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Wgm7dFj" coord="24,366.49,303.22,73.80,7.26;24,62.85,313.19,176.63,7.26">Proceedings of the 11th European Conference on Computer Systems (EuroSys&apos;16)</title>
		<meeting>the 11th European Conference on Computer Systems (EuroSys&apos;16)<address><addrLine>New York, NY, Article</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Subramanya R. Dulloor, Amitabha Roy, Zheguang Zhao, Narayanan Sundaram, Nadathur Satish, Rajesh Sankaran, Jeff Jackson, and Karsten Schwan. 2016. Data tiering in heterogeneous memory systems. In Proceedings of the 11th European Conference on Computer Systems (EuroSys&apos;16). New York, NY, Article 15, 16 pages.</note>
</biblStruct>

<biblStruct coords="24,62.85,323.19,377.47,7.22;24,62.85,333.11,377.44,7.26;24,62.85,343.07,64.22,7.26" xml:id="b16">
	<analytic>
		<title level="a" type="main" coord="24,430.40,323.19,9.91,7.22;24,62.85,333.15,205.47,7.22" xml:id="_nw78NGH">On automated feedback-driven data placement in hybrid memories</title>
		<author>
			<persName coords=""><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chad</forename><surname>Effler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Adam</forename><forename type="middle">P</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Jantz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kshitij</forename><forename type="middle">A</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prasad</forename><forename type="middle">A</forename><surname>Kulkarni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_sHRUn8R" coord="24,283.17,333.11,157.12,7.26;24,62.85,343.07,60.86,7.26">LNCS International Conference on Architecture of Computing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">T. Chad Effler, Adam P. Howard, Tong Zhou, Michael R. Jantz, Kshitij A. Doshi, and Prasad A. Kulkarni. 2018. On automated feedback-driven data placement in hybrid memories. In LNCS International Conference on Architecture of Computing Systems.</note>
</biblStruct>

<biblStruct coords="24,62.85,353.08,377.46,7.22;24,62.85,363.00,377.41,7.26;24,62.85,372.96,341.45,7.26" xml:id="b17">
	<analytic>
		<title level="a" type="main" coord="24,249.28,353.08,191.03,7.22;24,62.85,363.04,114.89,7.22" xml:id="_9eDWhdA">Performance potential of mixed data management modes for heterogeneous memory systems</title>
		<author>
			<persName coords=""><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chad</forename><surname>Effler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Jantz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Terry</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCHPC51950.2020.00007</idno>
		<ptr target="https://doi.org/10.1109/MCHPC51950.2020.00007" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_5u6Mgeb" coord="24,192.06,363.00,248.20,7.26;24,62.85,372.96,75.03,7.26">Proceedings of the IEEE/ACM Workshop on Memory Centric High Performance Computing (MCHPC&apos;20)</title>
		<meeting>the IEEE/ACM Workshop on Memory Centric High Performance Computing (MCHPC&apos;20)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10" to="16" />
		</imprint>
	</monogr>
	<note type="raw_reference">T. Chad Effler, Michael R. Jantz, and Terry Jones. 2020. Performance potential of mixed data management modes for heterogeneous memory systems. In Proceedings of the IEEE/ACM Workshop on Memory Centric High Performance Computing (MCHPC&apos;20). IEEE Computer Society, 10-16. https://doi.org/10.1109/MCHPC51950.2020.00007</note>
</biblStruct>

<biblStruct coords="24,62.85,382.96,377.45,7.22;24,62.60,392.89,378.47,7.26;24,62.85,402.85,377.74,7.26;24,62.61,412.85,192.26,7.22" xml:id="b18">
	<analytic>
		<title level="a" type="main" coord="24,121.42,392.93,279.15,7.22" xml:id="_mW3BZpz">Evaluating the effectiveness of program data features for guiding memory management</title>
		<author>
			<persName coords=""><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chad</forename><surname>Effler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brandon</forename><surname>Kammerdiener</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Jantz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Saikat</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prasad</forename><forename type="middle">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kshitij</forename><forename type="middle">A</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Terry</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357526.3357537</idno>
		<ptr target="https://doi.org/10.1145/3357526.3357537" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_pgQAdjw" coord="24,414.56,392.89,26.52,7.26;24,62.85,402.85,226.20,7.26">Proceedings of the International Symposium on Memory Systems (MEMSYS&apos;19)</title>
		<meeting>the International Symposium on Memory Systems (MEMSYS&apos;19)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="383" to="395" />
		</imprint>
	</monogr>
	<note type="raw_reference">T. Chad Effler, Brandon Kammerdiener, Michael R. Jantz, Saikat Sengupta, Prasad A. Kulkarni, Kshitij A. Doshi, and Terry Jones. 2019. Evaluating the effectiveness of program data features for guiding memory management. In Proceed- ings of the International Symposium on Memory Systems (MEMSYS&apos;19). Association for Computing Machinery, New York, NY, 383-395. https://doi.org/10.1145/3357526.3357537</note>
</biblStruct>

<biblStruct coords="24,62.85,422.81,377.46,7.22;24,62.85,432.74,254.99,7.26" xml:id="b19">
	<analytic>
		<title level="a" type="main" coord="24,229.74,422.81,210.57,7.22;24,62.85,432.78,69.61,7.22" xml:id="_mF2DgYP">Exokernel: An operating system architecture for application-level resource management</title>
		<author>
			<persName coords=""><forename type="first">D</forename><forename type="middle">R</forename><surname>Engler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">F</forename><surname>Kaashoek</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jr</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_sn5z5tc" coord="24,138.42,432.74,73.87,7.26">SIGOPS Oper. Syst. Rev</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="251" to="266" />
			<date type="published" when="1995-12">1995. December 1995</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. R. Engler, M. F. Kaashoek, and J. O&apos;Toole, Jr. 1995. Exokernel: An operating system architecture for application-level resource management. SIGOPS Oper. Syst. Rev. 29, 5 (December 1995), 251-266.</note>
</biblStruct>

<biblStruct coords="24,62.85,442.70,377.44,7.26;24,62.85,452.66,134.67,7.26" xml:id="b20">
	<analytic>
		<title level="a" type="main" coord="24,125.56,442.74,203.35,7.22" xml:id="_wmHz9Q7">A Scalable Concurrent malloc (3) Implementation for FreeBSD</title>
		<author>
			<persName coords=""><forename type="first">Jason</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_tEaDAH6" coord="24,344.18,442.70,96.10,7.26;24,62.85,452.66,75.36,7.26">the 3rd Annual Technical BSD Conference (BSDCan&apos;06)</title>
		<meeting><address><addrLine>Ottawa, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Jason Evans. 2006. A Scalable Concurrent malloc (3) Implementation for FreeBSD. In the 3rd Annual Technical BSD Conference (BSDCan&apos;06). Ottawa, Canada.</note>
</biblStruct>

<biblStruct coords="24,62.85,462.63,362.61,7.26" xml:id="b21">
	<analytic>
		<title level="a" type="main" coord="24,128.91,462.63,102.07,7.26" xml:id="_Y9c645s">A Thorough Introduction to eBPF</title>
		<author>
			<persName coords=""><forename type="first">Matt</forename><surname>Fleming</surname></persName>
		</author>
		<ptr target="https://lwn.net/Articles/740157/" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_7RG8WrU" coord="24,236.37,462.67,11.73,7.22">LWN</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Matt Fleming. 2021. A Thorough Introduction to eBPF. LWN.net. Retrieved from https://lwn.net/Articles/740157/.</note>
</biblStruct>

<biblStruct coords="24,62.85,472.63,377.44,7.22;24,62.85,482.55,377.42,7.26;24,62.85,492.51,268.56,7.26" xml:id="b22">
	<analytic>
		<title level="a" type="main" coord="24,262.91,472.63,164.48,7.22" xml:id="_hqXEa5b">Statistically rigorous java performance evaluation</title>
		<author>
			<persName coords=""><forename type="first">Andy</forename><surname>Georges</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dries</forename><surname>Buytaert</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lieven</forename><surname>Eeckhout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_CT5s7Fa" coord="24,62.85,482.55,377.42,7.26;24,62.85,492.51,192.37,7.26">Proceedings of the 22nd Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications (OOPSLA&apos;07). Assoc. for Computing Machinery</title>
		<meeting>the 22nd Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications (OOPSLA&apos;07). Assoc. for Computing Machinery<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="57" to="76" />
		</imprint>
	</monogr>
	<note type="raw_reference">Andy Georges, Dries Buytaert, and Lieven Eeckhout. 2007. Statistically rigorous java performance evaluation. In Proceedings of the 22nd Annual ACM SIGPLAN Conference on Object-Oriented Programming Systems, Languages and Applications (OOPSLA&apos;07). Assoc. for Computing Machinery, New York, NY, 57-76.</note>
</biblStruct>

<biblStruct coords="24,62.85,502.52,377.42,7.22;24,62.85,512.44,377.44,7.26;24,62.85,522.40,226.68,7.26" xml:id="b23">
	<analytic>
		<title level="a" type="main" coord="24,304.17,502.52,136.10,7.22;24,62.85,512.48,251.94,7.22" xml:id="_6XWrcUG">NightWatch: Integrating lightweight and transparent cache pollution control into dynamic memory allocation systems</title>
		<author>
			<persName coords=""><forename type="first">Rentong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiaofei</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianhui</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Guang</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_YaRZ2ed" coord="24,329.59,512.44,110.70,7.26;24,62.85,522.40,122.41,7.26">Proceedings of the USENIX Annual Technical Conference (USENIX ATC&apos;15)</title>
		<meeting>the USENIX Annual Technical Conference (USENIX ATC&apos;15)</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="307" to="318" />
		</imprint>
	</monogr>
	<note type="raw_reference">Rentong Guo, Xiaofei Liao, Hai Jin, Jianhui Yue, and Guang Tan. 2015. NightWatch: Integrating lightweight and transparent cache pollution control into dynamic memory allocation systems. In Proceedings of the USENIX Annual Technical Conference (USENIX ATC&apos;15). USENIX Association, 307-318.</note>
</biblStruct>

<biblStruct coords="24,62.85,532.40,378.65,7.22;24,62.85,542.32,377.42,7.26;24,62.85,552.28,185.14,7.26" xml:id="b24">
	<analytic>
		<title level="a" type="main" coord="24,223.18,532.40,215.10,7.22" xml:id="_fwq2mPX">Finding your cronies: Static analysis for dynamic object colocation</title>
		<author>
			<persName coords=""><forename type="first">Z</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Guyer</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Mckinley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_rAQcwZV" coord="24,71.30,542.32,368.97,7.26;24,62.85,552.28,80.77,7.26">Proceedings of the 19th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications (OOPSLA&apos;04)</title>
		<meeting>the 19th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications (OOPSLA&apos;04)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="237" to="250" />
		</imprint>
	</monogr>
	<note type="raw_reference">Samuel Z. Guyer and Kathryn S. McKinley. 2004. Finding your cronies: Static analysis for dynamic object colocation. In Proceedings of the 19th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications (OOPSLA&apos;04). ACM, New York, NY, 237-250.</note>
</biblStruct>

<biblStruct coords="24,62.85,562.24,377.44,7.26;24,62.85,572.21,378.38,7.26;24,62.59,582.21,110.88,7.22" xml:id="b25">
	<analytic>
		<title level="a" type="main" coord="24,129.42,562.28,135.15,7.22" xml:id="_HcCsJzJ">Data layouts for object-oriented programs</title>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Hirzel</surname></persName>
		</author>
		<idno type="DOI">10.1145/1254882.1254915</idno>
		<ptr target="https://doi.org/10.1145/1254882.1254915" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_zQvya6c" coord="24,278.88,562.24,161.40,7.26;24,62.85,572.21,255.58,7.26">Proceedings of the ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems (SIGMETRICS&apos;07)</title>
		<meeting>the ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems (SIGMETRICS&apos;07)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="265" to="276" />
		</imprint>
	</monogr>
	<note type="raw_reference">Martin Hirzel. 2007. Data layouts for object-oriented programs. In Proceedings of the ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems (SIGMETRICS&apos;07). ACM, New York, NY, 265-276. https: //doi.org/10.1145/1254882.1254915</note>
</biblStruct>

<biblStruct coords="24,62.84,592.17,378.68,7.22;24,62.85,602.09,377.44,7.26;24,62.85,612.06,378.31,7.26;24,62.85,622.06,152.27,7.22" xml:id="b26">
	<analytic>
		<title level="a" type="main" coord="24,81.13,602.13,200.07,7.22" xml:id="_wKrJbtM">The garbage collection advantage: Improving program locality</title>
		<author>
			<persName coords=""><forename type="first">Xianglong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stephen</forename><forename type="middle">M</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kathryn</forename><forename type="middle">S</forename><surname>Mckinley</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">B</forename><surname>Eliot</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zhenlin</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Perry</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/1028976.1028983</idno>
		<ptr target="https://doi.org/10.1145/1028976.1028983" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_xRP6X73" coord="24,294.13,602.09,146.16,7.26;24,62.85,612.06,304.46,7.26">Proceedings of the 19th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications (OOPSLA&apos;04)</title>
		<meeting>the 19th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications (OOPSLA&apos;04)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="69" to="80" />
		</imprint>
	</monogr>
	<note type="raw_reference">Xianglong Huang, Stephen M. Blackburn, Kathryn S. McKinley, J. Eliot B. Moss, Zhenlin Wang, and Perry Cheng. 2004. The garbage collection advantage: Improving program locality. In Proceedings of the 19th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications (OOPSLA&apos;04). ACM, New York, NY, 69-80. https://doi.org/10.1145/1028976.1028983</note>
</biblStruct>

<biblStruct coords="24,62.84,632.02,377.45,7.22;24,62.85,641.96,377.60,7.26;24,62.85,651.97,232.41,7.22" xml:id="b27">
	<analytic>
		<title level="a" type="main" coord="24,297.48,632.02,142.81,7.22;24,62.85,641.98,19.24,7.22" xml:id="_a7xnVE9">Practical structure layout optimization and advice</title>
		<author>
			<persName coords=""><forename type="first">Robert</forename><surname>Hundt</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Sandya</forename><surname>Mannarswamy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dhruva</forename><surname>Chakrabarti</surname></persName>
		</author>
		<idno type="DOI">10.1109/CGO.2006.29</idno>
		<ptr target="https://doi.org/10.1109/CGO.2006.29" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_Pd7z3Se" coord="24,95.65,641.96,289.06,7.26">Proceedings of the International Symposium on Code Generation and Optimization (CGO&apos;06)</title>
		<meeting>the International Symposium on Code Generation and Optimization (CGO&apos;06)<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="233" to="244" />
		</imprint>
	</monogr>
	<note type="raw_reference">Robert Hundt, Sandya Mannarswamy, and Dhruva Chakrabarti. 2006. Practical structure layout optimization and advice. In Proceedings of the International Symposium on Code Generation and Optimization (CGO&apos;06). IEEE Computer Society, Washington, DC, 233-244. https://doi.org/10.1109/CGO.2006.29</note>
</biblStruct>

<biblStruct coords="25,63.02,84.09,378.33,7.22;25,62.79,94.06,377.66,7.22;25,63.02,104.02,341.32,7.22" xml:id="b28">
	<monogr>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><surname>Izraelevitz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Juno</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Amirsaman</forename><surname>Memaripour</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yun</forename><forename type="middle">Joon</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zixuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Subramanya</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jishen</forename><surname>Dulloor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Steven</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Swanson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.05714</idno>
		<ptr target="http://arxiv.org/abs/1903.05714" />
		<title level="m" xml:id="_EURkwhX" coord="25,294.05,94.06,146.41,7.22;25,63.02,104.02,124.12,7.22">Basic performance measurements of the Intel Optane DC persistent memory module</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Joseph Izraelevitz, Jian Yang, Lu Zhang, Juno Kim, Xiao Liu, Amirsaman Memaripour, Yun Joon Soh, Zixuan Wang, Yi Xu, Subramanya R. Dulloor, Jishen Zhao, and Steven Swanson. 2019. Basic performance measurements of the Intel Optane DC persistent memory module. arXiv:1903.05714. Retrieved from http://arxiv.org/abs/1903.05714.</note>
</biblStruct>

<biblStruct coords="25,63.02,113.98,377.46,7.22;25,63.02,123.90,378.67,7.26;25,63.02,133.91,104.33,7.22" xml:id="b29">
	<analytic>
		<title level="a" type="main" coord="25,291.55,113.98,148.93,7.22;25,63.02,123.94,19.86,7.22" xml:id="_kWVKh3H">Efficient footprint caching for tagless DRAM caches</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_5j2Zm3B" coord="25,98.43,123.90,339.77,7.26">Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA&apos;16)</title>
		<meeting>the IEEE International Symposium on High Performance Computer Architecture (HPCA&apos;16)<address><addrLine>DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="237" to="248" />
		</imprint>
	</monogr>
	<note type="raw_reference">H. Jang, Y. Lee, J. Kim, Y. Kim, J. Kim, J. Jeong, and J. W. Lee. 2016. Efficient footprint caching for tagless DRAM caches. In Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA&apos;16). IEEE, Wash., DC, USA, 237-248.</note>
</biblStruct>

<biblStruct coords="25,63.02,143.87,377.48,7.22;25,63.02,153.79,377.43,7.26;25,63.02,163.75,312.62,7.26" xml:id="b30">
	<analytic>
		<title level="a" type="main" coord="25,330.80,143.87,109.69,7.22;25,63.02,153.83,113.05,7.22" xml:id="_ZWEs4MY">Cross-layer memory management for managed language applications</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Forrest</forename><forename type="middle">J</forename><surname>Jantz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prasad</forename><forename type="middle">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kshitij</forename><forename type="middle">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_B5VuzCD" coord="25,190.34,153.79,250.11,7.26;25,63.02,163.75,208.25,7.26">Proceedings of the ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA&apos;15)</title>
		<meeting>the ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA&apos;15)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="488" to="504" />
		</imprint>
	</monogr>
	<note type="raw_reference">Michael R. Jantz, Forrest J. Robinson, Prasad A. Kulkarni, and Kshitij A. Doshi. 2015. Cross-layer memory management for managed language applications. In Proceedings of the ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA&apos;15). ACM, New York, NY, 488-504.</note>
</biblStruct>

<biblStruct coords="25,63.02,173.76,378.80,7.22;25,63.02,183.68,377.47,7.26;25,63.02,193.64,333.31,7.26" xml:id="b31">
	<analytic>
		<title level="a" type="main" coord="25,366.72,173.76,75.10,7.22;25,63.02,183.72,138.62,7.22" xml:id="_bM8YnMH">A framework for application guidance in virtual memory systems</title>
		<author>
			<persName coords=""><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Carl</forename><surname>Jantz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthik</forename><surname>Strickland</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Martin</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kshitij</forename><forename type="middle">A</forename><surname>Dimitrov</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Wy75wu3" coord="25,215.64,183.68,224.85,7.26;25,63.02,193.64,138.46,7.26">Proceedings of the 9th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments (VEE&apos;13)</title>
		<meeting>the 9th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments (VEE&apos;13)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Assoc. for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="155" to="166" />
		</imprint>
	</monogr>
	<note type="raw_reference">Michael R. Jantz, Carl Strickland, Karthik Kumar, Martin Dimitrov, and Kshitij A. Doshi. 2013. A framework for appli- cation guidance in virtual memory systems. In Proceedings of the 9th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments (VEE&apos;13). Assoc. for Computing Machinery, New York, NY, 155-166.</note>
</biblStruct>

<biblStruct coords="25,63.02,203.65,377.45,7.22;25,63.02,213.57,378.33,7.26;25,62.84,223.57,195.53,7.22" xml:id="b32">
	<analytic>
		<title level="a" type="main" coord="25,248.78,203.65,191.69,7.22;25,63.02,213.61,25.36,7.22" xml:id="_zVcdFCz">Layout transformations for heap objects using static access patterns</title>
		<author>
			<persName coords=""><forename type="first">Jinseong</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Keoncheol</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hwansoo</forename><surname>Han</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1759937.1759954" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_AcuykTp" coord="25,101.90,213.57,258.18,7.26">Proceedings of the 16th International Conference on Compiler Construction (CC&apos;07)</title>
		<meeting>the 16th International Conference on Compiler Construction (CC&apos;07)<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="187" to="201" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jinseong Jeon, Keoncheol Shin, and Hwansoo Han. 2007. Layout transformations for heap objects using static access patterns. In Proceedings of the 16th International Conference on Compiler Construction (CC&apos;07). Springer-Verlag, Berlin, 187-201. http://dl.acm.org/citation.cfm?id=1759937.1759954.</note>
</biblStruct>

<biblStruct coords="25,63.02,233.53,377.46,7.22;25,63.02,243.46,378.67,7.26;25,63.02,253.46,186.22,7.22" xml:id="b33">
	<analytic>
		<title level="a" type="main" coord="25,321.82,233.53,118.66,7.22;25,63.02,243.50,83.49,7.22" xml:id="_jMUUNAG">Competitive randomized algorithms for non-uniform problems</title>
		<author>
			<persName coords=""><forename type="first">Anna</forename><forename type="middle">R</forename><surname>Karlin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Manasse</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Lyle</forename><forename type="middle">A</forename><surname>Mcgeoch</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Susan</forename><surname>Owicki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_qAYkX2N" coord="25,160.68,243.46,277.54,7.26">Proceedings of the 1st Annual ACM-SIAM Symposium on Discrete Algorithms (SODA&apos;90)</title>
		<meeting>the 1st Annual ACM-SIAM Symposium on Discrete Algorithms (SODA&apos;90)</meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="301" to="309" />
		</imprint>
	</monogr>
	<note type="raw_reference">Anna R. Karlin, Mark S. Manasse, Lyle A. McGeoch, and Susan Owicki. 1990. Competitive randomized algorithms for non-uniform problems. In Proceedings of the 1st Annual ACM-SIAM Symposium on Discrete Algorithms (SODA&apos;90). Society for Industrial and Applied Mathematics, 301-309.</note>
</biblStruct>

<biblStruct coords="25,63.02,263.42,378.83,7.22;25,63.02,273.34,377.45,7.26;25,63.02,283.31,211.39,7.26" xml:id="b34">
	<analytic>
		<title level="a" type="main" coord="25,284.29,263.42,157.56,7.22;25,63.02,273.38,127.92,7.22" xml:id="_t2RDeG9">The constrained ski-rental problem and its application to online cloud cost optimization</title>
		<author>
			<persName coords=""><forename type="first">Ali</forename><surname>Khanafer</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Murali</forename><surname>Kodialam</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Krishna</forename><forename type="middle">P N</forename><surname>Puttaswamy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_NrYN93X" coord="25,205.01,273.34,235.46,7.26;25,63.02,283.31,97.07,7.26">Proceedings of the Proceedings IEEE International Conference on Computer Communications (INFOCOM&apos;13</title>
		<meeting>the Proceedings IEEE International Conference on Computer Communications (INFOCOM&apos;13<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ali Khanafer, Murali Kodialam, and Krishna P. N. Puttaswamy. 2013. The constrained ski-rental problem and its appli- cation to online cloud cost optimization. In Proceedings of the Proceedings IEEE International Conference on Computer Communications (INFOCOM&apos;13). IEEE, New York, NY, 1492-1500.</note>
</biblStruct>

<biblStruct coords="25,63.02,293.30,378.79,7.22;25,63.02,303.22,378.78,7.26;25,63.02,313.23,329.87,7.22" xml:id="b35">
	<analytic>
		<title level="a" type="main" coord="25,252.14,293.30,189.66,7.22;25,63.02,303.26,74.04,7.22" xml:id="_tUD5YqM">Exploring the design space of page management for multitiered memory systems</title>
		<author>
			<persName coords=""><forename type="first">Jonghyeon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wonkyo</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeongseob</forename><surname>Ahn</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/atc21/presentation/kim-jonghyeon" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_J6btNP6" coord="25,150.98,303.22,290.81,7.26;25,63.02,313.23,15.34,7.22">Proceedings of the USENIX Annual Technical Conference (USENIX ATC&apos;21). USENIX Association</title>
		<meeting>the USENIX Annual Technical Conference (USENIX ATC&apos;21). USENIX Association<address><addrLine>Berkeley, CA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="715" to="728" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jonghyeon Kim, Wonkyo Choe, and Jeongseob Ahn. 2021. Exploring the design space of page management for multi- tiered memory systems. In Proceedings of the USENIX Annual Technical Conference (USENIX ATC&apos;21). USENIX Associ- ation, Berkeley, CA, 715-728. https://www.usenix.org/conference/atc21/presentation/kim-jonghyeon.</note>
</biblStruct>

<biblStruct coords="25,63.01,323.19,377.62,7.22;25,63.02,333.15,378.65,7.22;25,63.02,343.07,344.27,7.26" xml:id="b36">
	<analytic>
		<title level="a" type="main" coord="25,105.70,333.15,332.94,7.22" xml:id="_YaANsv6">Flipping bits in memory without accessing them: An experimental study of DRAM disturbance errors</title>
		<author>
			<persName coords=""><forename type="first">Yoongu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ross</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeremie</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Fallin</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Ji</forename><forename type="middle">Hye</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Donghyuk</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Konrad</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_t4mXkb3" coord="25,71.70,343.07,298.96,7.26">Proceedings of the ACM/IEEE 41st International Symposium on Computer Architecture (ISCA&apos;14</title>
		<meeting>the ACM/IEEE 41st International Symposium on Computer Architecture (ISCA&apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="361" to="372" />
		</imprint>
	</monogr>
	<note type="raw_reference">Yoongu Kim, Ross Daly, Jeremie Kim, Chris Fallin, Ji Hye Lee, Donghyuk Lee, Chris Wilkerson, Konrad Lai, and Onur Mutlu. 2014. Flipping bits in memory without accessing them: An experimental study of DRAM disturbance errors. In Proceedings of the ACM/IEEE 41st International Symposium on Computer Architecture (ISCA&apos;14). 361-372.</note>
</biblStruct>

<biblStruct coords="25,63.01,353.08,129.30,7.22" xml:id="b37">
	<monogr>
		<author>
			<persName coords=""><forename type="first">A</forename><surname>Kleen</surname></persName>
		</author>
		<title level="m" xml:id="_mkd4EHY" coord="25,113.29,353.08,75.59,7.22">A NUMA API for Linux</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note type="raw_reference">A. Kleen. 2004. A NUMA API for Linux.</note>
</biblStruct>

<biblStruct coords="25,63.01,363.00,378.33,7.26;25,62.84,373.00,163.01,7.22" xml:id="b38">
	<analytic>
		<title level="a" type="main" coord="25,186.95,363.04,127.22,7.22" xml:id="_Wapb7fq">Exascale computing in the united states</title>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Kothe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">I</forename><surname>Qualters</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCSE.2018.2875366</idno>
		<ptr target="https://doi.org/10.1109/MCSE.2018.2875366" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_2cHxZu7" coord="25,318.93,363.00,55.20,7.26">Comput. Sci. Eng</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="29" />
			<date type="published" when="2019-01">2019. January 2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">D. Kothe, S. Lee, and I. Qualters. 2019. Exascale computing in the united states. Comput. Sci. Eng. 21, 1 (January 2019), 17-29. https://doi.org/10.1109/MCSE.2018.2875366</note>
</biblStruct>

<biblStruct coords="25,63.02,382.96,377.46,7.22;25,63.02,392.89,377.44,7.26;25,63.02,402.85,378.68,7.26;25,63.02,412.85,25.95,7.22" xml:id="b39">
	<analytic>
		<title level="a" type="main" coord="25,261.16,382.96,179.32,7.22;25,63.02,392.93,53.41,7.22;25,104.12,402.85,43.47,7.26" xml:id="_AhxPhe5">Phase-based data placement scheme for heterogeneous memory systems</title>
		<author>
			<persName coords=""><forename type="first">Mohammad</forename><surname>Laghari</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Najeeb</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Didem</forename><surname>Unat</surname></persName>
		</author>
		<idno type="DOI">10.1109/CAHPC.2018.8645903</idno>
		<ptr target="https://doi.org/10.1109/CAHPC.2018.8645903" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_baXvGxJ" coord="25,130.46,392.89,309.99,7.26;25,63.02,402.85,35.34,7.26">Proceedings of the 30th International Symposium on Computer Architecture and High Performance Computing</title>
		<meeting>the 30th International Symposium on Computer Architecture and High Performance Computing<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
	<note>SBAC-PAD&apos;18)</note>
	<note type="raw_reference">Mohammad Laghari, Najeeb Ahmad, and Didem Unat. 2018. Phase-based data placement scheme for heterogeneous memory systems. In Proceedings of the 30th International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD&apos;18). IEEE Computer Society, Washington, DC, 189-196. https://doi.org/10.1109/CAHPC.2018. 8645903</note>
</biblStruct>

<biblStruct coords="25,63.01,422.81,312.53,7.22" xml:id="b40">
	<monogr>
		<title level="m" type="main" coord="25,119.96,422.81,15.86,7.22" xml:id="_jBh5Kj4">SICM</title>
		<author>
			<persName coords=""><forename type="first">Mike</forename><surname>Lang</surname></persName>
		</author>
		<ptr target="https://www.exascaleproject.org/research-project/sicm/" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Mike Lang. 2021. SICM. Retrieved from https://www.exascaleproject.org/research-project/sicm/.</note>
</biblStruct>

<biblStruct coords="25,63.01,432.78,377.42,7.22;25,63.02,442.70,378.23,7.26;25,63.02,452.66,153.03,7.26" xml:id="b41">
	<analytic>
		<title level="a" type="main" coord="25,184.31,432.78,256.12,7.22;25,63.02,442.74,57.60,7.22" xml:id="_QHW6MBU">Automatic pool allocation: Improving performance by controlling data structure layout in the heap</title>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Lattner</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Vikram</forename><surname>Adve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_pbJe6fQ" coord="25,135.00,442.70,306.25,7.26;25,63.02,452.66,49.08,7.26">Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI&apos;05)</title>
		<meeting>the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI&apos;05)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="129" to="142" />
		</imprint>
	</monogr>
	<note type="raw_reference">Chris Lattner and Vikram Adve. 2005. Automatic pool allocation: Improving performance by controlling data structure layout in the heap. In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implemen- tation (PLDI&apos;05). ACM, New York, NY, 129-142.</note>
</biblStruct>

<biblStruct coords="25,63.01,462.67,378.66,7.22;25,62.74,472.59,377.72,7.26;25,63.02,482.55,304.21,7.26" xml:id="b42">
	<analytic>
		<title level="a" type="main" coord="25,62.74,472.63,132.36,7.22" xml:id="_ApgmGa9">A fully associative, tagless DRAM cache</title>
		<author>
			<persName coords=""><forename type="first">Yongjun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jongwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hakbeom</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hyunggyun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jangwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jinkyu</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jae</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<idno type="DOI">10.1145/2749469.2750383</idno>
		<ptr target="https://doi.org/10.1145/2749469.2750383" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_xusgkcA" coord="25,210.59,472.59,229.87,7.26;25,63.02,482.55,68.94,7.26">Proceedings of the 42nd Annual International Symposium on Computer Architecture (ISCA&apos;15)</title>
		<meeting>the 42nd Annual International Symposium on Computer Architecture (ISCA&apos;15)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="211" to="222" />
		</imprint>
	</monogr>
	<note type="raw_reference">Yongjun Lee, Jongwon Kim, Hakbeom Jang, Hyunggyun Yang, Jangwoo Kim, Jinkyu Jeong, and Jae W. Lee. 2015. A fully associative, tagless DRAM cache. In Proceedings of the 42nd Annual International Symposium on Computer Architecture (ISCA&apos;15). ACM, New York, NY, 211-222. https://doi.org/10.1145/2749469.2750383</note>
</biblStruct>

<biblStruct coords="25,63.02,492.55,377.68,7.22;25,63.02,502.48,256.07,7.26" xml:id="b43">
	<analytic>
		<title level="a" type="main" coord="25,417.32,492.55,23.38,7.22;25,63.02,502.52,117.28,7.22" xml:id="_e4dJ2ej">Energy management for commercial servers</title>
		<author>
			<persName coords=""><forename type="first">Charles</forename><surname>Lefurgy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Karthick</forename><surname>Rajamani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Freeman</forename><surname>Rawson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Wes</forename><surname>Felter</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Kistler</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Tom</forename><forename type="middle">W</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_cqVSGAy" coord="25,185.43,502.48,31.25,7.26">Computer</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="39" to="48" />
			<date type="published" when="2003-12">2003. December 2003</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Charles Lefurgy, Karthick Rajamani, Freeman Rawson, Wes Felter, Michael Kistler, and Tom W. Keller. 2003. Energy management for commercial servers. Computer 36, 12 (December 2003), 39-48.</note>
</biblStruct>

<biblStruct coords="25,63.01,512.44,377.44,7.26;25,63.02,522.40,378.33,7.26;25,62.84,532.40,28.36,7.22" xml:id="b44">
	<analytic>
		<title level="a" type="main" coord="25,254.02,512.48,135.62,7.22" xml:id="_QU7Yshj">Utility-based hybrid memory management</title>
		<author>
			<persName coords=""><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_cwGYbZN" coord="25,403.65,512.44,36.81,7.26;25,63.02,522.40,234.98,7.26">Proceedings of the IEEE International Conference on Cluster Computing (CLUSTER&apos;17)</title>
		<meeting>the IEEE International Conference on Cluster Computing (CLUSTER&apos;17)<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="152" to="165" />
		</imprint>
	</monogr>
	<note type="raw_reference">Y. Li, S. Ghose, J. Choi, J. Sun, H. Wang, and O. Mutlu. 2017. Utility-based hybrid memory management. In Proceedings of the IEEE International Conference on Cluster Computing (CLUSTER&apos;17). IEEE Computer Society, Washington, DC, 152-165.</note>
</biblStruct>

<biblStruct coords="25,63.02,542.36,313.39,7.22" xml:id="b45">
	<monogr>
		<author>
			<persName coords=""><surname>Llnl</surname></persName>
		</author>
		<ptr target="https://asc.llnl.gov/CORAL-benchmarks" />
		<title level="m" xml:id="_Fyfv77x" coord="25,103.52,542.36,84.48,7.22">CORAL Benchmark Codes</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="raw_reference">LLNL. 2014. CORAL Benchmark Codes. Retrieved from https://asc.llnl.gov/CORAL-benchmarks.</note>
</biblStruct>

<biblStruct coords="25,63.01,552.32,377.42,7.22;25,63.02,562.24,377.75,7.26;25,62.79,572.25,60.93,7.22" xml:id="b46">
	<analytic>
		<title level="a" type="main" coord="25,188.20,552.32,252.24,7.22;25,63.02,562.28,19.86,7.22" xml:id="_Es2dGUe">Efficiently enabling conventional block sizes for very large die-stacked DRAM caches</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Loh</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_GgzWNVZ" coord="25,97.04,562.24,281.54,7.26">Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 44th Annual IEEE/ACM International Symposium on Microarchitecture<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="454" to="464" />
		</imprint>
	</monogr>
	<note type="raw_reference">Gabriel H. Loh and Mark D. Hill. 2011. Efficiently enabling conventional block sizes for very large die-stacked DRAM caches. In Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture. ACM, ACM, New York, NY, 454-464.</note>
</biblStruct>

<biblStruct coords="25,63.01,582.17,254.16,7.26" xml:id="b47">
	<monogr>
		<title level="m" type="main" coord="25,140.31,582.17,57.87,7.26" xml:id="_dE8ccv6">Ski Rental Problem</title>
		<author>
			<persName coords=""><forename type="first">Mark</forename><forename type="middle">S</forename><surname>Manasse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer US</publisher>
			<biblScope unit="page" from="849" to="851" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
	<note type="raw_reference">Mark S. Manasse. 2008. Ski Rental Problem. Springer US, Boston, MA, 849-851.</note>
</biblStruct>

<biblStruct coords="25,63.02,592.17,377.97,7.22;25,63.02,602.13,74.35,7.22" xml:id="b48">
	<monogr>
		<title level="m" type="main" coord="25,132.08,592.17,31.58,7.22" xml:id="_HC6NEMK">CGROUPS</title>
		<author>
			<persName coords=""><forename type="first">Paul</forename><surname>Menage</surname></persName>
		</author>
		<ptr target="https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt" />
		<imprint>
			<date type="published" when="2021-09-24">2021. September 24</date>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="raw_reference">Paul Menage. 2021. CGROUPS. Retrieved September 24, 2021 from https://www.kernel.org/doc/Documentation/ cgroup-v1/cgroups.txt.</note>
</biblStruct>

<biblStruct coords="25,63.01,612.10,378.78,7.22;25,63.02,622.04,377.44,7.26;25,63.02,632.00,325.93,7.26" xml:id="b49">
	<analytic>
		<title level="a" type="main" coord="25,333.88,612.10,107.92,7.22;25,63.02,622.08,247.20,7.22" xml:id="_DUNyuF5">Heterogeneous memory architectures: A HW/SW approach for mixing die-stacked and off-package memories</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Meswani</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Blagodurov</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">D</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Slice</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Ignatowski</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><forename type="middle">H</forename><surname>Loh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_9Rgb2ay" coord="25,324.37,622.04,116.09,7.26;25,63.02,632.00,212.57,7.26">Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA&apos;15)</title>
		<meeting>the IEEE International Symposium on High Performance Computer Architecture (HPCA&apos;15)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="126" to="136" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. R. Meswani, S. Blagodurov, D. Roberts, J. Slice, M. Ignatowski, and G. H. Loh. 2015. Heterogeneous memory architec- tures: A HW/SW approach for mixing die-stacked and off-package memories. In Proceedings of the IEEE International Symposium on High Performance Computer Architecture (HPCA&apos;15). IEEE Computer Society, 126-136.</note>
</biblStruct>

<biblStruct coords="26,62.85,84.09,377.42,7.22;26,62.85,94.02,367.66,7.26" xml:id="b50">
	<analytic>
		<title level="a" type="main" coord="26,369.82,84.09,70.45,7.22;26,62.85,94.06,243.09,7.22" xml:id="_Hxq4NmF">Enabling efficient and scalable hybrid memories using fine-granularity DRAM cache management</title>
		<author>
			<persName coords=""><forename type="first">Justin</forename><surname>Meza</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jichuan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hanbin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Parthasarathy</forename><surname>Ranganathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_4S6mVZM" coord="26,311.92,94.02,78.40,7.26">IEEE Comput. Arch. Lett</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Justin Meza, Jichuan Chang, HanBin Yoon, Onur Mutlu, and Parthasarathy Ranganathan. 2012. Enabling efficient and scalable hybrid memories using fine-granularity DRAM cache management. IEEE Comput. Arch. Lett. 11, 2 (2012).</note>
</biblStruct>

<biblStruct coords="26,62.85,103.98,378.33,7.26;26,62.85,113.98,233.10,7.22" xml:id="b51">
	<analytic>
		<title level="a" type="main" coord="26,159.43,104.02,171.82,7.22" xml:id="_6pxTm9q">A survey of techniques for architecting DRAM caches</title>
		<author>
			<persName coords=""><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><forename type="middle">S</forename><surname>Vetter</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPDS.2015.2461155</idno>
		<ptr target="https://doi.org/10.1109/TPDS.2015.2461155" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_kVQvHDE" coord="26,336.27,103.98,104.91,7.26">IEEE Trans. Parallel. Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1852" to="1863" />
			<date type="published" when="2016-06">2016. June 2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">S. Mittal and J. S. Vetter. 2016. A survey of techniques for architecting DRAM caches. IEEE Trans. Parallel. Distrib. Syst. 27, 6 (June 2016), 1852-1863. https://doi.org/10.1109/TPDS.2015.2461155</note>
</biblStruct>

<biblStruct coords="26,62.84,123.94,377.44,7.22;26,62.55,133.87,378.51,7.26;26,62.85,143.83,378.70,7.26;26,62.85,153.83,25.95,7.22" xml:id="b52">
	<analytic>
		<title level="a" type="main" coord="26,273.90,123.94,166.38,7.22;26,62.55,133.91,92.79,7.22" xml:id="_ypR5CDQ">Whirlpool: Improving dynamic cache management with static data classification</title>
		<author>
			<persName coords=""><forename type="first">Anurag</forename><surname>Mukkara</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nathan</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
		<idno type="DOI">10.1145/2872362.2872363</idno>
		<ptr target="https://doi.org/10.1145/2872362.2872363" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_9mJnkkA" coord="26,168.79,133.87,272.28,7.26;26,62.85,143.83,168.12,7.26">Proceedings of the 21st International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;16)</title>
		<meeting>the 21st International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS&apos;16)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="113" to="127" />
		</imprint>
	</monogr>
	<note type="raw_reference">Anurag Mukkara, Nathan Beckmann, and Daniel Sanchez. 2016. Whirlpool: Improving dynamic cache management with static data classification. In Proceedings of the 21st International Conference on Architectural Support for Program- ming Languages and Operating Systems (ASPLOS&apos;16). ACM, New York, NY, 113-127. https://doi.org/10.1145/2872362. 2872363</note>
</biblStruct>

<biblStruct coords="26,62.85,163.75,377.44,7.26;26,62.60,173.76,227.13,7.22" xml:id="b53">
	<analytic>
		<title level="a" type="main" coord="26,194.22,163.79,97.32,7.22" xml:id="_ksrcGr2">RowHammer: A retrospective</title>
		<author>
			<persName coords=""><forename type="first">Onur</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jeremie</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCAD.2019.2915318</idno>
		<ptr target="https://doi.org/10.1109/TCAD.2019.2915318" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_UbdRXTT" coord="26,297.60,163.75,123.97,7.26">Trans. Comp.-Aid. Des. Integ. Cir. Sys</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1555" to="1571" />
			<date type="published" when="2020-08">2020. August 2020</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Onur Mutlu and Jeremie S. Kim. 2020. RowHammer: A retrospective. Trans. Comp.-Aid. Des. Integ. Cir. Sys. 39, 8 (August 2020), 1555-1571. https://doi.org/10.1109/TCAD.2019.2915318</note>
</biblStruct>

<biblStruct coords="26,62.85,193.68,378.00,7.22;26,62.85,203.65,116.99,7.22" xml:id="b54">
	<monogr>
		<title level="m" type="main" coord="26,115.27,193.68,84.65,7.22" xml:id="_rqDQuWe">GP100 Pascal Whitepaper</title>
		<author>
			<persName coords=""><surname>Nvidia</surname></persName>
		</author>
		<ptr target="ttps://images.nvidia.com/content/pdf/tesla/whitepaper/pascal-architecture-whitepaper.pdf" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="raw_reference">NVIDIA. 2016. GP100 Pascal Whitepaper. Retrieved from ttps://images.nvidia.com/content/pdf/tesla/whitepaper/ pascal-architecture-whitepaper.pdf.</note>
</biblStruct>

<biblStruct coords="26,62.85,213.61,378.81,7.22;26,62.85,223.57,378.25,7.22;26,62.85,233.53,100.19,7.22" xml:id="b55">
	<monogr>
		<title level="m" type="main" coord="26,138.81,213.61,234.44,7.22" xml:id="_29UQC6c">Executive Order-Creating a National Strategic Computing Initiative</title>
		<author>
			<persName coords=""><forename type="first">Barack</forename><surname>Obama</surname></persName>
		</author>
		<ptr target="https://obamawhitehouse.archives.gov/the-press-office/2015/07/29/executive-order-creating-national-strategic-computing-initiative" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Barack Obama. 2015. Executive Order-Creating a National Strategic Computing Initiative. White House. Re- trieved from https://obamawhitehouse.archives.gov/the-press-office/2015/07/29/executive-order-creating-national- strategic-computing-initiative.</note>
</biblStruct>

<biblStruct coords="26,62.84,243.50,377.44,7.22;26,62.85,253.42,378.67,7.26;26,62.57,263.42,230.12,7.22" xml:id="b56">
	<analytic>
		<title level="a" type="main" coord="26,343.97,243.50,96.31,7.22;26,62.85,253.46,93.88,7.22" xml:id="_4sZcSUj">Portable application guidance for complex memory systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Brandon</forename><surname>Kammerdiener</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><surname>Jantz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kshitij</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Terry</forename><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357526.3357575</idno>
		<ptr target="https://doi.org/10.1145/3357526.3357575" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_CQBHCwt" coord="26,170.59,253.42,245.59,7.26">Proceedings of the International Symposium on Memory Systems (MEMSYS&apos;19)</title>
		<meeting>the International Symposium on Memory Systems (MEMSYS&apos;19)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page" from="156" to="166" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. Olson, Brandon Kammerdiener, Michael Jantz, Kshitij Doshi, and Terry Jones. 2019. Portable application guidance for complex memory systems. In Proceedings of the International Symposium on Memory Systems (MEMSYS&apos;19), Vol. 0. ACM, New York, NY, 156-166. https://doi.org/10.1145/3357526.3357575</note>
</biblStruct>

<biblStruct coords="26,62.84,273.38,378.66,7.22;26,62.85,283.31,378.32,7.26;26,62.57,293.30,71.41,7.22" xml:id="b57">
	<analytic>
		<title level="a" type="main" coord="26,82.04,283.35,233.09,7.22" xml:id="_8WjGPMz">Cross-layer memory management to improve DRAM energy efficiency</title>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><surname>Benjamin Olson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Joseph</forename><forename type="middle">T</forename><surname>Teague</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Divyani</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Michael</forename><forename type="middle">R</forename><surname>Jantz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Kshitij</forename><forename type="middle">A</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Prasad</forename><forename type="middle">A</forename><surname>Kulkarni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j" xml:id="_jgxzvA8" coord="26,320.70,283.31,100.60,7.26">ACM Trans. Arch. Code Optim</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">20</biblScope>
			<date type="published" when="2018-05">2018. May 2018</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Matthew Benjamin Olson, Joseph T. Teague, Divyani Rao, Michael R. Jantz, Kshitij A. Doshi, and Prasad A. Kulkarni. 2018. Cross-layer memory management to improve DRAM energy efficiency. ACM Trans. Arch. Code Optim. 15, 2, Article 20 (May 2018).</note>
</biblStruct>

<biblStruct coords="26,62.84,303.26,377.45,7.22;26,62.85,313.19,377.44,7.26;26,62.85,323.15,305.49,7.26" xml:id="b58">
	<analytic>
		<title level="a" type="main" coord="26,328.70,303.26,111.59,7.22;26,62.85,313.23,119.59,7.22" xml:id="_Hkk2myF">MemBrain: Automated application guidance for hybrid memory systems</title>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">B</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">R</forename><surname>Jantz</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">K</forename><forename type="middle">A</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">M</forename><forename type="middle">G</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">O</forename><surname>Hernandez</surname></persName>
		</author>
		<idno type="DOI">10.1109/NAS.2018.8515694</idno>
		<ptr target="https://doi.org/10.1109/NAS.2018.8515694" />
	</analytic>
	<monogr>
		<title level="m" xml:id="_wbDEpTp" coord="26,196.51,313.19,243.78,7.26;26,62.85,323.15,66.01,7.26">Proceedings of the IEEE International Conference on Networking, Architecture and Storage (NAS&apos;18)</title>
		<meeting>the IEEE International Conference on Networking, Architecture and Storage (NAS&apos;18)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
	<note type="raw_reference">M. B. Olson, T. Zhou, M. R. Jantz, K. A. Doshi, M. G. Lopez, and O. Hernandez. 2018. MemBrain: Automated application guidance for hybrid memory systems. In Proceedings of the IEEE International Conference on Networking, Architecture and Storage (NAS&apos;18). IEEE Computer Society, 1-10. https://doi.org/10.1109/NAS.2018.8515694</note>
</biblStruct>

<biblStruct coords="26,62.84,333.15,377.43,7.22;26,62.85,343.11,378.25,7.22;26,62.85,353.08,67.47,7.22" xml:id="b59">
	<analytic>
		<title level="a" type="main" coord="26,252.79,333.15,187.48,7.22;26,62.85,343.11,17.45,7.22" xml:id="_ztRFfjv">to Deliver Record-Setting Frontier Supercomputer at ORNL</title>
		<ptr target="https://www.ornl.gov/news/us-department-energy-and-cray-deliver-record-setting-frontier-supercomputer-ornl" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_PuSUXrq" coord="26,62.84,333.15,17.44,7.22">ORNL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>U.S. Department of Energy and Cray</orgName>
		</respStmt>
	</monogr>
	<note type="raw_reference">ORNL. 2019. U.S. Department of Energy and Cray to Deliver Record-Setting Frontier Supercomputer at ORNL. Retrieved from https://www.ornl.gov/news/us-department-energy-and-cray-deliver-record-setting-frontier- supercomputer-ornl.</note>
</biblStruct>

<biblStruct coords="26,62.85,363.04,377.97,7.22;26,62.85,373.00,107.86,7.22" xml:id="b60">
	<monogr>
		<title level="m" type="main" coord="26,100.91,363.04,176.08,7.22" xml:id="_PmSBGMs">2021. pagemap, from the userspace perspective</title>
		<ptr target="https://www.kernel.org/doc/Documentation/vm/pagemap.txt" />
		<imprint/>
	</monogr>
	<note type="raw_reference">pagemap. 2021. pagemap, from the userspace perspective. Retrieved from https://www.kernel.org/doc/ Documentation/vm/pagemap.txt.</note>
</biblStruct>

<biblStruct coords="26,62.84,382.96,377.73,7.22;26,62.85,392.89,377.44,7.26;26,62.85,402.85,200.05,7.26" xml:id="b61">
	<analytic>
		<title level="a" type="main" coord="26,404.77,382.96,35.80,7.22;26,62.85,392.93,162.99,7.22" xml:id="_nETMtq9">RTHMS: A tool for data placement on hybrid memory system</title>
		<author>
			<persName coords=""><forename type="first">Bo</forename><surname>Ivy</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Roberto</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Gokcen</forename><surname>Gioiosa</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pietro</forename><surname>Kestor</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Erwin</forename><surname>Cicotti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Stefano</forename><surname>Laure</surname></persName>
		</author>
		<author>
			<persName coords=""><surname>Markidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_PkK7YCM" coord="26,240.41,392.89,199.88,7.26;26,62.85,402.85,103.21,7.26">Proceedings of the ACM SIGPLAN International Symposium on Memory Management (ISMM&apos;17)</title>
		<meeting>the ACM SIGPLAN International Symposium on Memory Management (ISMM&apos;17)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="82" to="91" />
		</imprint>
	</monogr>
	<note type="raw_reference">Ivy Bo Peng, Roberto Gioiosa, Gokcen Kestor, Pietro Cicotti, Erwin Laure, and Stefano Markidis. 2017. RTHMS: A tool for data placement on hybrid memory system. In Proceedings of the ACM SIGPLAN International Symposium on Memory Management (ISMM&apos;17). ACM, New York, NY, 82-91.</note>
</biblStruct>

<biblStruct coords="26,62.84,412.85,378.68,7.22;26,62.85,422.81,53.08,7.22" xml:id="b62">
	<monogr>
		<ptr target="https://perf.wiki.kernel.org/index.php/Main_Page" />
		<title level="m" xml:id="_vTdyXSf" coord="26,134.32,412.85,140.32,7.22">Linux Profiling with Performance Counters</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="raw_reference">kernel.org 2019. perf: Linux Profiling with Performance Counters. Retrieved from https://perf.wiki.kernel.org/index. php/Main_Page.</note>
</biblStruct>

<biblStruct coords="26,62.85,432.74,377.45,7.26;26,62.85,442.70,231.87,7.26" xml:id="b63">
	<analytic>
		<title level="a" type="main" coord="26,213.29,432.78,192.33,7.22" xml:id="_RfGpRD5">Segregating heap objects by reference behavior and lifetime</title>
		<author>
			<persName coords=""><forename type="first">Matthew</forename><forename type="middle">L</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Benjamin</forename><forename type="middle">G</forename><surname>Zorn</surname></persName>
		</author>
		<idno type="DOI">10.1145/291006.291012</idno>
		<ptr target="https://doi.org/10.1145/291006.291012" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_KdzmaA4" coord="26,410.41,432.74,29.89,7.26;26,62.85,442.70,13.07,7.26">SIGPLAN Not</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="12" to="23" />
			<date type="published" when="1998-10">1998. October 1998</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Matthew L. Seidl and Benjamin G. Zorn. 1998. Segregating heap objects by reference behavior and lifetime. SIGPLAN Not. 33, 11 (October 1998), 12-23. https://doi.org/10.1145/291006.291012</note>
</biblStruct>

<biblStruct coords="26,62.85,452.70,377.45,7.22;26,62.85,462.63,378.66,7.26;26,62.85,472.63,108.34,7.22" xml:id="b64">
	<analytic>
		<title level="a" type="main" coord="26,300.49,452.70,139.80,7.22;26,62.85,462.67,86.66,7.22" xml:id="_mX7YMpB">Automating the application data placement in hybrid memory systems</title>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Servat</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">A</forename><forename type="middle">J</forename><surname>Peña</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">G</forename><surname>Llort</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">E</forename><surname>Mercadal</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">J</forename><surname>Labarta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_XK5JG53" coord="26,164.47,462.63,273.47,7.26">Proceedings of the IEEE International Conference on Cluster Computing (CLUSTER&apos;17)</title>
		<meeting>the IEEE International Conference on Cluster Computing (CLUSTER&apos;17)</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="126" to="136" />
		</imprint>
	</monogr>
	<note type="raw_reference">H. Servat, A. J. Peña, G. Llort, E. Mercadal, H. Hoppe, and J. Labarta. 2017. Automating the application data placement in hybrid memory systems. In Proceedings of the IEEE International Conference on Cluster Computing (CLUSTER&apos;17). IEEE Computer Society, 126-136.</note>
</biblStruct>

<biblStruct coords="26,62.85,482.59,378.78,7.22;26,62.85,492.51,377.45,7.26;26,62.85,502.48,378.31,7.26;26,62.66,512.48,20.95,7.22" xml:id="b65">
	<analytic>
		<title level="a" type="main" coord="26,369.43,482.59,72.20,7.22;26,62.85,492.55,239.55,7.22" xml:id="_X6smWR8">Creating and preserving locality of java applications at allocation and garbage collection times</title>
		<author>
			<persName coords=""><forename type="first">Yefim</forename><surname>Shuf</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Manish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hubertus</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Andrew</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jaswinder Pal</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_vBmRrP9" coord="26,316.72,492.51,123.58,7.26;26,62.85,502.48,304.44,7.26">Proceedings of the 17th ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications (OOPSLA&apos;02)</title>
		<meeting>the 17th ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications (OOPSLA&apos;02)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="13" to="25" />
		</imprint>
	</monogr>
	<note type="raw_reference">Yefim Shuf, Manish Gupta, Hubertus Franke, Andrew Appel, and Jaswinder Pal Singh. 2002. Creating and preserv- ing locality of java applications at allocation and garbage collection times. In Proceedings of the 17th ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications (OOPSLA&apos;02). ACM, New York, NY, 13-25.</note>
</biblStruct>

<biblStruct coords="26,62.85,522.44,378.79,7.22;26,62.55,532.36,377.74,7.26;26,62.85,542.32,248.33,7.26" xml:id="b66">
	<analytic>
		<title level="a" type="main" coord="26,381.65,522.44,59.99,7.22;26,62.55,532.40,182.64,7.22" xml:id="_SG8nyXV">Transparent hardware management of stacked DRAM as part of memory</title>
		<author>
			<persName coords=""><forename type="first">Jaewoong</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Alaa</forename><forename type="middle">R</forename><surname>Alameldeen</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Zeshan</forename><surname>Chishti</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chris</forename><surname>Wilkerson</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Hyesoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_PGR7pfV" coord="26,260.97,532.36,179.32,7.26;26,62.85,542.32,103.97,7.26">Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture</title>
		<meeting>the 47th Annual IEEE/ACM International Symposium on Microarchitecture</meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
	<note type="raw_reference">Jaewoong Sim, Alaa R. Alameldeen, Zeshan Chishti, Chris Wilkerson, and Hyesoon Kim. 2014. Transparent hard- ware management of stacked DRAM as part of memory. In Proceedings of the 47th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO-47). IEEE Computer Society, 13-24.</note>
</biblStruct>

<biblStruct coords="26,62.84,552.32,249.68,7.22" xml:id="b67">
	<monogr>
		<title level="m" type="main" coord="26,102.92,552.32,50.14,7.22" xml:id="_bn3hDsk">SPEC CPU 2017</title>
		<ptr target="https://www.spec.org/cpu" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="raw_reference">SPEC. 2017. SPEC CPU 2017. Retrieved from https://www.spec.org/cpu2017/.</note>
</biblStruct>

<biblStruct coords="26,62.84,562.24,377.43,7.26;26,62.85,572.21,333.25,7.26" xml:id="b68">
	<analytic>
		<title level="a" type="main" coord="26,254.14,562.28,135.11,7.22" xml:id="_twZQpzU">Jenga: Software-defined cache hierarchies</title>
		<author>
			<persName coords=""><forename type="first">Po-An</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Nathan</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Daniel</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_Sj2yvZm" coord="26,403.47,562.24,36.80,7.26;26,62.85,572.21,229.30,7.26">Proceedings of the 44th International Symposium on Computer Architecture (ISCA&apos;17)</title>
		<meeting>the 44th International Symposium on Computer Architecture (ISCA&apos;17)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="652" to="665" />
		</imprint>
	</monogr>
	<note type="raw_reference">Po-An Tsai, Nathan Beckmann, and Daniel Sanchez. 2017. Jenga: Software-defined cache hierarchies. In Proceedings of the 44th International Symposium on Computer Architecture (ISCA&apos;17). ACM, New York, NY, 652-665.</note>
</biblStruct>

<biblStruct coords="26,62.84,582.21,377.59,7.22;26,62.85,592.13,378.70,7.26;26,62.85,602.13,25.95,7.22" xml:id="b69">
	<analytic>
		<title level="a" type="main" coord="26,331.99,582.21,108.44,7.22;26,62.85,592.17,38.77,7.22" xml:id="_Eyn7yHR">On-the-fly structure splitting for heap objects</title>
		<author>
			<persName coords=""><forename type="first">Zhenjiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Chenggang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Pen-Chung</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Jianjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Di</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2086696.2086705</idno>
		<ptr target="https://doi.org/10.1145/2086696.2086705" />
	</analytic>
	<monogr>
		<title level="j" xml:id="_zw9rJGC" coord="26,106.37,592.13,101.49,7.26">ACM Trans. Archit. Code Optim</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">20</biblScope>
			<date type="published" when="2012-01">2012. January 2012</date>
		</imprint>
	</monogr>
	<note type="raw_reference">Zhenjiang Wang, Chenggang Wu, Pen-Chung Yew, Jianjun Li, and Di Xu. 2012. On-the-fly structure splitting for heap objects. ACM Trans. Archit. Code Optim. 8, 4, Article 26 (January 2012), 20 pages. https://doi.org/10.1145/2086696. 2086705</note>
</biblStruct>

<biblStruct coords="26,62.85,612.11,378.80,7.22;26,62.85,622.04,378.29,7.26;26,62.85,632.00,230.15,7.26" xml:id="b70">
	<analytic>
		<title level="a" type="main" coord="26,209.02,612.11,232.63,7.22;26,62.85,622.08,81.55,7.22" xml:id="_RgusDZM">Unimem: Runtime data managementon non-volatile memory-based heterogeneous main memory</title>
		<author>
			<persName coords=""><forename type="first">Kai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Yingchao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName coords=""><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m" xml:id="_pZQrYZm" coord="26,158.44,622.04,282.71,7.26;26,62.85,632.00,89.65,7.26">Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC&apos;17)</title>
		<meeting>the International Conference for High Performance Computing, Networking, Storage and Analysis (SC&apos;17)<address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Article 58, 14 pages</note>
	<note type="raw_reference">Kai Wu, Yingchao Huang, and Dong Li. 2017. Unimem: Runtime data managementon non-volatile memory-based het- erogeneous main memory. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC&apos;17). ACM, New York, NY, Article 58, 14 pages.</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
